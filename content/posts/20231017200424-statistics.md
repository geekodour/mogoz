+++
title = "Statistics"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Math]({{< relref "20221101134840-math.md" >}}), [Causal Inference]({{< relref "20231017204511-causal_inference.md" >}})

    {{< figure src="/ox-hugo/20231017200424-statistics-830473767.png" >}}


## FAQ {#faq}


### Bayesian or Frequentist {#bayesian-or-frequentist}

-   The statistical war is over, we no longer talk about this
-   See [Statistics: Are you Bayesian or Frequentist?](https://towardsdatascience.com/statistics-are-you-bayesian-or-frequentist-4943f953f21b)
-   We're more interested in how we justify our statistical procedures, whether they're Bayesian or frequentist, which leads us to [Causal Inference]({{< relref "20231017204511-causal_inference.md" >}})


### Rounding Numbers {#rounding-numbers}

It was surprising to me that I am was on the wrong side when coming to rounding numbers! Most important thing is to **round in one step** and to **round in the last step of the calculation.**

-   Draw a line mentally at the point where you want to round.
-   If the number next to the line is 0-4, throw away everything to the right of the line.
-   If the number next to the line is 5-9, raise the digit to the left by one and throw away everything to the right of the line.

Eg. `1.2|4768` ~ `1.2` but `1.2|7432` ~ `1.3`


## Basics {#basics}


### Sample and Population {#sample-and-population}

![](/ox-hugo/20231017200424-statistics-852730979.png)
Because of this, we can see two versions of formulas, one for population and one for sample
![](/ox-hugo/20231017200424-statistics-548727731.png)


#### Population {#population}

There are no set rules to apply when defining a population except knowledge, common sense, and judgment


#### Sample {#sample}

-   It's an approximation of the population
-   Always will have some error in them
-   It is usually a subgroup of the population, but in a census, the whole population is the sample. The sample size is usually denoted as n and the population size as N and the sample size is always a definite number.

<!--list-separator-->

-  Good and bad samples

    **A good sample** is a smaller group that is representative of the population, all valid samples are chosen through probability means. Following are some ways to collect samples, ordered by preference:

    <!--list-separator-->

    -  Random Samples

        Random doesn't mean unplanned; even collecting random samples needs proper planning. For this, you need a list and some way to select random subjects from the list for your sample.

    <!--list-separator-->

    -  Systematic Samples

        Best explained through an example, Standing outside the grocery store all day, you survey every 40th person. That is a systematic sample with k=40.

    <!--list-separator-->

    -  Cluster Samples

        This one makes a big assumption, that the individuals in each cluster are representative of the whole population. A cluster sample cannot be analyzed in all the same ways as random or systematic samples. You subdivide the population into a large number of subunits(clusters) and then construct random samples from the clusters.

    <!--list-separator-->

    -  Stratified Samples

        This needs analyzing what data you'll be working with, if you can identify subgroups(strata) that have something in common related to what you're trying to study, you want to ensure that you have the same mix of those groups as the population. Eg. 45% Girls and 55% Boys in a school, If you're taking samples of 400, it should be 45% x 400 and 55% x 400, each mini sample should be constructed using other random sampling methods.

    <!--list-separator-->

    -  Census

        A census sample contains every member of the population.


### Statistics {#statistics}


#### Descriptive Statistics {#descriptive-statistics}

-   Summarizing and presenting the data that was measured
-   Can be done for both quantitative and categorical data
-   **statistic**
    -   a statement of Descriptive stats
    -   a numerical summary of a sample


#### Inferential Statistics {#inferential-statistics}

-   Making statements about the population based on measurements of a smaller sample.
-   **parameter**
    -   a statement of Inferential stats
    -   a numerical summary of a population


### Errors {#errors}

In stats, errors are not like programing errors but are the discrepancy between your findings and reality.


#### Sampling error {#sampling-error}

These are part of the sampling process, cannot be eliminated can be minimized by increasing the sample size


#### Non-sampling error {#non-sampling-error}

When you mess up in collecting data/analyzing data etc.


### Data and experiments {#data-and-experiments}


#### Variables {#variables}

Variables are the question and data points are the answers. Eg. birth weight is the variable and 5Kg will be the data point. sometimes variable type is also called data type.

In either observational study or experimental study there are two variables:

-   **Explanatory variables/factors**: Suspected causes
-   **Response variables**: Suspected effects/results

<!--list-separator-->

-  Explanatory variables that make the results/response variables questionable:

    -   **Lurking variables**: A hidden variable that isn't measured but affects the outcome. A careful randomized experimental study can get rid of these.

    -   **Confounding variables**: You know what they are, but you cannot untangle their effect from what you actually wanted. Try to rule these out if possible before experimentation.

<!--list-separator-->

-  Quantitative and Qualitative

    <!--list-separator-->

    -  Quantitative

        Numeric, sometimes it's hard to differentiate between discrete and contd. but it's important to identify the difference when you need to graph them

        -   Discrete: how many
        -   Contd. : how much

    <!--list-separator-->

    -  Qualitative/Categorical

        non-numeric


#### Gathering data {#gathering-data}

<!--list-separator-->

-  Observational study

    A retrospective study. Lurking variables are the reason an obs study can never establish cause/causation, no matter how strong of an association do you find.

<!--list-separator-->

-  Experiment

    Here we can manipulate the explanatory variables, each level of the assigned explanatory variable is known as a treatment. If we do have a randomized experiment, we can prove causation. Eg. Doing a study by giving your 3 children different toys, the explanatory variable is `toy`, and treatments are the different toys.


### Basics of designing experiments {#basics-of-designing-experiments}


#### Completely Randomized Design {#completely-randomized-design}

Randomly assign members to the various treatment groups, this is called randomization


#### Randomized Block Design {#randomized-block-design}

When there is a confounding variable that you can detect, before conducting the experiment divide subjects into blocks according to that variable, then randomize within each block. This variable is called the blocking variable. i.e confounding variable became the blocking variable here.

> "Block what you can, randomize what you cannot"


#### Matched pairs {#matched-pairs}

Type of randomized block design where each block contains two identical subjects without any fear of lurking variables. (Eg. twins) another special type is matching experimental results with itself (eg. delta)


#### Control groups and placebo {#control-groups-and-placebo}

When doing experiments involving the placebo effect, the group that gets the placebo is called the control group


## Distribution {#distribution}


### Frequency Distribution {#frequency-distribution}

{{< figure src="/ox-hugo/20231017200424-statistics-1338030680.png" >}}


## Descriptive Stats {#descriptive-stats}


### 3M (Center of Data) {#3m--center-of-data}

-   Mean, Median and Mode
-   Measure the `center` of the data


#### Mean {#mean}

<!--list-separator-->

-  Arithmetic Mean/Sample mean

    -   Useful for additive processes

<!--list-separator-->

-  Geometric Mean

    -   Useful for multiplicative processes
    -   Useful w Growth rates because they depend on multiplication and not in addition
        ![](/ox-hugo/20231017200424-statistics-1567265898.png)
        ![](/ox-hugo/20231017200424-statistics-817498960.png)


#### Median {#median}

-   `sort` data in `inc` order
-   `odd` observations: middle value of data array
-   `even` observations: `mean` of the `2` middle values of data array


#### Mode {#mode}

-   Observation that `occurs the most`
-   Dataset can have one, multiple or no modes


#### Median vs Mean {#median-vs-mean}

![](/ox-hugo/20231017200424-statistics-1417433532.png)
Solution can be Trimmed Mean
![](/ox-hugo/20231017200424-statistics-491743553.png)


### Percentiles, Quartiles(4), Quintiles(5), &amp; Deciles(10) {#percentiles-quartiles--4--quintiles--5--and-deciles--10}

{{< figure src="/ox-hugo/20231017200424-statistics-966232344.png" >}}

-   These just help us locate an observation in a `sorted (low to max)` dataset; an address
    -   Doesn't have to a value in the dataset.
        ![](/ox-hugo/20231017200424-statistics-1820198744.png)
    -   There's a location formula, we can calculate the actual value of the percentile from the location even if the address doesn't point to a data point
        ![](/ox-hugo/20231017200424-statistics-612061447.png)
-   Quartiles, Quintiles, &amp; Deciles are variants of `percetile`
-   `Percentiles`
    -   The `number of values out of the total` that are at or below that percentile
    -   The observations lie below the said `percentile` or above the said `percentile`
-   Formula to find `percentile` for some data point
    ![](/ox-hugo/20231017200424-statistics-1772885859.png)


#### IQR (Inter Quartile(4) Range) {#iqr--inter-quartile4--range}

The `median` will lie somewhere in between the `IQR`
![](/ox-hugo/20231017200424-statistics-833077485.png)
![](/ox-hugo/20231017200424-statistics-1999813275.png)
![](/ox-hugo/20231017200424-statistics-1111788062.png)
![](/ox-hugo/20231017200424-statistics-56491207.png)
![](/ox-hugo/20231017200424-statistics-1297186666.png)
![](/ox-hugo/20231017200424-statistics-2130326027.png)


### Variability {#variability}

-   Useful when comparing datasets
-   Related to the `mean`
-   If things are "spread out"
-   Variability answers, **"How far is `each` data point from the mean? (DISTANCE)"**
    ![](/ox-hugo/20231017200424-statistics-1441461549.png)


#### Standard Deviation {#standard-deviation}

-   `Standard Deviation` is just the `positive square root` of the `variance`
    -   SD is the sqrt of the `sum of` the `square` of the `difference of the data point and the mean` divided by the `no. of observations`
        ![](/ox-hugo/20231017200424-statistics-1355432260.png)
-   What it says?
    -   If most data points are close to the mean, variance &amp; SD will be lower
    -   If most data points are further to the mean(spread out), variance &amp; SD will be higher


#### Z-score {#z-score}

![](/ox-hugo/20231017200424-statistics-394288214.png)
![](/ox-hugo/20231017200424-statistics-566762106.png)

-   Z-score answers: **"How far is `any` given data point from the mean? (DISTANCE)"**
    -   How many SD away from the mean? (It measures distance in the unit of `sd` and ignores any original units such as inches/hours etc.)
-   The `Z-score` for the `mean` itself is `0` because, it's `0` distance away from the `mean`
    ![](/ox-hugo/20231017200424-statistics-1329000241.png)


#### Bi-variance {#bi-variance}

Relationship between two variables

<!--list-separator-->

-  Covariance

    {{< figure src="/ox-hugo/20231017200424-statistics-1984831947.png" >}}

    -   Shows the `linear` association btwn 2 variables.
    -   It shows the `direction`, not the `strength`
        -   `+ve`: increasing linear relation
        -   `-ve`: decreasing linear relation
    -   No upper/lower bounds, scale depends on variables
    -   Covariance Matrix
        ![](/ox-hugo/20231017200424-statistics-1688467105.png)

<!--list-separator-->

-  Correlation

    {{< figure src="/ox-hugo/20231017200424-statistics-1942864188.png" >}}

    -   It shows the `direction`, AND the `strength`
        -   `strength` of correlation does not mean the correlation is statically significant
    -   **Only applicable to linear relations**
    -   Always between `-1` and `1`, i.e scale is independent of the scale of the variables
    -   Covariance is not standarized, correlation is standarized. i.e we can use correlation to compare two data sets using different units etc.

<!--list-separator-->

-  Linear Regression

<!--list-separator-->

-  Non Linear data

    {{< figure src="/ox-hugo/20231017200424-statistics-963039336.png" >}}


## Visualization {#visualization}

See [Data Visualization]({{< relref "20230504114958-data_visualization.md" >}})


## Hypothesis testing (\\(H\_0\\)) {#hypothesis-testing--h-0}


### Null Hypothesis {#null-hypothesis}

-   The hypothesis that there is no difference between things is called the Null Hypothesis.
    -   We look for in the data if it convinces us to `reject` the hypothesis that `there is no difference`
-   Used to check if two things are different without using any preliminary data/test/experiment
