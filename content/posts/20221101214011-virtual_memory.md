+++
title = "Virtual Memory"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Computer Memory]({{< relref "20221101203327-computer_memory.md" >}}), [Filesystems]({{< relref "20221101145053-filesystems.md" >}})

> All of this is linux specific


## Memory Management {#memory-management}

-   It's not exact science, everything is tradeoff between performance and accuracy.
-   Example: If you limit memory it might affect disk I/O because it'll start evicting the page cache and start using swap.


## Virtual Memory {#virtual-memory}

{{< figure src="/ox-hugo/20221101214011-virtual_memory-1392942868.png" >}}


### Why do we need it {#why-do-we-need-it}

-   Modern computers run multiple tasks, directly from/to physical memory is a bad idea because each program would request its own contagious block.
-   We don't have enough memory to give each program its own contagious memory, so we give it non-contagious and slap virtual memory on top of it
-   It tricks the program to thinking that it has contagious memory now
-   That too till the end of the virtual address space, not just till the end of physical memory. Big W.
-   So essentially, we hide the fragmentation from the program by using virtual memory.
-   A translation from physical to virtual address happens and vice-versa.
-   This non-contagious memory, which appears to be contagious may be spread across, disk, physical memory, cache, or may even not be physically represented etc.


### How is it implemented? {#how-is-it-implemented}

-   Segmentation
-   Page Table/Paged Virtual Memory


### Resources {#resources}

-   [Concepts overview — The Linux Kernel documentation](https://www.kernel.org/doc/html/latest/admin-guide/mm/concepts.html)


## Paging {#paging}

<div class="warning small-text">

> Paging is an implementation of virtual memory that the Linux kernel uses.
>
> This topic should have been under virtual memory or Kernel but it's a big enough topic so let it have its own Heading. But details will be specific to linux kernel.
</div>

See [Memory Allocation]({{< relref "20221101213324-memory_allocation.md" >}}) and [Memory Design]({{< relref "20221101202144-memory_design.md" >}}) and [Memory Hierarchy]({{< relref "20221101213401-memory_hierarchy.md" >}})


### Components {#components}


#### MMU {#mmu}

{{< figure src="/ox-hugo/20221101214011-virtual_memory-1972270768.png" >}}

<!--list-separator-->

-  Page Table and Directory

    -   In theory, MMU sits between CPU and memory but in practice, it's right there in the CPU chip itself.
    -   The MMU maps memory through a [series of tables](https://wiki.osdev.org/Paging) which are stored in RAM.
        -   Page directory (`PD`), Pointed by Page Table Base Register (`PTBR`), PTBR stored at `CR3`
        -   Page table (`PT`), Contains Page Table Entries (`PTE`)
    -   Now there are even systems which do not have a MMU, The linux memory management system for it is called `nommu`

<!--list-separator-->

-  Isolation and protection

    -   Paging is possible due to the MMU which is a hardware thing
    -   Paging provides HW level isolation ( **memory protection** )
        -   User processes can only see and modify data which is paged in on their own address space.
        -   System pages are also protected from user processes.

<!--list-separator-->

-  TLB (Translation Lookaside Buffer)

    {{< figure src="/ox-hugo/20221101214011-virtual_memory-1359457724.png" >}}

    -   Memory cache in the `MMU`
    -   If there is a TLB miss, it'll trigger a  `page walk` (Search down the page table levels to look for entry)
    -   Stores recent translations of virtual memory to physical memory.
    -   The majority of desktop, laptop, and server processors include one or more TLBs
    -   Applications with large memory [WSS]({{< relref "20221101215651-memory_metrics.md#wss" >}}) will experience performance hit because of TLB misses.
    -   See [MMU gang wars: the TLB drive-by shootdown | Hacker News](https://news.ycombinator.com/item?id=23214535)


#### Pages {#pages}

-   Also called **virtual pages** , and **block** (see Block below)
-   Usually the allocator deals with this and not user programs directly.
-   Usually 4-64 KB in size (Huge Pages upto 2MB/1GB)
-   May or May Not be backed by a `page frame`
-   Some architectures allow selection of the page size, this can be configured at the kernel build time

<!--listend-->

```shell
λ getconf PAGESIZE
4096
```


#### Blocks {#blocks}

-   `pagesize = blocksize` (Sometime `Page` can simply refer to the `Block`)


#### Page Frame {#page-frame}

-   Also called **Frame** / Physical Memory Page
-   This is actual physical memory(RAM)
-   A page frame may back multiple pages. e.g.shared memory or memory-mapped files.


#### Page Table {#page-table}

-   Kernel maintains **per process page table**
-   One way address translation, figure out physical address from virtual address.
-   Stored in the RAM.
-   Kernel may have multiple page tables, active one is the one pointed by `CR3`
-   Organized hierarchically(multi-levels) to reduce [waste in memory](https://os.phil-opp.com/paging-introduction/)
    ![](/ox-hugo/20221101214011-virtual_memory-1389536447.png)

| block | page frame |
|-------|------------|
| 1     | 13         |
| 2     | 76         |
| 3     | 55         |

-   Steps in accessing memory via page table
    -   Access requested for `block 3`
    -   MMU intercepts &amp; looks up page table to find the corresponding page frame
    -   Generate a physical address, pointing to `55th page frame` in the RAM.
-   An access memory location can be
    -   An instruction
    -   Load/Store to some data.
-   Additional links
    -   [Complete virtual memory map with 4-level page tables](https://www.kernel.org/doc/html/latest/x86/x86_64/mm.html)
    -   [Examining Process Page Tables — The Linux Kernel documentation](https://www.kernel.org/doc/html/latest/admin-guide/mm/pagemap.html)
    -   [kernel - Viewing pagetable for a process - Unix &amp; Linux Stack Exchange](https://unix.stackexchange.com/questions/369185/viewing-pagetable-for-a-process)
    -   [dwks/pagemap: Userspace tool to map virtual page addresses](https://github.com/dwks/pagemap)


### Important Features {#important-features}


#### Demand Paging {#demand-paging}

-   This is used by both Standard IO and Memory Mapped IO. See [Disk I/O]({{< relref "20230402013722-disk_i_o.md" >}})
-   All pages are backed to disk, so starting on disk is similar to being swapped out
-   OS copies a disk page into physical memory only if an attempt is made to access it and that page is not already in memory.


#### Page faults {#page-faults}

-   Hard/Major faults: Needs to fetch things from disk
-   Soft/Minor faults: Updates things in memory, just the PTE needs to be created.


#### Locking Pages {#locking-pages}

-   When a page fault occurs and kernel needs to do I/O for paging in
-   Execution time of an instruction skyrockets
-   We can Lock pages for programs that are sensitive to that. See `mlock`


## User process {#user-process}


### Virtual address space (VAS) {#virtual-address-space--vas}

The VAS of a process consist of stack, heap, code, data, arguments etc.

![](/ox-hugo/20221101132431-gc_essentials_udemy-1968463754.png)
The 3 and 1 GB split, is [only for 32bit systems](https://en.wikipedia.org/wiki/High_memory), rest of the image can be followed. For 64bit representation [see this](https://www.kernel.org/doc/html/latest/x86/x86_64/mm.html).

```c
printf("%p", &i); // shows virtual address
```

-   A process is represented by its VAS, a contagious address space from `addr0` to `MAX_SIZE`.
-   When compiling with `gcc`, the `a.out` (the executable) file contains information about how to create the virtual address space.


#### Virtual Addresses {#virtual-addresses}

The virtual addresses(block addresses) consists of two parts

-   `table index` and the `offset`.
-   The physical address is constructed from the `offset` and the page frame.


#### Shared resources {#shared-resources}

{{< figure src="/ox-hugo/20221101214011-virtual_memory-487395810.png" >}}

-   A process has VMA holding information used to build its own page table
-   This then points to a shared table and so on. (See page table levels)
-   Example
    -   A process requests a for a `page`
    -   This actual `resource` mapped by this `page` may be in RAM and mapped by other processes using it
    -   The process that requested it does not have a PTE for it.
    -   It'll take a soft page fault and establish PTE and go on.


### Tread local storage {#tread-local-storage}

-   It's just storage specific to the running thread
-   Many threads can have a variable called `a`; `a` is local to each thread. One usage can be multiple threads accumulating information into a global variable with some kind of sync ofc.


### Memory Layout {#memory-layout}

See [Memory Allocation]({{< relref "20221101213324-memory_allocation.md" >}})
Compilers use a technique called escape analysis to determine if something can be allocated on the stack or must be placed on the heap.


#### Stack {#stack}

{{< figure src="/ox-hugo/stackframe1.png" >}}

credit: eli's [blog](https://eli.thegreenplace.net/2011/02/04/where-the-top-of-the-stack-is-on-x86/#id4).

[`ebp` is the](https://stackoverflow.com/questions/1395591/what-is-exactly-the-base-pointer-and-stack-pointer-to-what-do-they-point) frame pointer/base pointer and `esp` is the stack pointer. When we say "top of the stack" on x86, we actually mean the lowest address in the memory area, hence the negative offset. `esp` points to the top of the stack.

> `rbp` and `rsp` are just the 64-bit equivalents to the 32-bit [`ebp`](https://practicalmalwareanalysis.com/2012/04/03/all-about-ebp/) and `esp` variables.


#### Heap {#heap}

Similar to the stack pointer(`esp`) the top of the heap is called the **program break**. Moving the **program break** `up` allocates memory and moving it `down` deallocates memory.

-   Writing to higher addresses than the program break will result in segfaults for ovious reasons.
-   Heap fragmentation can have a substantial impact on the CPU
-   We can use [syscalls]({{< relref "20230330194208-syscalls.md" >}}) to allocate heap, helper functions such as `malloc` help us call those underlying syscalls.


## Programming language memory model {#programming-language-memory-model}


### Python {#python}

-   Every variable in python is simply a pointer to a C struct with standard Python object info at the beginning.
-   Everything in Python is allocated on the heap.
-   CPython does not make a syscall for each new object. Rather it allocates memory in bulk from the OS and then maintains an internal allocator for single-object


### Golang {#golang}

-   [Chris's Wiki blog/programming/GoProgramMemoryUse](https://utcc.utoronto.ca/~cks/space/blog/programming/GoProgramMemoryUse)


#### Memory Split {#memory-split}

-   Turns out that Go organizes memory in spans (idle, inuse, stack) (maybe outdated)


#### Stack size {#stack-size}

-   Go has is it uses its own non-C stack
-   Go defaults to 2k while the smallest C stack I know of is 64k on old OpenBSD, then macOS’s 512k for the non-main threads


## Kernel {#kernel}

-   Linux kernels greater than version 2.2 use slab pools to manage memory above the page level.
-   Frequently used objects in the Linux kernel (buffer heads, inodes, dentries, etc.)  have their own cache.  The file /proc/slabinfo gives statistics on these caches.


### Zones {#zones}

-   Hardware often poses restrictions on how different physical memory ranges can be accessed.
-   In some cases, devices cannot perform DMA to all the addressable memory. In other cases, the size of the physical memory exceeds the maximal addressable size of virtual memory etc.
-   Linux groups memory pages into zones to handle this.
-   Full list: [mmzone.h - include/linux/mmzone.h - Linux source code (v5.11-rc4) - Bootlin](https://elixir.bootlin.com/linux/v5.11-rc4/source/include/linux/mmzone.h#L348) (See code comments)
-   `ZONE_DMA, ZONE_DMA32, ZONE_NORMAL, ZONE_HIGHMEM, ZONE_MOVABLE, ZONE_DEVICE`
-   [Is it time to remove ZONE_DMA? {LWN.net}](https://lwn.net/Articles/753273/)


### Paging details specific to Linux {#paging-details-specific-to-linux}


#### Reclaim {#reclaim}

-   Reclaim : Try to free a page. Unload stuff memory. Make space for new stuff.
-   Reclaimable memory is not guaranteed
    -   We need to ask when is it reclaimable, because it's not a binary option
    -   Some page types maybe totally unreclaimable like some kernel structures.
    -   Some page in the cache might be super hot and reclaiming them makes no sense
    -   Sometimes you need to do something else before you can reclaim, eg. flush dirty pages.
-   How?
    -   `kswapd` reclaim : Background kernel thread, proactively tries reclaiming memory
    -   Direct: If `kswapd` was sufficient, we'd never need direct reclaim. Direct reclaim usually blocks application as it tries to free up memory for it.


#### Memory types {#memory-types}

There are semantics in linux that are specific to the pages that CPU is not aware of.

-   Anonymous (non file-backed) memory/pages
    -   Not backed by anything.
    -   Created for program’s stack and heap or by explicit calls to `mmap(2)` / `malloc`
    -   If memory gets tight, the best the OS can do is write it out to `swap`, then back in again. Ick.
    -   **Read Access and Write Access**
        -   When the anonymous mapping is created, it only defines virtual memory areas that the program is allowed to access.
        -   Read accesses: Will create a `PTE` that references a special physical page filled with `zeroes`.
        -   Write attempt: A regular physical page will be allocated to hold the written data. The page will be marked dirty and if the kernel decides to repurpose it, the dirty page will be swapped out.
-   Backed pages
    -   Executable on disk mapped into RAM so you can run your program.
-   Dirty backed pages
    -   Pages in the `page cache` which have been modified and are waiting to be written back to disk.
    -   If we want to reclaim a dirty page, we need to flush it to disk first.
-   Cache and buffers
    -   Pages responsible for caching data and metadata related to files accessed by those programs in order to speed up future access.


#### Huge Pages {#huge-pages}

-   Applications with large memory working set will experience performance hit because of TLB misses.
-   Even though, the page size is architecture specific and set during kernel build time. Higher level page tables have can be used to use page size of values of 2MB and 1GB in Linux. In Linux, they're called `huge`
-   It reduces pressure on the `TLB`, improves overall performance.
-   2 ways: hugetlbfs &amp; THP

<!--list-separator-->

-  HugeTLB filesystem / `hugetlbfs`

    -   A pseudo filesystem that uses RAM as its backing store

<!--list-separator-->

-  Transparent HugePages, or THP

    -   More modern, requires less configuration, kernel handles stuff
    -   THP can be massive source of bugs, weird behaviors, and total system failures
    -   Currently THP only works for anonymous memory mappings and tmpfs/shmem.


### Allocation {#allocation}

-   Linux uses both the buddy allocator and the slab allocator for different types of memory allocations in the kernel. (This is extremely generalized)
-   [The SLUB allocator {LWN.net}](https://lwn.net/Articles/229984/)
-   [SLQB - and then there were four {LWN.net}](https://lwn.net/Articles/311502/)
-   [The Slab Allocator in the Linux kernel](https://hammertux.github.io/slab-allocator) (SLOB)
-   [A memory allocator for BPF code {LWN.net}](https://lwn.net/Articles/883454/)
-   [A deep dive into CMA {LWN.net}](https://lwn.net/Articles/486301/)
-   [The slab and protected-memory allocators {LWN.net}](https://lwn.net/Articles/753154/)
-   See [Memory Allocation]({{< relref "20221101213324-memory_allocation.md" >}})


### Swap {#swap}


#### What {#what}

-   We might want to disable swap for certain/main "workloads" but we **still** want it for the whole system.
-   vm.swappiness and writeback throttling WBT(see page cache)
-   Things that swap is good at, you can't get them any other way. Having big memory makes no difference.
-   It is not for emergency memory, to me it is how vm works in linux and all the memory hie. swap is not directly related to RAM, it has a certain usecases that cannot be done without swap.
-   you can run a system without swap but it will not have the benifits that swap has to offer. People think if you don't use swap everything will happen in memory only, but it still has to to flush stuff to disk (filemapped io?) (DOUBT)
-   SWAP is related to the memory types we discussed earlier and not directly related to RAM, and provides a backing store for them(anonymous memory doesn't have a place to go back to so goes to swap).
-   You also cannot reclaim/evict anonymous memory if there is no swap!
    -   Swap allows reclaim on types of memory which otherwise be locked in memory.
    -   We're just going to throw the file cache (thrash the file cache) to make space for it
    -   Worse: We might have to reclaim/evict actual hot page to make space for it.
-   Misconception: Disabling swap will lead prevent disk io in case of memory contention
-   On SSDs, swapping out anonymous pages and reclaiming file pages are essentially equivalent in terms of performance/latency. On older spinning disks, swap reads are slower due to random reads, so a lower vm.swappiness setting makes sense there

Metrics for swap:

-   Swap Activity (swap-ins and swap outs)
-   Amount of swap space used

The kernel's aggressiveness in preemptively swapping-out pages is governed by a kernel parameter called swappiness. It can be set to a number from 0 to 100, where 0 means that more is kept in memory and 100 means that the kernel should try and swap-out as many pages as possible. The default value is 60.


#### Good {#good}

-   Gives us time for memory contention to ramp up gradually


#### Bad {#bad}

-   Can delay invocation of OOM killer


#### Swapfile vs Swap Partitions {#swapfile-vs-swap-partitions}

-   Pros/cons of Swapfile vs Swap partition?
-   No significant advantage of having a partition
-   Might as well use just a swapfile
-   using zswap/zram for swap versus file based swap
    -   Similar benefits (at similar costs) can be achieved using zswap or zram. The two are generally similar in intent although not operation: zswap operates as a compressed RAM cache and neither requires (nor permits) extensive userspace configuration, whereas zram is a kernel module which can be used to create a compressed block device in RAM. zswap works in conjunction with a swap device while zram does not require a backing swap device.


### Thrashing {#thrashing}

-   **Cache thrashing**: Multiple main memory locations competing for the same cache lines, resulting in excessive cache misses.
-   **TLB thrashing**: The translation lookaside buffer (TLB) is too small for the working set of pages, causing excessive page faults and poor performance.
-   **Heap thrashing**: Frequent garbage collection due to insufficient free memory or insufficient contiguous free memory, resulting in poor performance.
-   **Process thrashing**: Processes experience poor performance when their working sets cannot be coscheduled, causing them to be repeatedly scheduled and unscheduled.


### OOM {#oom}

-   Reactive, not proactive, based on reclaim failure
-   Linux does not know when it is out of memory. Only when it tries to reclaim memory and fails for a long time it realizes it's out of memory.
-   Because of this variability in reclaimimnng stuff and because it's not so simple it's hard to predict when we be out of memory
-   How many pages can be freed can only be known via trying to reclaim them
-   Goal is to avoid having to invoke oom killer at all and to avoid memory starvation
-   It is not aware of what it is even killing, it just kills. So if the memory congestion issue needed proccess X to be killed, OOM might just kill Y.
-   One scenario about OOMs, even if there are a lot of cache and buffers available, if the kernel decides that there are important things in these it will not evict them! Therefore you system might go out of memory even if it has stuff in cache and buffer.
-   Because of this, we need a OOM daemon(based on memory pressure) which can kill things before we actually run out of memory. oomd is one example.
    -   context aware decisons
    -   kills when killing is needed
    -   more configurable than kernal oom
    -   oomd, systemdoomd, earlyoom


### Cache {#cache}

See [Caches]({{< relref "20221101214226-caches.md" >}})
![](/ox-hugo/20221101214011-virtual_memory-438201078.png)


#### Buffer cache {#buffer-cache}

-   Sits between the FS and disk
-   Caches disk blocks in memory


#### Page cache {#page-cache}

-   Sits between the [VFS]({{< relref "20221101145053-filesystems.md#vfs" >}}) layer and the FS
-   Caches memory pages
-   Major part in I/O
    -   Whenever a file is read, the data is put into the page cache to avoid expensive disk access on the subsequent reads.
    -   When one writes to a file, the data is placed in the page cache and eventually gets into the backing storage device.
    -   See [Disk I/O]({{< relref "20230402013722-disk_i_o.md" >}})
-   No need to call into filesystem code at all if the desired page is present already.
-   [The future of the page cache {LWN.net}](https://lwn.net/Articles/712467/)


#### Unified cache {#unified-cache}

-   Ingo Molnar unified both of them in 1999.
-   Now, the `buffer cache` still exists, but its entries point into the `page cache`
-   Logic behind Page Cache is explained by Temporal locality principle, that states that recently accessed pages will be accessed again at some point in nearest future.
-   Page Cache also improves IO performance by delaying writes and coalescing adjacent reads.


#### Skipping the page cache with Direct IO (`O_DIRECT`) {#skipping-the-page-cache-with-direct-io--o-direct}

<!--list-separator-->

-  When to use it

    -   It's not generally recommended but there can be usecases. Eg. PostgreSQL uses it for `WAL`, since need to writes as fast as possible(durability) and they know this data won't be read immediately.

<!--list-separator-->

-  Block/Sector Alignment

    When using page cache, we don't need to be worrying about aligning our data, but if we doing direct IO, we better do.
    ![](/ox-hugo/20221101214011-virtual_memory-925996661.png)
    ![](/ox-hugo/20221101214011-virtual_memory-1520135453.png)


#### Controlling Page Cache {#controlling-page-cache}

-   cgroups can be used
-   `fadvice` and `madvice` sysalls


### Memory overcommit {#memory-overcommit}

-   I run all my production systems with vm.overcommit_memory = 2: why shouldn't I; or alternatively: why doesn't everyone else do this?
-   `vm.overcommit_memory` is probably the worst sysctl
-   if virtually allocated memory is greater than system memory, then we refuse to overcommit anymore


## Filesystem {#filesystem}


## Memory {#memory}


## Disk {#disk}
