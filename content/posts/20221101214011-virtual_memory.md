+++
title = "Virtual Memory"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Computer Memory]({{< relref "20221101203327-computer_memory.md" >}}), [Filesystems]({{< relref "20221101145053-filesystems.md" >}})

> All of this is linux specific


## Memory Management {#memory-management}

-   It's not exact science, everything is tradeoff between performance and accuracy.
-   Example: If you limit memory it might affect disk I/O because it'll start evicting the page cache and start using swap.
-   [Concepts overview — The Linux Kernel documentation](https://www.kernel.org/doc/html/latest/admin-guide/mm/concepts.html)


## Virtual Memory {#virtual-memory}

{{< figure src="/ox-hugo/20221101214011-virtual_memory-1392942868.png" >}}

Only the kernel uses physical memory addresses directly.  Userspace programs exclusively use virtual addresses.


### Why do we need it {#why-do-we-need-it}

-   Modern computers run multiple tasks, directly from/to physical memory is a bad idea because each program would request its own contagious block.
-   We don't have enough memory to give each program its own contagious memory, so we give it non-contagious and slap virtual memory on top of it
-   It tricks the program to thinking that it has contagious memory now
-   That too till the end of the virtual address space, not just till the end of physical memory. Big W.
-   So essentially, we hide the fragmentation from the program by using virtual memory.
-   A translation from physical to virtual address happens and vice-versa.
-   This non-contagious memory, which appears to be contagious may be spread across, disk, physical memory, cache, or may even not be physically represented etc.


### How is it implemented? {#how-is-it-implemented}

-   Segmentation
-   Page Table/Paged Virtual Memory


## Paging {#paging}

<div class="warning small-text">

> Paging is an implementation of virtual memory that the Linux kernel uses.
>
> This topic should have been under virtual memory or Kernel but it's a big enough topic so let it have its own Heading. But details will be specific to linux kernel.
</div>

See [Memory Allocation]({{< relref "20221101213324-memory_allocation.md" >}}) and [Memory Design]({{< relref "20221101202144-memory_design.md" >}}) and [Memory Hierarchy]({{< relref "20221101213401-memory_hierarchy.md" >}})


### Components {#components}


#### MMU {#mmu}

{{< figure src="/ox-hugo/20221101214011-virtual_memory-1972270768.png" >}}

<!--list-separator-->

-  Page Table and Directory

    -   In theory, MMU sits between CPU and memory but in practice, it's right there in the CPU chip itself.
    -   The MMU maps memory through a [series of tables](https://wiki.osdev.org/Paging) which are stored in RAM.
        -   Page directory (`PD`), Pointed by Page Table Base Register (`PTBR`), PTBR stored at `CR3`
        -   Page table (`PT`), Contains Page Table Entries (`PTE`)
    -   Now there are even systems which do not have a MMU, The linux memory management system for it is called `nommu`

<!--list-separator-->

-  Isolation and protection

    -   Paging is possible due to the MMU which is a hardware thing
    -   Paging provides HW level isolation ( **memory protection** )
        -   User processes can only see and modify data which is paged in on their own address space.
        -   System pages are also protected from user processes.

<!--list-separator-->

-  TLB (Translation Lookaside Buffer)

    {{< figure src="/ox-hugo/20221101214011-virtual_memory-1359457724.png" >}}

    -   Memory cache in the `MMU`
    -   If there is a TLB miss, it'll trigger a  `page walk` (Search down the page table levels to look for entry)
    -   Invalidating a TLB entry flushes the associated cache line.
    -   Stores recent translations of virtual memory to physical memory.
    -   The majority of desktop, laptop, and server processors include one or more TLBs
    -   Applications with large memory [WSS]({{< relref "20221101215651-memory_metrics.md#wss" >}}) will experience performance hit because of TLB misses.
    -   See [MMU gang wars: the TLB drive-by shootdown | Hacker News](https://news.ycombinator.com/item?id=23214535)

<!--list-separator-->

-  Page Fault Handler

    -   Page Fault Handler is a ISR/Interrupt Handler (See [Interrupts]({{< relref "20221101173720-interrupts.md" >}}))
    -   Called by MMU when we see a "page fault"
    -   3 responses
        -   Soft/Minor faults: Updates things in memory, just the PTE needs to be created.
        -   Hard/Major faults: Needs to fetch things from disk
        -   Cannot resolve: Sends a `SIGSEGV` [signals]({{< relref "20221101173527-ipc.md#signals" >}}).
    -   If a page fault happens in the ISR, then it'll be a "double fault" and kernel will panic.
    -   Triple fault? bro ded.


#### Pages {#pages}

-   Also called **virtual pages** , and **block** (see Block below)
-   Usually the allocator deals with this and not user programs directly.
-   Usually 4-64 KB in size (Huge Pages upto 2MB/1GB)
-   May or May Not be backed by a `page frame`
-   Some architectures allow selection of the page size, this can be configured at the kernel build time

<!--listend-->

```shell
λ getconf PAGESIZE
4096
```


#### Blocks {#blocks}

-   `pagesize = blocksize` (Sometime `Page` can simply refer to the `Block`)


#### Page Frame {#page-frame}

-   Also called **Frame** / Physical Memory Page
-   This is actual physical memory(RAM)
-   A page frame may back multiple pages. e.g.shared memory or memory-mapped files.


#### Page Table {#page-table}

-   [Complete virtual memory map with 4-level page tables](https://www.kernel.org/doc/html/latest/x86/x86_64/mm.html)
-   Essentially data structure that maps virtual address to physical. (unidirectional, virtual address -&gt; physical address)
-   Kernel maintains **per process page table**, kernel also has a few page table for things like disk cache.
-   Stored in the RAM.
-   Kernel may have multiple page tables, active one is the one pointed by `CR3`
-   Organized hierarchically(multi-levels) to reduce [waste in memory](https://os.phil-opp.com/paging-introduction/)
    ![](/ox-hugo/20221101214011-virtual_memory-1389536447.png)

    {{< figure src="/ox-hugo/20221101214011-virtual_memory-547895362.png" >}}

| block | page frame |
|-------|------------|
| 1     | 13         |
| 2     | 76         |
| 3     | 55         |

-   Steps in accessing memory via page table
    -   Access requested for `block 3`
    -   MMU intercepts &amp; looks up page table to find the corresponding page frame
    -   Generate a physical address, pointing to `55th page frame` in the RAM.
-   An access memory location can be
    -   An instruction
    -   Load/Store to some data.
-   Additional links
    -   [Examining Process Page Tables — The Linux Kernel documentation](https://www.kernel.org/doc/html/latest/admin-guide/mm/pagemap.html)
    -   [kernel - Viewing pagetable for a process - Unix &amp; Linux Stack Exchange](https://unix.stackexchange.com/questions/369185/viewing-pagetable-for-a-process)
    -   [dwks/pagemap: Userspace tool to map virtual page addresses](https://github.com/dwks/pagemap)


### Important Features {#important-features}


#### Demand Paging {#demand-paging}

-   This is used by both Standard IO and Memory Mapped IO. See [Disk I/O]({{< relref "20230402013722-disk_i_o.md" >}})
-   All pages are backed to disk, so starting on disk is similar to being swapped out
-   OS copies a disk page into physical memory only if an attempt is made to access it and that page is not already in memory.


#### Locking Pages {#locking-pages}

-   When a page fault occurs and kernel needs to do I/O for paging in
-   Execution time of an instruction skyrockets
-   We can Lock pages for programs that are sensitive to that. See `mlock`


## User process {#user-process}


### Virtual address space (VAS) {#virtual-address-space--vas}

The VAS of a process consist of stack, heap, code, data, arguments etc.

![](/ox-hugo/20221101132431-gc_essentials_udemy-1968463754.png)
The 3 and 1 GB split, is [only for 32bit systems](https://en.wikipedia.org/wiki/High_memory), rest of the image can be followed. For 64bit representation [see this](https://www.kernel.org/doc/html/latest/x86/x86_64/mm.html).

```c
printf("%p", &i); // shows virtual address
```

-   A process is represented by its VAS, a contagious address space from `addr0` to `MAX_SIZE`.
-   When compiling with `gcc`, the `a.out` (the executable) file contains information about how to create the virtual address space.


#### Virtual Addresses {#virtual-addresses}

The virtual addresses(block addresses) consists of two parts

-   `table index` and the `offset`.
-   The physical address is constructed from the `offset` and the page frame.


#### Shared resources {#shared-resources}

{{< figure src="/ox-hugo/20221101214011-virtual_memory-487395810.png" >}}

-   A process has VMA holding information used to build its own page table
-   This then points to a shared table and so on. (See page table levels)
-   Example
    -   A process requests a for a `page`
    -   This actual `resource` mapped by this `page` may be in RAM and mapped by other processes using it
    -   The process that requested it does not have a PTE for it.
    -   It'll take a soft page fault and establish PTE and go on.


#### Creating new process {#creating-new-process}

-   When we `fork()`, page table is shared with child w [Copy on Write]({{< relref "20230405020429-copy_on_write.md" >}}) semantics.
-   When we `exec()`, it blanks the process's current page table, discarding all
    existing mappings, and replaces them with a fresh page table containing
    a small number of new mappings
    -   An executable `mmap()` of the new file passed to the `exec()` call.
    -   env vars and command line arguments, same pid, a new process stack, and so on.
-   That's why to launch a new process in Unix-like systems, we do `fork()`, followed immediately by a call to `exec()` (`execve()`)


### Tread local storage {#tread-local-storage}

-   It's just storage specific to the running thread
-   Many threads can have a variable called `a`; `a` is local to each thread. One usage can be multiple threads accumulating information into a global variable with some kind of sync ofc.


### Memory Layout {#memory-layout}

See [Memory Allocation]({{< relref "20221101213324-memory_allocation.md" >}})
Compilers use a technique called escape analysis to determine if something can be allocated on the stack or must be placed on the heap.


#### Stack {#stack}

{{< figure src="/ox-hugo/stackframe1.png" >}}

credit: eli's [blog](https://eli.thegreenplace.net/2011/02/04/where-the-top-of-the-stack-is-on-x86/#id4).

[`ebp` is the](https://stackoverflow.com/questions/1395591/what-is-exactly-the-base-pointer-and-stack-pointer-to-what-do-they-point) frame pointer/base pointer and `esp` is the stack pointer. When we say "top of the stack" on x86, we actually mean the lowest address in the memory area, hence the negative offset. `esp` points to the top of the stack.

> `rbp` and `rsp` are just the 64-bit equivalents to the 32-bit [`ebp`](https://practicalmalwareanalysis.com/2012/04/03/all-about-ebp/) and `esp` variables.


#### Heap {#heap}

Similar to the stack pointer(`esp`) the top of the heap is called the **program break**. Moving the **program break** `up` allocates memory and moving it `down` deallocates memory.

-   Writing to higher addresses than the program break will result in segfaults for ovious reasons.
-   Heap fragmentation can have a substantial impact on the CPU
-   We can use [syscalls]({{< relref "20230330194208-syscalls.md" >}}) to allocate heap, helper functions such as `malloc` help us call those underlying syscalls.


## Programming language memory model {#programming-language-memory-model}


### Python {#python}

-   Every variable in python is simply a pointer to a C struct with standard Python object info at the beginning.
-   Everything in Python is allocated on the heap.
-   CPython does not make a syscall for each new object. Rather it allocates memory in bulk from the OS and then maintains an internal allocator for single-object


### Golang {#golang}

-   [Chris's Wiki blog/programming/GoProgramMemoryUse](https://utcc.utoronto.ca/~cks/space/blog/programming/GoProgramMemoryUse)


#### Memory Split {#memory-split}

-   Turns out that Go organizes memory in spans (idle, inuse, stack) (maybe outdated)


#### Stack size {#stack-size}

-   Go has is it uses its own non-C stack
-   Go defaults to 2k while the smallest C stack I know of is 64k on old OpenBSD, then macOS’s 512k for the non-main threads


### Other {#other}

-   [The memory models that underlie programming languages (2016) | Hacker News](https://news.ycombinator.com/item?id=27455509)
-   [The Memory Models That Underlie Programming Languages | Hacker News](https://news.ycombinator.com/item?id=13293290)
-   [The memory models that underlie programming languages](http://canonical.org/~kragen/memory-models/)


## Kernel {#kernel}


### Zones {#zones}

-   Hardware often poses restrictions on how different physical memory ranges can be accessed.
-   In some cases, devices cannot perform DMA to all the addressable memory. In other cases, the size of the physical memory exceeds the maximal addressable size of virtual memory etc.
-   Linux groups memory pages into zones to handle this.
-   Full list: [mmzone.h - include/linux/mmzone.h - Linux source code (v5.11-rc4) - Bootlin](https://elixir.bootlin.com/linux/v5.11-rc4/source/include/linux/mmzone.h#L348) (See code comments)
-   `ZONE_DMA, ZONE_DMA32, ZONE_NORMAL, ZONE_HIGHMEM, ZONE_MOVABLE, ZONE_DEVICE`
-   [Is it time to remove ZONE_DMA? {LWN.net}](https://lwn.net/Articles/753273/)


### Paging reclaim and memory types {#paging-reclaim-and-memory-types}


#### Reclaim {#reclaim}

-   Reclaim : Try to free a page. Unload stuff memory. Make space for new stuff.
-   Reclaimable memory is not guaranteed
    -   We need to ask when is it reclaimable, because it's not a binary option
    -   Some page types maybe totally unreclaimable like some kernel structures.
    -   Some page in the cache might be super hot and reclaiming them makes no sense
    -   Sometimes you need to do something else before you can reclaim, eg. flush dirty pages.
-   How?
    -   `kswapd` reclaim : Background kernel thread, proactively tries reclaiming memory
    -   Direct: If `kswapd` was sufficient, we'd never need direct reclaim. Direct reclaim usually blocks application as it tries to free up memory for it.


#### Memory types {#memory-types}

There are semantics in linux that are specific to the pages that CPU is not aware of.

-   Anonymous (non file-backed) memory/pages
    -   Not backed by anything.
    -   Created for program’s stack and heap or by explicit calls to `mmap(2)` / `malloc`
    -   If memory gets tight, the best the OS can do is write it out to `swap`, then back in again. Ick.
    -   **Read Access and Write Access**
        -   When the anonymous mapping is created, it only defines virtual memory areas that the program is allowed to access.
        -   Read accesses: Will create a `PTE` that references a special physical page filled with `zeroes`.
        -   Write attempt: A regular physical page will be allocated to hold the written data. The page will be marked dirty and if the kernel decides to repurpose it, the dirty page will be swapped out.
-   Backed pages
    -   Executable on disk mapped into RAM so you can run your program.
-   Dirty backed pages
    -   Pages in the `page cache` which have been modified and are waiting to be written back to disk.
    -   If we want to reclaim a dirty page, we need to flush it to disk first.
-   Cache and buffers
    -   Pages responsible for caching data and metadata related to files accessed by those programs in order to speed up future access.


### Huge Pages {#huge-pages}

<div class="warning small-text">

> "It's funny how this moves in circles: Automatic merging into hugepages (THP) is added to the kernel. Some workloads suffer, so it gets turned off again in some distributions (not all of course). But for many different workloads (some say the majority), THP is actually really, really beneficial, so it is ported to more and more mallocs over time. It might have been more straightforward to teach the problematic workloads to opt out of THP once the problems were discovered."
>
> -   Someone from orange site
</div>

-   Essentially larger page allocation.
-   Suppose TLB is 1MB and your program WSS is 5MB. TLB will obviously take hits. To get around this, we can use Huge Pages. It reduces pressure on the `TLB`, The CPU has a limited number of TLB entries. This allows a single TLB entry to translate addresses for a large amount of contiguous physical memory.
-   In linux 2MB and 1GB pages are supported.
-   Has tradeoffs as usual. Look them up. To have performance problems you need to use large amounts of memory with small allocations and not only that, but also free some to cause fragmentation.
    ```shell
    $  cat /sys/kernel/mm/transparent_hugepage/enabled
    [always] madvise never
    $ cat /sys/kernel/mm/hugepages/hugepages-1048576kB/nr_hugepages
    0
    ```


#### HugeTLB filesystem / `hugetlbfs` {#hugetlb-filesystem-hugetlbfs}

-   A pseudo filesystem that uses RAM as its backing store
-   Hugepages need contiguous free physical pages. Without preallocating them at boot time, with higher system uptime, chances of finding such region to satisfy the allocation are slimmer, especially for 1G pages, to the point when even services starting later at boot time might not get them due to external fragmentation caused by 4k pages allocations.
-   A database server seems like the ideal use case for (real) huge pages: a single application with a low-ish number of threads on a machine with a huge amount of RAM. Lots of page table misses with normal sized pages, but very little memory wasted from huge pages and no memory contention from other software.


#### Transparent HugePages, or THP {#transparent-hugepages-or-thp}

-   Need to be enabled for programs via `madvice()`. So that only THP aware applications can use it.
-   More modern, requires less configuration, kernel handles stuff
-   THP can be massive source of bugs, weird behaviors, and total system failures
-   **Currently THP only works for anonymous memory mappings and tmpfs/shmem**
-   [Transparent Hugepages: measuring the performance impact - The mole is digging](https://alexandrnikitin.github.io/blog/transparent-hugepages-measuring-the-performance-impact/)
-   [kcompacd0 using 100% CPU with VMware Workstation 16](https://unix.stackexchange.com/questions/679706/kcompacd0-using-100-cpu-with-vmware-workstation-16)
-   [Transparent Huge Pages: Why We Disable It for Databases | PingCAP](https://www.pingcap.com/blog/transparent-huge-pages-why-we-disable-it-for-databases/)
    ```shell
       echo never > /sys/kernel/mm/transparent_hugepage/defrag
       echo 0     > /sys/kernel/mm/transparent_hugepage/khugepaged/defrag
       echo 1     > /proc/sys/vm/compaction_proactiveness # default 20
    ```


#### More on Hugepages {#more-on-hugepages}

-   [Hugepages and databases working with abundant memory in modern servers - YouTube](https://www.youtube.com/watch?v=jTJ_X3fJ1Ik)
-   [Reliably allocating huge pages in Linux](https://mazzo.li/posts/check-huge-page.html)
-   One could just plug in Google's tcmalloc maybe <https://github.com/google/tcmalloc>. Also see `GLIBC_TUNABLES=glibc.malloc.hugetlb`
-   [Temeraire: Hugepage-Aware Allocator | tcmalloc](https://google.github.io/tcmalloc/temeraire.html)


### Allocation {#allocation}

-   Linux uses both the buddy allocator and the slab allocator for different types of memory allocations in the kernel. (This is extremely generalized)
-   [The SLUB allocator {LWN.net}](https://lwn.net/Articles/229984/)
-   [SLQB - and then there were four {LWN.net}](https://lwn.net/Articles/311502/)
-   [The Slab Allocator in the Linux kernel](https://hammertux.github.io/slab-allocator) (SLOB)
-   [A memory allocator for BPF code {LWN.net}](https://lwn.net/Articles/883454/)
-   [A deep dive into CMA {LWN.net}](https://lwn.net/Articles/486301/)
-   [The slab and protected-memory allocators {LWN.net}](https://lwn.net/Articles/753154/)
-   See [Memory Allocation]({{< relref "20221101213324-memory_allocation.md" >}})


### Swap {#swap}

<div class="warning small-text">

> swapping is not a bad thing. swap thrashing is bad - it means you need more memory.
</div>


#### What it is/ How does it help? {#what-it-is-how-does-it-help}

-   Swap provides a backing store for them(anonymous memory doesn't have a place to go back to so goes to swap).
-   We might want to disable swap for certain/main "workloads" but we **still** want it for the whole system. (Eg. [Elastic](https://www.elastic.co/guide/en/elasticsearch/reference/current/setup-configuration-memory.html), [Kubernetes](https://github.com/kubernetes/enhancements/tree/master/keps/sig-node/2400-node-swap))
-   Things that swap is good at, you can't get them any other way. Having big memory makes no difference.
-   Gives us time for memory contention to ramp up gradually
-   You also cannot reclaim/evict anonymous memory if there is no swap!
    -   Swap allows reclaim on types of memory which otherwise be locked in memory.
    -   If we want to swap out `anon` memory for something else, now(if no `swap` specified) we're just going to thrash the `page cache` to make space for it, which might be worse because we might have to reclaim/evict actual hot page to make space for it.
-   One thing to be aware of however is that, it can delay invocation of OOM killer. This is why we need things like `oomd` / `earlyoom`
-   It is what allows your computer to go into hibernation(suspend to disk)


#### What it's not {#what-it-s-not}

-   It is not for emergency memory, to me it is how vm works in linux and all the memory hie. swap is not directly related to RAM, it has a certain usecases that cannot be done without swap.
-   Not using swap does not magically make everything happen in memory. Kernel still has to flush things to disk, file backed memory as an example. See [Disk I/O]({{< relref "20230402013722-disk_i_o.md" >}})


#### Swappiness {#swappiness}

-   One of the answers to the _"memory is availbl, y r tings on swap?"_ question.
-   `vm.swappiness` : It's the kernel's aggressiveness in preemptively swapping-out pages.
    -   Range: `0-100`
    -   `0` means that more is kept in memory
    -   `100` means that the kernel should try and swap-out as many pages as possible.
    -   The default value is `60`
        ```shell
        $ sudo sysctl -a|sudo rg swapp
        vm.swappiness = 60

        ```
-   On SSDs, swapping out anonymous pages and reclaiming file pages are essentially equivalent in terms of performance/latency. On older spinning disks, swap reads are slower due to random reads, so a lower vm.swappiness setting makes sense there


#### FAQ {#faq}

<!--list-separator-->

-  Memory available, but why are things on swap then?

    -   If swapping happens, that does not necessarily mean that there is memory is under-pressure.
    -   If the kernel sees that, okay this portion is not getting used so often, i'll just put it in swap for now. So that I can make space for other stuff immediately.
    -   Eg. If your swap is 100% and memory at 60% then either
        -   Your `swappiness` value is wrong
        -   You need more swap space
        -   Both of the above.
    -   NEED MORE INFO

<!--list-separator-->

-  If excessive swap is fine, when to worry?

    -   If swap size is too big (because kernel will keep thrashing things and you'll never know)
    -   If pages are being swapped because of memory pressure

<!--list-separator-->

-  How to fine-tune swap

    -   Disabling Swap
    -   Disabling swap for individual processes (Using things like [cgroups]({{< relref "20230304201630-cgroups.md" >}}))
    -   Use `mlock` (caution! not really a good usecase)
    -   `vfs_cache_pressure`: for caching of directory and inode by swapping


#### Metrics for swap {#metrics-for-swap}

-   Swap Activity (swap-ins and swap outs)
-   Amount of swap space used


#### Swapfile vs Swap Partitions {#swapfile-vs-swap-partitions}

-   Pros/cons of Swapfile vs Swap partition?
-   No significant advantage of having a partition
-   Might as well use just a swapfile
-   <https://utcc.utoronto.ca/~cks/space/blog/linux/SwapPartitionsNoMore>
-   using zswap/zram for swap versus file based swap
    -   Similar benefits (at similar costs) can be achieved using zswap or zram. The two are generally similar in intent although not operation: zswap operates as a compressed RAM cache and neither requires (nor permits) extensive userspace configuration, whereas zram is a kernel module which can be used to create a compressed block device in RAM. zswap works in conjunction with a swap device while zram does not require a backing swap device.


### Thrashing {#thrashing}

In low memory situations, each new allocation involves stealing an in-use page from elsewhere, saving its current contents, and loading new contents. When that page is again referenced, another page must be stolen to replace it, saving the new contents and reloading the old contents. [WSS]({{< relref "20221101215651-memory_metrics.md#wss" >}}) takes a hit.

-   **Cache thrashing**: Multiple main memory locations competing for the same cache lines, resulting in excessive cache misses.
-   **TLB thrashing**: The translation lookaside buffer (TLB) is too small for the working set of pages, causing excessive page faults and poor performance.
-   **Heap thrashing**: Frequent garbage collection due to insufficient free memory or insufficient contiguous free memory, resulting in poor performance.
-   **Process thrashing**: Processes experience poor performance when their working sets cannot be coscheduled, causing them to be repeatedly scheduled and unscheduled.


### OOM {#oom}

<div class="warning small-text">

> An aircraft company discovered that it was cheaper to fly its planes with less fuel on board. The planes would be lighter and use less fuel and money was saved. On rare occasions however the amount of fuel was insufficient, and the plane would crash. This problem was solved by the engineers of the company by the development of a special OOF (out-of-fuel) mechanism. In emergency cases a passenger was selected and thrown out of the plane. (When necessary, the procedure was repeated.) A large body of theory was developed and many publications were devoted to the problem of properly selecting the victim to be ejected. Should the victim be chosen at random? Or should one choose the heaviest person? Or the oldest? Should passengers pay in order not to be ejected, so that the victim would be the poorest on board? And if for example the heaviest person was chosen, should there be a special exception in case that was the pilot? Should first class passengers be exempted? Now that the OOF mechanism existed, it would be activated every now and then, and eject passengers even when there was no fuel shortage. The engineers are still studying precisely how this malfunction is caused.
</div>

-   Reactive, not proactive, based on reclaim failure
-   Linux does not know when it is out of memory. Only when it tries to reclaim memory and fails for a long time it realizes it's out of memory.
-   Because of this variability in reclaimimnng stuff and because it's not so simple it's hard to predict when we be out of memory
-   How many pages can be freed can only be known via trying to reclaim them
-   Goal is to avoid having to invoke oom killer at all and to avoid memory starvation
-   It is not aware of what it is even killing, it just kills. So if the memory congestion issue needed proccess X to be killed, OOM might just kill Y.
-   One scenario about OOMs, even if there are a lot of cache and buffers available, if the kernel decides that there are important things in these it will not evict them! Therefore you system might go out of memory even if it has stuff in cache and buffer.
-   Because of this, we need a OOM daemon(based on memory pressure) which can kill things before we actually run out of memory. oomd is one example.
    -   context aware decisons
    -   kills when killing is needed
    -   more configurable than kernal oom
    -   oomd, systemdoomd, earlyoom
-   systemdoomd is based off facebooks oomd, uses pressure
-   earlyoom is systemoomd alternative
-   excludes certain processes from being killed so you won't have have things like losing ssh with it.


### Cache {#cache}

See [Caches]({{< relref "20221101214226-caches.md" >}})
![](/ox-hugo/20221101214011-virtual_memory-438201078.png)


#### Buffer cache {#buffer-cache}

-   Sits between the FS and disk
-   Caches disk blocks in memory


#### Page cache {#page-cache}

-   Sits between the [VFS]({{< relref "20221101145053-filesystems.md#vfs" >}}) layer and the FS
-   Caches memory pages
-   Major part in I/O
    -   Whenever a file is read, the data is put into the page cache to avoid expensive disk access on the subsequent reads.
    -   When one writes to a file, the data is placed in the page cache and eventually gets into the backing storage device.
    -   See [Disk I/O]({{< relref "20230402013722-disk_i_o.md" >}})
-   No need to call into filesystem code at all if the desired page is present already.
-   [The future of the page cache {LWN.net}](https://lwn.net/Articles/712467/)


#### Unified cache {#unified-cache}

-   Ingo Molnar unified both of them in 1999.
-   Now, the `buffer cache` still exists, but its entries point into the `page cache`
-   Logic behind Page Cache is explained by Temporal locality principle, that states that recently accessed pages will be accessed again at some point in nearest future.
-   Page Cache also improves IO performance by delaying writes and coalescing adjacent reads.


#### Skipping the page cache with Direct IO (`O_DIRECT`) {#skipping-the-page-cache-with-direct-io--o-direct}

<!--list-separator-->

-  When to use it

    -   It's not generally recommended but there can be usecases. Eg. PostgreSQL uses it for `WAL`, since need to writes as fast as possible(durability) and they know this data won't be read immediately.

<!--list-separator-->

-  Block/Sector Alignment

    When using page cache, we don't need to be worrying about aligning our data, but if we doing direct IO, we better do.
    ![](/ox-hugo/20221101214011-virtual_memory-925996661.png)
    ![](/ox-hugo/20221101214011-virtual_memory-1520135453.png)


#### Controlling Page Cache {#controlling-page-cache}

-   cgroups can be used
-   `fadvice` and `madvice` sysalls


#### Other notes {#other-notes}

-   writeback throttling


### Slab {#slab}

-   See [Memory Allocation]({{< relref "20221101213324-memory_allocation.md" >}})
-   Linux kernels greater than version 2.2 use slab pools to manage memory above the page level.
-   Frequently used objects in the Linux kernel (buffer heads, inodes, dentries, etc.)  have their own cache.  The file /proc/slabinfo gives statistics on these caches.


### Memory Fragmentation {#memory-fragmentation}

-   [Linux Kernel vs. Memory Fragmentation (Part I) | PingCAP](https://www.pingcap.com/blog/linux-kernel-vs-memory-fragmentation-1/)
-   [Linux Kernel vs. Memory Fragmentation (Part II) | PingCAP](https://www.pingcap.com/blog/linux-kernel-vs-memory-fragmentation-2/)


### Memory overcommit {#memory-overcommit}

-   People who don't understand how virtual memory works often insist on tracking the relationship between virtual and physical memory, and attempting to enforce some correspondence between them (when there isn't one), instead of controlling their programs' behavior. (copied from the great `vm.txt`)
-   `vm.overcommit_memory` can be set 0,1,2. RTFM.
-   By default, `vm.overcommit_ratio` is `50`. This more like the percentage
