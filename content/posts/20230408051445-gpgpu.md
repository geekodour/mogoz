+++
title = "GPGPU"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Floating Point]({{< relref "20221101164343-floating_point.md" >}}), [Concurrency]({{< relref "20221126204257-concurrency.md" >}}), [Flynn's Taxonomy]({{< relref "20230408161230-flynn_s_taxonomy.md" >}}), [Machine Learning]({{< relref "20230408190056-machine_learning.md" >}})


## Learning resources {#learning-resources}

-   [Are GPUs For You](http://lava.cs.virginia.edu/gpu_summary.html)
-   [GPU Programming: When, Why and How? — GPU programming: why, when and how? documentation](https://enccs.github.io/gpu-programming/)
-   <https://dl.acm.org/doi/pdf/10.1145/3570638>
-   [What is a flop? | Hacker News](https://news.ycombinator.com/item?id=37389361)
-   [Course on CUDA Programming](https://people.maths.ox.ac.uk/gilesm/cuda/)
-   [Can we 10x Rust hashmap throughput? - by Win Wang](https://wiwa.substack.com/p/can-we-10x-rust-hashmap-throughput)
-   [1. Introduction — parallel-thread-execution 8.1 documentation](https://docs.nvidia.com/cuda/parallel-thread-execution/index.html)
-   [Udacity CS344: Intro to Parallel Programming | NVIDIA Developer](https://developer.nvidia.com/udacity-cs344-intro-parallel-programming)
-   [How GPU Computing Works | GTC 2021 - YouTube](https://www.youtube.com/watch?v=3l10o0DYJXg)
-   [The CUDA Parallel Programming Model - 1. Concepts - Fang's Notebook](https://nichijou.co/cuda1/)
-   [Convolutions with cuDNN – Peter Goldsborough](http://www.goldsborough.me/cuda/ml/cudnn/c++/2017/10/01/14-37-23-convolutions_with_cudnn/)


## Performance {#performance}

-   Typically measured in floating point operations per second or `FLOPS` / `GFLOPS`
-   Good if the no. of floating point operations per memory access is high


## Floating Point support {#floating-point-support}

See [Floating Point]({{< relref "20221101164343-floating_point.md" >}})

-   GPUs support `half`, `single` and `double` precisions
-   `double` precision support on GPUs is fairly recent.
-   GPU vendors have their own things and support


## F32 {#f32}

float32 is very widely used in gaming.

-   float32 multiplication is really a 24-bit multiplication, which is about 1/2 the cost of a 32-bit multiplication. So an int32 multiplication is about 2x as expensive as a float32 multiplication.
-   On modern desktop GPUs, the difference in performance (FLOPS) between float32 and float64 is close to 4x


## Nvdia GPUs {#nvdia-gpus}

{{< figure src="/ox-hugo/20230408051445-gpgpu-464814650.png" >}}

-   Read [A history of NVidia Stream Multiprocessor](https://fabiensanglard.net/cuda/index.html)


### CUDA core {#cuda-core}

-   CUDA cores each core can only do one multiply-accumulate(MAC) on 2 FP32 values
-   eg. x += x\*y


### Tensor core {#tensor-core}

-   Tensor core can take a `4x4 FP16` matrix and multiply it by another `4x4 FP16` matrix then add either a `FP16/FP32 4x4` matrix to the resulting product and return it as a new matrix.
    ![](/ox-hugo/20230408051445-gpgpu-757935764.png)
-   Certain Tensor cores added support for `INT8` and `INT4` precision modes for quantization.
    ![](/ox-hugo/20230408051445-gpgpu-1482695540.png)
-   Now there are various architecture variants that Nvdia build upon, Like Turing Tensor, Ampere Tensor etc.

![](/ox-hugo/20230408051445-gpgpu-468904839.png)
![](/ox-hugo/20230408051445-gpgpu-806194578.png)
See [Category:Nvidia microarchitectures - Wikipedia](https://en.wikipedia.org/wiki/Category:Nvidia_microarchitectures)


### RAM {#ram}

???


### VRAM {#vram}

-   Memory = how big the model is allowed to be


## Frameworks {#frameworks}

-   OpenCL: Dominant open GPGPU computing language
-   OpenAI Titron: Language and compiler for parallel programming
-   CUDA: Dominant proprietary framework


## More on CUDA {#more-on-cuda}

-   Graphic cards support upto certain cuda version. Eg. my card when `nvidia-smi` is run shows CUDA 12.1, it doesn't mean cuda is installed
-   So I can install cudatoolkit around that version.
-   But cudatoolkit is separate from nvdia driver. You can possibly run cudatoolkit for your graphic card without having the driver.


### Pytorch {#pytorch}

-   Eg. To run Pytorch you don't need cudatoolkit because they ship their own CUDA runtime and math libs.
-   Local CUDA toolkit will be used if we build PyTorch from source etc.
-   If pytorch-cuda is built w cuda11.7, you need cuda11.7 installed in your machine. Does it not ship the runtime????
-   `nvcc` is the cuda compiler
-   torhaudio: <https://pytorch.org/audio/main/installation.html>
