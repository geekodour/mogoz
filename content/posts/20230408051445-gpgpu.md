+++
title = "GPGPU"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Floating Point]({{< relref "20221101164343-floating_point.md" >}}), [Concurrency]({{< relref "20221126204257-concurrency.md" >}}), [Flynn's Taxonomy]({{< relref "20230408161230-flynn_s_taxonomy.md" >}}), [Machine Learning]({{< relref "20230408190056-machine_learning.md" >}})

Must read: [Are GPUs For You](http://lava.cs.virginia.edu/gpu_summary.html)


## Performance {#performance}

-   Typically measured in floating point operations per second or `FLOPS` / `GFLOPS`
-   Good if the no. of floating point operations per memory access is high


## Floating Point support {#floating-point-support}

See [Floating Point]({{< relref "20221101164343-floating_point.md" >}})

-   GPUs support `half`, `single` and `double` precisions
-   `double` precision support on GPUs is fairly recent.
-   GPU vendors have their own things and support


## F32 {#f32}

float32 is very widely used in gaming.

-   float32 multiplication is really a 24-bit multiplication, which is about 1/2 the cost of a 32-bit multiplication. So an int32 multiplication is about 2x as expensive as a float32 multiplication.
-   On modern desktop GPUs, the difference in performance (FLOPS) between float32 and float64 is close to 4x


## Nvdia GPUs {#nvdia-gpus}


### CUDA core {#cuda-core}

-   CUDA cores each core can only do one multiply-accumulate(MAC) on 2 FP32 values
-   eg. x += x\*y


### Tensor core {#tensor-core}

-   Tensor core can take a `4x4 FP16` matrix and multiply it by another `4x4 FP16` matrix then add either a `FP16/FP32 4x4` matrix to the resulting product and return it as a new matrix.
    ![](/ox-hugo/20230408051445-gpgpu-757935764.png)
-   Certain Tensor cores added support for `INT8` and `INT4` precision modes for quantization.
    ![](/ox-hugo/20230408051445-gpgpu-1482695540.png)
-   Now there are various architecture variants that Nvdia build upon, Like Turing Tensor, Ampere Tensor etc.

![](/ox-hugo/20230408051445-gpgpu-468904839.png)
![](/ox-hugo/20230408051445-gpgpu-806194578.png)
See [Category:Nvidia microarchitectures - Wikipedia](https://en.wikipedia.org/wiki/Category:Nvidia_microarchitectures)


### RAM {#ram}

???


### VRAM {#vram}

-   Memory = how big the model is allowed to be


## Frameworks {#frameworks}

-   OpenCL: Dominant open GPGPU computing language
-   OpenAI Titron: Language and compiler for parallel programming
-   CUDA: Dominant proprietary framework
