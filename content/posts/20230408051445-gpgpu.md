+++
title = "GPGPU"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Floating Point]({{< relref "20221101164343-floating_point.md" >}}), [Concurrency]({{< relref "20221126204257-concurrency.md" >}}), [Flynn's Taxonomy]({{< relref "20230408161230-flynn_s_taxonomy.md" >}}), [Machine Learning]({{< relref "20230408190056-machine_learning.md" >}})


## Performance {#performance}

-   Typically measured in floating point operations per second or `FLOPS` / `GFLOPS`
-   Good if the no. of floating point operations per memory access is high


## Floating Point support {#floating-point-support}

See [Floating Point]({{< relref "20221101164343-floating_point.md" >}})

-   GPUs support `half`, `single` and `double` precisions
-   `double` precision support on GPUs is fairly recent.
-   GPU vendors have their own things and support


## Nvdia GPUs {#nvdia-gpus}


### CUDA core {#cuda-core}

-   CUDA cores each core can only do one multiply-accumulate(MAC) on 2 FP32 values
-   eg. x += x\*y


### Tensor core {#tensor-core}

-   Tensor core can take a `4x4 FP16` matrix and multiply it by another `4x4 FP16` matrix then add either a `FP16/FP32 4x4` matrix to the resulting product and return it as a new matrix.
    ![](/ox-hugo/20230408051445-gpgpu-757935764.png)
-   Certain Tensor cores added support for `INT8` and `INT4` precision modes for quantization.
    ![](/ox-hugo/20230408051445-gpgpu-1482695540.png)
-   Now there are various architecture variants that Nvdia build upon, Like Turing Tensor, Ampere Tensor etc.

![](/ox-hugo/20230408051445-gpgpu-468904839.png)
![](/ox-hugo/20230408051445-gpgpu-806194578.png)
See [Category:Nvidia microarchitectures - Wikipedia](https://en.wikipedia.org/wiki/Category:Nvidia_microarchitectures)


### RAM {#ram}

???


### VRAM {#vram}

-   Memory = how big the model is allowed to be


## Frameworks {#frameworks}


### OpenCL {#opencl}

-   Dominant open GPGPU computing language


### CUDA {#cuda}

-   2006
-   Dominant proprietary framework
