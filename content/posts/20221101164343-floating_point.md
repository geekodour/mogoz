+++
title = "Floating Point"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Math]({{< relref "20221101134840-math.md" >}}), [Fixed Point]({{< relref "20230408081425-fixed_point.md" >}})

<div class="warning small-text">

> The term floating point refers to the fact that the number's radix point can "float" anywhere to the left, right, or between the significant digits of the number. This position is indicated by the exponent, so floating-point can be considered a form of scientific notation.
</div>


## Introduction {#introduction}

FP are just scientific notion


### Advantage {#advantage}

-   Speed: Commonly [measured](https://electronics.stackexchange.com/questions/22410/how-does-division-occur-in-our-computers/421366#421366) in terms of `FLOPS`. (Okay THIS MIGHT BE WRONG)
    ```markdown
    # int
    Integer add: 1 cycle
    32-bit integer multiply: 10 cycles
    64-bit integer multiply: 20 cycles
    32-bit integer divide: 69 cycles
    64-bit integer divide: 133 cycles
    # float
    Floating point add: 4 cycles
    Floating point multiply: 7 cycles
    Double precision multiply: 8 cycles
    Floating point divide: 23 cycles
    Double precision divide: 36 cycles

    source: http://www.phys.ufl.edu/~coldwell/MultiplePrecision/fpvsintmult.htm
    ```
-   Efficiency : Can deal with really big and small numbers without needing large amount of space.


### How {#how}


#### Without FPU {#without-fpu}

A floating-point unit(FPU) is a part of the processor specifically designed to carry out floating-point numbers ops.


#### With FPU {#with-fpu}


### Bases {#bases}

-   In practice, most floating-point numbers use `base 2`, though `base10` (decimal floating point) is also common, there are also other bases used for FP.
-   **The base determines the fractions that can be represented**
    -   `1/5` cannot be represented exactly as a floating-point number using a `binary` base
    -   `1/5` can be represented exactly using a `decimal` base (0.2, or 2Ã—10<sup>-1</sup>)
    -   `1/3` cannot be represented exactly using `binary` or `decimal` base
    -   `1/3` can be represented exactly using base 3 (0.1, or 1Ã—3<sup>-1</sup>)


## Anatomy {#anatomy}

> A floating-point format is specified by
>
> -   Base (radix): `b`
> -   Precision: `p` (significand, i think)
> -   Exponent range: `emin` to `emax`, w `emin = 1 âˆ’ emax` for all IEEE 754


### Significand {#significand}

A signed (+/-) `digit string` of a given `length` in a given `base(radix)`.

-   This digit string is referred to as the `significand`, `mantissa`, or `coefficient`.
-   The `radix(base)` point position is always somewhere within the `significand`
-   The length of the `significand determines the precision` to which numbers can be represented.
-   Eg. `p=24`, `b=2`, single-precision(32bit) : `significand` will be `string` of 24 bits. So precision will be till 24bits.


### Exponent {#exponent}

-   A signed integer exponent (also referred to as the `characteristic`, or `scale`)
-   Modifies the magnitude of the number.
    -   eg. -5 is smaller than 3 but greater in magnitude than 3 (-5+3=-2)
-   The exponent shifts the radix point in the significand and changes the magnitude


## Rounding and Error {#rounding-and-error}


### Associativity and Commutativity {#associativity-and-commutativity}

It's not associative, not commutative

```text
octave:1> x=0.1+0.2+0.3
x = 0.60000
octave:2> y=0.3+0.2+0.1
y = 0.60000
octave:3> x==y
ans = 0
octave:4> x-y
ans = 1.1102e-16
```


## Deterministic? {#deterministic}

-   They're deterministic but partially implementation defined.
-   Some instructions don't guarantee the maximum possible precision, and implementations can differ in the least significant bits of their result.
-   There are minor differences between hardware (x87 vs SSE is the most famous one, but there are others). Changing compiler, its version or options may produce subtly different results (the most obvious example is the -ffast-math flag). And even the bigger problem is implementation of non-primitive (e.g. trigonometric) functions. Usually your program will use implementation from a system or vendor library, which probably have different underlying implementations.


## Precision (based on IEEE 754) {#precision--based-on-ieee-754}

{{< figure src="/ox-hugo/20221101164343-floating_point-1244518851.png" >}}


### 16-bit: Half (binary16) {#16-bit-half--binary16}

{{< figure src="/ox-hugo/20221101164343-floating_point-1310443196.png" >}}


### 32-bit: Single (binary32) {#32-bit-single--binary32}

{{< figure src="/ox-hugo/20221101164343-floating_point-1946642259.png" >}}


### 64-bit: Double (binary64) {#64-bit-double--binary64}

{{< figure src="/ox-hugo/20221101164343-floating_point-1260406825.png" >}}

-   Can represent about 15 decimal digits of precision, enough to describe any position in the solar system with millimeter precision.


### Extended precision {#extended-precision}

{{< figure src="/ox-hugo/20221101164343-floating_point-2130532546.png" >}}

-   The x86 extended precision format is an 80-bit format
-   `long double` in `C` is 80bits


### 128-bit: Quadruple (binary128) {#128-bit-quadruple--binary128}

{{< figure src="/ox-hugo/20221101164343-floating_point-1609288247.png" >}}

-   This has no hardware support
-   `__float128` lacks hardware support, hence is [slower than](https://stackoverflow.com/questions/27771354/performance-benchmark-of-float128-type) double.
-   Many space trajectory calculations use quadruple precision arithmetic, and most galactic evolution research considers statistical distributions rather than say something precise about the future state of the galaxy.


### Arbitrary precision {#arbitrary-precision}

-   [Arbitrary-precision arithmetic](https://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic)
-   Implementations of much larger numeric types (w a storage count that usually is not a power of two) using special software
-   The `dc` and `bc` programs are arbitrary precision calculators
-   [Javascript]({{< relref "20221126085225-javascript.md" >}}) uses arbitrary precision for `BigInt`
-   IEEE754 does not require correctly rounded mathematical functions, it only recommends them. So, the accuracies vary from one mathematical library to another


### When deciding what to use {#when-deciding-what-to-use}

For the same cost of doing a single float64 multiplication, you can do four float32 multiplications. So in practice, people choose the smallest data size that is good enough to work.


## Floating Point and Processors {#floating-point-and-processors}


### No FPU {#no-fpu}


### What happens when certain precision is not supported by the CPU? {#what-happens-when-certain-precision-is-not-supported-by-the-cpu}

-   The quad precision software floating point should speed up slightly, but it's still implemented in scalar integer arithmetic.


### x87 co-processor {#x87-co-processor}


## Languages usage {#languages-usage}


### Javascript {#javascript}

See [Javascript]({{< relref "20221126085225-javascript.md" >}})
JS has just 2 number types


#### `number` {#number}

Uses `double-precision 64-bit binary format IEEE 754` or `binary64`.

-   Safely represents between -(2<sup>53</sup> âˆ’ 1) and 2<sup>53</sup> âˆ’ 1 without loss of precision.
-   `Number.MAX_VALUE` : Largest number possible to represent
-   `Number.MAX_SAFE_INTEGER` : Largest integer to be used safely in calculations.
-   `Number.MIN_SAFE_INTEGER` : Smallest integer to be used safely in calculations.


#### `BigInt` {#bigint}

-   Numeric primitive in JavaScript that can represent integers with [arbitrary precision](https://en.wikipedia.org/wiki/Arbitrary-precision_arithmetic).
-   Makes it possible to correctly perform integer arithmetic without overflowing.


## Resources {#resources}


### Basics {#basics}

-   [Float Exposed](https://float.exposed/0x0024) ðŸŒŸ (Also accompanied blogpost)
-   [Floating Point Numbers - YouTube](https://www.youtube.com/watch?v=gc1Nl3mmCuY&t=326s)
-   [Float Toy | Hacker News](https://news.ycombinator.com/item?id=22113485)
-   [Floating Point Math](https://0.30000000000000004.com/) ðŸŒŸ | [Why does 0.1 + 0.2 = 0.30000000000000004?](https://jvns.ca/blog/2023/02/08/why-does-0-1-plus-0-2-equal-0-30000000000000004/)
-   [Fundamentals of Data Representation: Floating point numbers](https://en.wikibooks.org/wiki/A-level_Computing/AQA/Paper_2/Fundamentals_of_data_representation/Floating_point_numbers)
-   [What Every Programmer Should Know About FP numbers](https://floating-point-gui.de/)
-   [Binary representation of the floating-point numbers | Trekhleb](https://trekhleb.dev/blog/2021/binary-floating-point/)
-   [Floating Point Visually Explained](https://fabiensanglard.net/floating_point_visually_explained/)


#### Wikipedia {#wikipedia}

-   [Floating-point arithmetic - Wikipedia](https://en.wikipedia.org/wiki/Floating-point_arithmetic)
-   [Significand - Wikipedia](https://en.wikipedia.org/wiki/Significand)
-   [Floating-point arithmetic - Wikipedia](https://en.wikipedia.org/wiki/Floating-point_arithmetic)


#### Videos {#videos}

-   [Representation of Floating Point Numbers - 1 - YouTube](https://www.youtube.com/watch?v=ji3SfClm8TU)
-   [Numbers in a computer-(Unsigned Integers)-Part 1 of 5 - YouTube](https://www.youtube.com/watch?v=HhtecBhM_oA&list=PLD71F13843965439D&index=2)


### Intermediate {#intermediate}

-   [Examples of floating point problems](https://jvns.ca/blog/2023/01/13/examples-of-floating-point-problems/) ðŸŒŸ
-   [Supporting half-precision floats is really annoying (2021) | Hacker News](https://news.ycombinator.com/item?id=34396925)
-   [How many floating-point numbers are in the interval {0,1}? â€“ Daniel Lemire's ...](https://lemire.me/blog/2017/02/28/how-many-floating-point-numbers-are-in-the-interval-01/)
-   [You can use floating-point numbers for money (evanjones.ca)](https://www.evanjones.ca/floating-point-money.html)
-   [Ordering Numbers, How Hard Can It Be? | orlp.net](https://orlp.net/blog/ordering-numbers/)
-   [Formatting floating point numbers](https://www.zverovich.net/2019/02/11/formatting-floating-point-numbers.html)


#### Precision {#precision}

-   [Half-Precision Floating-Point, Visualized / Ricky Reusser / Observable](https://observablehq.com/@rreusser/half-precision-floating-point-visualized)
-   [Demystifying Floating Point Precision Â« The blog at the bottom of the sea](https://blog.demofox.org/2017/11/21/floating-point-precision/)


#### Error {#error}

-   [Taming Floating Point Error](https://web.archive.org/web/20230221103731/https://www.johnbcoughlin.com/posts/floating-point-axiom/)
