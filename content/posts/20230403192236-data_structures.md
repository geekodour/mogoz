+++
title = "Data Structures"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Algorithms]({{< relref "20230205172402-algorithms.md" >}}), [Bit Shifting]({{< relref "20230403194317-bit_shifting.md" >}})


## Abstract data types {#abstract-data-types}

-   When we start questioning "is this a datastructure or an algorithm?"
-   These are defined by what you can do with it; what operations it supports.
-   Well, it uses a [Data Structure]({{< relref "20230403192236-data_structures.md" >}}) and uses an [Algorithm]({{< relref "20230205172402-algorithms.md" >}}) on top of the data structure.


## Maps {#maps}


### Hashmap {#hashmap}


#### Requirement {#requirement}

-   Stability: Given the same key, your hash function must return the same answer.
-   Distribution: Given two near identical keys, the result should be [wildly different](https://en.wikipedia.org/wiki/Collision_resistance).
-   Load factor: Once the number of entries across each bucket passes some percentage of their total size(load factor)
    -   The map will grow by doubling the number of buckets
    -   Redistributing the entries across them.


#### Implementation {#implementation}

-   Implemented using array/linked list
-   An [array of buckets](https://dave.cheney.net/2018/05/29/how-the-go-runtime-implements-maps-efficiently-without-generics) each of which contains a pointer to an array of key/value entries.
-   Entries are added to a map, assuming a good hash function distribution
-   Hash(Key) = Hashed_Key =&gt; mask it &amp; get bottom few bits to get bucket offset

{{< figure src="/ox-hugo/20230403192236-data_structures-476352458.png" >}}


#### Resources {#resources}

-   [Faster hash table probing](https://outerproduct.net/trivial/2022-10-06_hash.html)


#### Worst case {#worst-case}

-   WC can be caused by bad distribution/collision. Eg. All key resolutions point to the same bucket, now for the specific bucket it'll be linear search so if everything gets dumped into the same bucket, you essentially have a linked list, i.e \\(O(n)\\) running time for lookup.


### Merklizing the key/value store {#merklizing-the-key-value-store}

-   Read [Merklizing the key/value store for fun and profit | Joel Gustafson](https://joelgustafson.com/posts/2023-05-04/merklizing-the-key-value-store-for-fun-and-profit)


### Maps ADT {#maps-adt}


#### LRU {#lru}

-   Usually used for caching. It's not exactly a FIFO because we update the position of things based on when they are accessed.
-   We need fast lookup: Hashmap
-   Once a value is fetched from the cache, we need to move the `fetched value` to the `front` of the list. So the end of the list will have the `lru` value.
-   It's like a **combination of hashmap + linked list**. The `value` points to actual nodes directly. Therefore we get \\(O(1)\\) for
    -   lookup/get : `moving from position to front` and return `value`
    -   update
        -   `insertion-at-front` (if new key)
        -   `moving from position to front` (if updating existing key)
        -   Even though update technically doesn't mean they fetched the value, but they touched it. Our logic says, if its touched it's `used`.
    -   expand_cache: `deletion-at-end` (remove lru)

<!--list-separator-->

-  Implementation

    {{< figure src="/ox-hugo/20230403192236-data_structures-1250548971.png" >}}

    -   Eviction
        -   When over capacity, we need to evict things.
        -   Since eviction happens without _user input_. We need a way to know what `key` we have for our `value`. For this we need a `reverseLookup` map, then we can also delete the original `key` from the `lookup` map.


### Bloom filters {#bloom-filters}

A fast index to tell if a value is probably in a table or certainly isn't.


## Lists {#lists}


### Array {#array}

-   Pedantic definition here
-   `capacity` must defined &amp; allocated before using, otherwise cannot form array
-   Cannot expand
-   Good for random access


### Linked list {#linked-list}

<div class="warning small-text">

> Technically, every linked list is a tree, which means every linked list is a graph.
</div>

-   Node based data structure
-   Singly linked, Doubly linked
-   Better control over memory than array, you could create a object pool etc.
-   You **can't do a binary search on a linked list**, you must traverse. (I mean you can do but it'll not be efficient for obvious reasons). So usually bad choice if you need random access.

{{< figure src="/ox-hugo/20230403192236-data_structures-1436879991.png" >}}


### Dynamic Array/Array List {#dynamic-array-array-list}

-   Rapper names: grow-able array, array list
-   Implementations: Go Slices, Java ArrayList, Python's list, Javascript []
-   You get: Random access. Well suited for stack likes (LIFO)
-   `insert/remove` at the end/tail is \\(O(1)\\), good stuff
-   `insert/remove` at the start/head is \\(O(n)\\), pretty bad
-   Optimal `stack` implementation is possible, Optimal `queue` implementation [is not possible](https://stackoverflow.com/questions/41665425/why-arraylist-doesnt-implements-queue).
    -   i.e Implementing a `queue` in Javascript with `.push` and `.shift` is probably not the best idea. But there [have been improvements](https://bugzilla.mozilla.org/show_bug.cgi?id=1348772#c7) in `.shift` [recently](https://codereview.stackexchange.com/questions/255698/queue-with-o1-enqueue-and-dequeue-with-js-arrays).
-   Whenever there is no room for appending a new element
    -   It creates a larger array
    -   Copies the contents of the current array to it
    -   Deletes the current array
    -   Sets the newly created array as current
    -   Finally, appends the element normally.


### Array Buffer / Ring Buffer / Circular buffer {#array-buffer-ring-buffer-circular-buffer}

{{< figure src="/ox-hugo/20230403192236-data_structures-5888242.png" >}}

-   No fixed head or tail. This also grows like Dynamic arrays but at the same time maintains order.
-   No shifting of other elements in the buffer needed at insertion/removal unlike dynamic array/arraylist. So, Well suited for queue like interfaces (FIFO)
-   You can implement things like queue/double ended queue(deque)/circular queue etc. w it.


### List ADTs {#list-adts}


#### Queue {#queue}

-   Can be implemented on top of a Linked List data structure.
    -   It constrains what you **can do** with a linked list.
    -   We don't even need a doubly linked list to implement a basic queue.
-   FIFO
    -   The algorithm a queue implements is FIFO.
    -   FIFO is bad for temporal locality
-   Interface
    -   `insert-at-end` : \\(O(1)\\) (aka `enqueue`)
    -   `remove-at-front` : \\(O(1)\\) (aka `dequeue`)


#### Stack {#stack}

{{< figure src="/ox-hugo/20230403192236-data_structures-1882101605.png" >}}

-   LIFO
    -   LIFO is good for temporal locality
-   Good to think of arrows pointing backwards from the head. So usually just `prev` on a Node works.
-   Interface
    -   `insert-at-end` : \\(O(1)\\) (aka `push`)
    -   `remove-at-end` : \\(O(1)\\) (aka `pop`)


#### Deque (double ended queue) {#deque--double-ended-queue}

-   Not to be confused w the `dequeue` operation. (Spelling is different! `ue`)
-   This is an abstract data type, Java has an [implementation](https://docs.oracle.com/javase/7/docs/api/java/util/ArrayDeque.html) of this [interface](https://docs.oracle.com/javase/7/docs/api/java/util/Deque.html). It can be implemented via Linked list/or some custom implementation like Java does with `ArrayDeque`.
-   `insert-at-end` : \\(O(1)\\) (aka `enqueue` / `push`)
-   `remove-at-end` : \\(O(1)\\) (aka `pop`)
-   `remove-at-front` : \\(O(1)\\) (aka `dequeue`)
-   `insert-at-front` : \\(O(1)\\)

<div class="warning small-text">

> In Java, `ArrayDeque` doesn't have
>
> -   The overhead of node allocations that [LinkedList](https://docs.oracle.com/javase/7/docs/api/java/util/LinkedList.html) does
> -   The overhead of shifting the array contents left on remove that [ArrayList](https://docs.oracle.com/javase/7/docs/api/java/util/ArrayList.html) has.
> -   So eg. in Java, if you had to implement a `Queue`, you'd go with `ArrayDeque` as the underlying data structure rather than `LinkedList` or `ArrayList`.
> -   Summary: [It's pretty good.](https://stackoverflow.com/a/6129967)
</div>


## Trees {#trees}

> Technically, Trees are just linked-lists with multiple paths.


### Basic definitions {#basic-definitions}

-   Root: the most parent node. The First. Adam.
-   Height: The longest path from the root to the most child node
-   Leaves: a node without children
-   Branching factor: the amount of children a tree has
-   General tree: A tree with 0 or more children
-   Binary tree
    -   A tree in which has at most 2 children, at least 0 children
    -   Each level in a binary tree is approx. half the size of the entire tree above it.


### Tree ADTs {#tree-adts}


#### Binary search tree (BST) {#binary-search-tree--bst}

{{< figure src="/ox-hugo/20230403192236-data_structures-1245268333.png" >}}

-   Rule at every node: Left &le; Node &lt; Right (Similar to Quicksort)
-   A tree in which has a specific ordering to the nodes and at most 2 children
-   In-order traversal will result in an ordered list

<!--list-separator-->

-  Search

    -   Searching: Searching is similar to Binary search
    -   Time Complexity: \\(O(log(n))\\) - \\(O(n)\\). \\(O(n)\\) in the case where the BST is simply a linked list. i.e How **balanced** your BST will determine the complexity.

<!--list-separator-->

-  Insertion

    -   Insertion inherently un-balances the tree

<!--list-separator-->

-  Deletion

    -   Choose `smallest on large-side` or `largest on small-side`.
        -   We can choose any but if we have the `individual height` of the node, we can make a better decision as we'll know which one to choose and shrink our tree.
    -   Then replace the parent to be removed with it.


#### Heap/Priority Queue {#heap-priority-queue}

-   Heap is an implementation of a Priority Queue ADT
-   There's usually no need to traversing the tree as you only look at the top for priority
-   A binary tree where either:
    -   every child and grand child is smaller (MaxHeap) than current node
    -   every child and grand child is larger (MinHeap) than current node
    -   i.e Root node is at some extreme
-   Self balancing
    -   Every level of the tree is complete
    -   Whenever a node is added, we must adjust the tree
    -   Whenever a node is deleted, we must adjust the tree

<!--list-separator-->

-  Array Implementation

    {{< figure src="/ox-hugo/20230403192236-data_structures-906634527.png" >}}


#### Tries/Prefix Tree/Re'trie'val tree {#tries-prefix-tree-re-trie-val-tree}

-   Used for auto-completion/caching type stuff
-   Value is usually a string

<!--list-separator-->

-  Complexity

    -   You need to think about what `n` is.
    -   The complexity of creating a trie is O(W\*L), where W is the number of words, and L is an average length of the word: you need to perform L lookups on the average for each of the W words in the set.
    -   Same goes for looking up words later: you perform L steps for each of the W words.
        -   If the longest english letter is 12, then it's 12 node check. That's constant time.
        -   In this case `n` is the height and the height is bound by the en dictionary.
        -   So trie is good for cases where you can determine the height in advance


#### B-tree {#b-tree}

-   [Binary Search Tree(BST) vs B-tree](https://www.reddit.com/r/compsci/comments/m7wb6l/what_are_the_important_types_of_tree_data/)
-   [Relational database on top of key-value store explained (or why B-trees are cool) - Gregory Trubetskoy](https://grisha.org/blog/2013/05/11/relational-database-on-top-of-key-value-store-explained/)
-   [The Taming of the B-Trees - ScyllaDB](https://www.scylladb.com/2021/11/23/the-taming-of-the-b-trees/)


#### Balanced tree {#balanced-tree}

-   perfectly balanced when any node’s left and right children have the same height.
-   There are different ways to balance a tree. Popular rotation techniques are AVL and RB trees.
    -   RB : If I don't find things often but I insert a lot. (Fast balancing)
    -   AVL: If I find a lot but insert rarely. (Slower balancing)


#### Skip List {#skip-list}


## Graphs {#graphs}

-   See [Graphs]({{< relref "20230521010631-graphs.md" >}})
