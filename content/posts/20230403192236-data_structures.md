+++
title = "Data Structures"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Algorithms]({{< relref "20230205172402-algorithms.md" >}}), [Bit Shifting]({{< relref "20230403194317-bit_shifting.md" >}})

![](/ox-hugo/20230403192236-data_structures-704171477.png)
image found on twitter^ (gkcs_)


## Abstract data types {#abstract-data-types}

-   When we start questioning "is this a datastructure or an algorithm?"
-   These are defined by what you can do with it; what operations it supports.
-   Well, it uses a [Data Structure]({{< relref "20230403192236-data_structures.md" >}}) and uses an [Algorithm]({{< relref "20230205172402-algorithms.md" >}}) on top of the data structure.


## Maps {#maps}


### Hashmap/Hashtable {#hashmap-hashtable}


#### Requirement {#requirement}

-   Stability: Given the same key, your hash function must return the same answer.
-   Distribution: Given two near identical keys, the result should be [wildly different](https://en.wikipedia.org/wiki/Collision_resistance).
-   Load factor: Once the number of entries across each bucket passes some percentage of their total size(load factor)
    -   The map will grow by doubling the number of buckets
    -   Redistributing the entries across them.


#### Implementation {#implementation}

{{< figure src="/ox-hugo/20230403192236-data_structures-1303424842.png" >}}

<!--list-separator-->

-  Chaining

    -   Implemented using array/linked list
    -   An [array of buckets](https://dave.cheney.net/2018/05/29/how-the-go-runtime-implements-maps-efficiently-without-generics) each of which contains a pointer to an array of key/value entries.
    -   Entries are added to a map, assuming a good hash function distribution
    -   Hash(Key) = Hashed_Key =&gt; mask it &amp; get bottom few bits to get bucket offset

    {{< figure src="/ox-hugo/20230403192236-data_structures-476352458.png" >}}

<!--list-separator-->

-  Open Addressing

    -   No hash buckets


#### Worst case {#worst-case}

-   WC can be caused by bad distribution/collision. Eg. All key resolutions point to the same bucket, now for the specific bucket it'll be linear search so if everything gets dumped into the same bucket, you essentially have a linked list, i.e \\(O(n)\\) running time for lookup.


#### Resources {#resources}

-   [Faster hash table probing](https://outerproduct.net/trivial/2022-10-06_hash.html)
-   [Hash Tables](https://maksimkita.com/blog/hash_tables.html)
-   <https://twitter.com/FilasienoF/status/1717419319640883225>


### Merklizing the key/value store {#merklizing-the-key-value-store}

-   Read [Merklizing the key/value store for fun and profit | Joel Gustafson](https://joelgustafson.com/posts/2023-05-04/merklizing-the-key-value-store-for-fun-and-profit)


### Bloom filters {#bloom-filters}

A fast index to tell if a value is probably in a table or certainly isn't.


### Maps ADTs ðŸ‘€ {#maps-adts}


#### LRU {#lru}

-   Usually used for caching. It's not exactly a FIFO because we update the position of things based on when they are accessed.
-   We need fast lookup: Hashmap
-   Once a value is fetched from the cache, we need to move the `fetched value` to the `front` of the list. So the end of the list will have the `lru` value.
-   It's like a **combination of hashmap + linked list**. The `value` points to actual nodes directly. Therefore we get \\(O(1)\\) for
    -   lookup/get : `moving from position to front` and return `value`
    -   update
        -   `insertion-at-front` (if new key)
        -   `moving from position to front` (if updating existing key)
        -   Even though update technically doesn't mean they fetched the value, but they touched it. Our logic says, if its touched it's `used`.
    -   expand_cache: `deletion-at-end` (remove lru)

<!--list-separator-->

-  Implementation

    {{< figure src="/ox-hugo/20230403192236-data_structures-1250548971.png" >}}

    -   Eviction
        -   When over capacity, we need to evict things.
        -   Since eviction happens without _user input_. We need a way to know what `key` we have for our `value`. For this we need a `reverseLookup` map, then we can also delete the original `key` from the `lookup` map.


## Lists {#lists}


### Array {#array}

-   Pedantic definition here
-   `capacity` must defined &amp; allocated before using, otherwise cannot form array
-   Cannot expand
-   Good for random access


### Linked list {#linked-list}

<div class="warning small-text">

> Technically, every linked list is a tree, which means every linked list is a graph.
</div>

-   Node based data structure
-   Singly linked, Doubly linked
-   Better control over memory than array, you could create a object pool etc.
-   You **can't do a binary search on a linked list**, you must traverse. (I mean you can do but it'll not be efficient for obvious reasons). So usually bad choice if you need random access.

{{< figure src="/ox-hugo/20230403192236-data_structures-1436879991.png" >}}


#### Linked List BAD for modern CPUs! {#linked-list-bad-for-modern-cpus}

> From a reddit post:
>
> When linked lists were first discussed in algorithms books in the 1970's, processor speeds were roughly comparable to the speed of accessing main memory. So there wasn't a substantial difference in cost between accessing the next element of an array vs the cost of following a pointer to the next element in a linked list.
>
> But on modern computers the difference is staggering: the cpu can execute hundreds or even thousands of instructions in the time it takes to access a random address in memory. (And accessing the disk is thousands of times slower than that.)
>
> To make up for this difference, for decades processors have been using caches: the L1 cache is built into the processor and is extremely fast but relatively small, the L2 cache is larger and slightly slower to access, the L3 cache is even larger and even slower.
>
> The caches do a lot to mitigate the cost of accessing memory, but they work best when you're accessing contiguous blocks of memory.
>
> And that's why linked lists don't do as well in practice as they seem to when you just count the number of instructions they execute.
>
> With a linked list, every time you traverse to the next item in the list, you're jumping to a totally random location in memory, and there's a decent chance that address isn't in cache.
>
> With an array, getting the next element in the array is in adjacent memory, so after the first access the rest of the array is probably cached.
>
> So, what most people find is that in practice, a dynamic array is much faster in practice for many common list sizes.
>
> Also, keep in mind that linked lists are not necessarily that efficient in terms of memory: if each item is 8 bytes or smaller, then every item in the list needs an additional 8 bytes of overhead (for the pointer). So a linked list might have 2x overhead, while a dynamically growing array might perform quite well with only 25% overhead.
>
> Of course, it totally depends on the exact usage. There are absolutely cases where linked lists perform better than alternatives, and they should still be used in those cases.

-   [c++ - Bjarne Stroustrup says we must avoid linked lists - Stack Overflow](https://stackoverflow.com/questions/34170566/bjarne-stroustrup-says-we-must-avoid-linked-lists)
-   [In defense of linked lists | Hacker News](https://news.ycombinator.com/item?id=33473497)


### Dynamic Array/Array List {#dynamic-array-array-list}

-   Rapper names: grow-able array, array list
-   Implementations: Go Slices, Java ArrayList, Python's list, Javascript []
-   You get: Random access. Well suited for stack likes (LIFO)
-   `insert/remove` at the end/tail is \\(O(1)\\), good stuff
-   `insert/remove` at the start/head is \\(O(n)\\), pretty bad
-   Optimal `stack` implementation is possible, Optimal `queue` implementation [is not possible](https://stackoverflow.com/questions/41665425/why-arraylist-doesnt-implements-queue).
    -   i.e Implementing a `queue` in Javascript with `.push` and `.shift` is probably not the best idea. But there [have been improvements](https://bugzilla.mozilla.org/show_bug.cgi?id=1348772#c7) in `.shift` [recently](https://codereview.stackexchange.com/questions/255698/queue-with-o1-enqueue-and-dequeue-with-js-arrays).
-   Whenever there is no room for appending a new element
    -   It creates a larger array
    -   Copies the contents of the current array to it
    -   Deletes the current array
    -   Sets the newly created array as current
    -   Finally, appends the element normally.


### Array Buffer / Ring Buffer / Circular buffer {#array-buffer-ring-buffer-circular-buffer}

{{< figure src="/ox-hugo/20230403192236-data_structures-5888242.png" >}}

-   No fixed head or tail. This also grows like Dynamic arrays but at the same time maintains order.
-   No shifting of other elements in the buffer needed at insertion/removal unlike dynamic array/arraylist. So, Well suited for queue like interfaces (FIFO)
-   You can implement things like queue/double ended queue(deque)/circular queue etc. w it.
-   [Circular Buffer Performance Trick - Cybernetist](https://cybernetist.com/2024/04/11/circular-buffer-performance-trick/)


### List ADTs ðŸ‘€ {#list-adts}


#### Queue {#queue}

-   Can be implemented on top of a Linked List data structure.
    -   It constrains what you **can do** with a linked list.
    -   We don't even need a doubly linked list to implement a basic queue.
-   FIFO
    -   The algorithm a queue implements is FIFO.
    -   FIFO is bad for temporal locality
-   Interface
    -   `insert-at-end` : \\(O(1)\\) (aka `enqueue`)
    -   `remove-at-front` : \\(O(1)\\) (aka `dequeue`)


#### Stack {#stack}

{{< figure src="/ox-hugo/20230403192236-data_structures-1882101605.png" >}}

-   LIFO
    -   LIFO is good for temporal locality
-   Good to think of arrows pointing backwards from the head. So usually just `prev` on a Node works.
-   Interface
    -   `insert-at-end` : \\(O(1)\\) (aka `push`)
    -   `remove-at-end` : \\(O(1)\\) (aka `pop`)


#### Deque (double ended queue) {#deque--double-ended-queue}

-   Not to be confused w the `dequeue` operation. (Spelling is different! `ue`)
-   This is an abstract data type, Java has an [implementation](https://docs.oracle.com/javase/7/docs/api/java/util/ArrayDeque.html) of this [interface](https://docs.oracle.com/javase/7/docs/api/java/util/Deque.html). It can be implemented via Linked list/or some custom implementation like Java does with `ArrayDeque`.
-   `insert-at-end` : \\(O(1)\\) (aka `enqueue` / `push`)
-   `remove-at-end` : \\(O(1)\\) (aka `pop`)
-   `remove-at-front` : \\(O(1)\\) (aka `dequeue`)
-   `insert-at-front` : \\(O(1)\\)

<div class="warning small-text">

> In Java, `ArrayDeque` doesn't have
>
> -   The overhead of node allocations that [LinkedList](https://docs.oracle.com/javase/7/docs/api/java/util/LinkedList.html) does
> -   The overhead of shifting the array contents left on remove that [ArrayList](https://docs.oracle.com/javase/7/docs/api/java/util/ArrayList.html) has.
> -   So eg. in Java, if you had to implement a `Queue`, you'd go with `ArrayDeque` as the underlying data structure rather than `LinkedList` or `ArrayList`.
> -   Summary: [It's pretty good.](https://stackoverflow.com/a/6129967)
</div>


## Trees {#trees}

> -   Technically, trees are just linked-lists with multiple paths.
> -   Trees are also graphs
> -   i.e `link list = tree = graph`

![](/ox-hugo/20230403192236-data_structures-10847626.png)
See [Trees]({{< relref "20230929065117-trees.md" >}})


### Tree ADTs ðŸ‘€ {#tree-adts}

-   Priority Queue (Implemented using Heap)


## Graphs {#graphs}

-   See [Graphs]({{< relref "20230521010631-graphs.md" >}})
