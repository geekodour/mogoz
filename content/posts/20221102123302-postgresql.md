+++
title = "PostgreSQL"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Database]({{< relref "20221102123145-database.md" >}})


## FAQ {#faq}


### How does Supabase work? {#how-does-supabase-work}

{{< figure src="/ox-hugo/20221102123302-postgresql-808560891.png" >}}

-   Supabase is more of BaaS (Backend-as-a-Service) than a [Database]({{< relref "20221102123145-database.md" >}}) service
-   Every table, view, and stored procedure is automatically mapped to a RESTful API.
-   Supabase uses PostgREST


### What's PostgREST? {#what-s-postgrest}

-   Also see [REST / RESTful / REpresentational State Transfer]({{< relref "20230302195820-api_design.md#rest-restful-representational-state-transfer" >}}), PostgREST is not really REST.
-   PostgREST is based on the idea if you are accessing the database as a particular user, [Row Level Security (RLS)](#row-level-security--rls) will ensure that you'll have access to the data you need.
    -   Users need to be authenticated and authorized.
    -   As this is like the backend now, some kind of pool for connections would be needed
-   Its API looks like a mixture of RESTful and GraphQL.


### How to use EXPLAIN? {#how-to-use-explain}

{{< figure src="/ox-hugo/20221102123302-postgresql-1354143187.png" >}}

-   Can give you the cost before hand
-   Can estimate how many rows it'll return etc.


### How to use ANALYZE? {#how-to-use-analyze}

-   It runs the shit that collects the statics that the query planner uses and what we see when we run `EXPLAIN`


### Postgres by default runs in `read committed`, so transactions are not serializable. How is this acceptable? {#postgres-by-default-runs-in-read-committed-so-transactions-are-not-serializable-dot-how-is-this-acceptable}

-   With `read committed`, Query in a transaction
    -   Sees only data committed before the query began;
    -   Never sees uncommitted data
    -   Never sees changes committed by concurrent transactions during the query's execution
-   This is slightly different for writes( See [MVCC]({{< relref "20231116004530-mvcc.md" >}}) )
-   The partial transaction isolation provided by Read Committed mode is adequate for many applications


## Features {#features}


### Temporary Tables {#temporary-tables}

-   Implemented like regular tables, but uses `temp_buffers` and uses disk for storing metadata and overflows etc.
-   Unlike regular tables, not guaranteed to be used by multiple connections at the same time, are not subject to locks, are not written to WAL, etc.
-   Located in PostgreSQL system tables. In addition, for each table, one or more files are created on disk (by default, in the same folder as the files for regular tables).
-   Allows different sessions to use the same temporary table name for different purposes
-   Automatically dropped at the end of a session, or optionally at the end of the current transaction based on `ON COMMIT`


#### Issues {#issues}

-   Autovacuum daemon cannot access and therefore cannot vacuum or analyze temporary tables.
-   Need to be purged
    -   `DELETE ALL` bad cuz MVCC is used for Temporary tables aswell and deleting records is slow  (See [Concurrency]({{< relref "20221126204257-concurrency.md" >}}))
    -   So we `TRUNCATE`: `TRUNCATE` simply creates a new file on disk and does an UPDATE of the `pg_class` table.


#### Read more {#read-more}

-   [PostgreSQL and Temporary Tables - DEV Community](https://dev.to/crushby/postgresql-and-temporary-tables-1ned)


### UPSERT {#upsert}

```sql
-- canonical
INSERT INTO table (col1, col2, col3)
VALUES (val1, val2, val3)
ON
 CONFLICT conflict_target conflict_action;
-- example
INSERT INTO employees (id, name, email)
VALUES (2, ‘Dennis’, ‘dennisp@weyland.corp’)
ON CONFLICT (id) DO UPDATE;
-- conflict_target: (id)
-- conflict_action: DO UPDATE
```


### Granular Access Control {#granular-access-control}


#### Row Level Security (RLS) {#row-level-security--rls}

-   **It needs to be enabled on per-table basis**
-   Access control is fully modeled inside the database
-   It doesn't matter as who you connect to it anymore
-   As long as your request can be appropriately authenticated into a database role for that row in the table you're good.
-   Row security policies can be specific to commands, or to roles, or to both.


### Nested/Sub Transactions {#nested-sub-transactions}

```sql
BEGIN; -- Start the main transaction
-- Perform some operations
INSERT INTO accounts (account_id, amount) VALUES (1, 1000);
-- Create a savepoint
SAVEPOINT my_savepoint;
-- Perform operations within the nested transaction
INSERT INTO transactions (trans_id, account_id, amount) VALUES (101, 1, 100);
UPDATE accounts SET amount = amount - 100 WHERE account_id = 1;
-- Decide to roll back the nested transaction
ROLLBACK TO SAVEPOINT my_savepoint;
-- The nested transaction's changes are undone, but the main transaction is still in progress
-- Continue with other operations
INSERT INTO logs (message) VALUES ('Nested transaction was rolled back.');
-- Finally, commit the main transaction
COMMIT;
```

-   See [Database Transactions]({{< relref "20231113145513-database_transactions.md" >}})
-   We want to avoid such nested transactions if possible.
-   Let the application logic create the transaction and put things in. If possible don't try to handle this at the DB level.
-   Subtransactions can commit or abort without affecting their parent transactions, allowing parent transactions to continue.
-   Started using `SAVEPOINT` / `EXECEPTION`


## Relevant Notes {#relevant-notes}

-   **Initializing a postgres directory:** After logging into the `postgres` user, you can create a directory with necessary postgres files with the `initdb` command. It creates a directory in the file system and then you can start a postgres server directing it to that directory.
-   **Tablespaces:** All tables are by default created in `pg_default` tablespace, creating in a tablespace does not affect the logical SQL schema.
-   **public schema:** All databases will have a public schema created by the `postgres` user. By default `\d` will be showing relations from the public schema.


## Management/Admin {#management-admin}

![](/ox-hugo/20221102123302-postgresql-282793601.png)
![](/ox-hugo/20221102123302-postgresql-1713348057.png)


### Postmaster &amp; Backend workers &amp; Backend processes {#postmaster-and-backend-workers-and-backend-processes}

-   `postmaster` is the primary daemon process for a PostgreSQL server.
-   The use of term is not done these days, the primary daemon process is simply being called `postgres` now.
-   Responsibilities
    -   Initializes the database system, prepares it for incoming connections, and ensures that the system is in a consistent state.
    -   Manages connections (spun off processes/ `backends processes`)
    -   [Logging]({{< relref "20221101183142-logging.md" >}})
-   `Backens processes`
    -   Config: `max_connections`
    -   Backend processes communicate with each other and with other processes of the instance using
    -   semaphores and shared memory to ensure data integrity throughout concurrent data access.
    -   i.e `connection` = `backend process` = `session` (unless using `pools`)
-   `Background workers`
    -   Config: `max_worker_processes`
    -   The `backend processes` spin up the `background workers` for actual work if parallel is enabled.
    -   This is different from `background writer` which is a Auxilary process.


### Shared Memory {#shared-memory}

-   As we read things from the disk etc. we put things in memory
-   Everything that is shared, it lives here. Pages, WAL records etc.
-   Every `backend process` and `backend worker` and everything else has access to it.
-   Related terms: `shared_buffers`, buffer pools
-   Provisioned by [mmap]({{< relref "20230405022848-mmap.md" >}}) (?)


### Auxiliary Processes {#auxiliary-processes}

{{< figure src="/ox-hugo/20221102123302-postgresql-746130329.png" >}}

-   `background writer` : It writes the dirty pages to the OS periodically. So it frees up shared memory in the `postgres` process but doesn't ensure that shit actually got to disk, that's upto the os whether it keeps it in the fs chache or what etc.
-   `checkpointer`: Flushes pages+wal to disk. Ensure that things have reached the disk.
-   `Autovacuum Launcher`
    -   config: `autovacuum_max_workers`
    -   It launches `autovacuum` workers
-   `wal`
    -   `archiver`: Archives everything for the wal (can be used for replay)
    -   `reciever`: Runs where we want to receive the WAL changes.
    -   `senders`: Senders of the WALs to replicas
    -   `writer`: Writing the WAL records from shared memory to disk. Controlled by `wal_writer_delay`
-   `starter`
    -   This combines `checkpointer` and `wal` for REDO(roll-forward recovery).
    -   Doesn't let any incoming connection till it ensures that it loaded the wal into the shared memory and made that data dirty before the `checkpoint`
    -   Now you're back in the same point before the crash happened.


### Cluster {#cluster}


#### What it's NOT {#what-it-s-not}

-   It is NOT what we popularly understand as cluster(compute cluster). This tripped me up for years man.
    -   If you were do that, you'll end up creating cluster of postgres clusters. (This is basically bad naming for historic reasons)
-   It's unrelated to the keyword `CLUSTER` which allows you to [organize a table](https://www.postgresql.org/docs/current/sql-cluster.html).


#### What it is {#what-it-is}

-   A `postmaster` / primary postgres process and a group of subsiduary processes
-   These processes manage a shared data directory that contains one or more databases.
-   It's created for you when you do `initdb`
-   Unusual for a basic or intermediate user to ever need to create clusters or manage multiple clusters as one cluster can manage multiple DBs
-   Having multiple cluster means having &gt;1 `postmaster` / primary `postgres` processes running on different ports. This is usually not needed.
-   See [PostgreSQL: Documentation: 16: 19.2. Creating a Database Cluster](https://www.postgresql.org/docs/current/creating-cluster.html)


### Replication {#replication}

See [Data Replication]({{< relref "20231021151742-data_replication.md" >}})


### System Metadata {#system-metadata}


#### System Catalogs {#system-catalogs}


#### System Views {#system-views}


## WAL {#wal}

-   Main idea: Changes to data files (where tables and indexes reside) must be written only after those changes have been logged
-   With the WAL, we will be able to recover the database so we need not flush pages on every transaction
-   When restoring the WAL after a crash, we'll recover to the `checkpoint` created by the `checkpointer`


### Synchronous v/s Async commit {#synchronous-v-s-async-commit}

-   It is possible to have both synchronous and asynchronous commit transactions running concurrently
-   Certain transactions are always synchronous, eg. DROP TABLE, [Two Phase Commit (2PC)]({{< relref "20231116010456-two_phase_locking_2pl.md" >}}) prepared transaction etc.


#### Synchronous {#synchronous}

Usually commits are synchronous, i.e if there's a commit, there'll be a WAL flush, and the commit is successful only if the flush was successful. (This is the default)


#### Asynchronous {#asynchronous}

-   [PostgreSQL]({{< relref "20221102123302-postgresql.md" >}}) allows you to do asynchronous commits aswell.
-   This introduces the risk of data loss. (not data corruption)
-   Should not be used if the client will take external actions relying on the assumption that the transaction will be remembered.
-   But for things like [Event]({{< relref "20230406185222-event_sourcing.md" >}}) [Logging]({{< relref "20221101183142-logging.md" >}}), maybe this would be fine


### WAL flush v/s Data flush {#wal-flush-v-s-data-flush}

-   The WAL file is written sequentially, and so the cost of syncing the WAL is much less than the cost of flushing the data pages.
-   If DB is handling many small concurrent transactions, one fsync of the WAL file may suffice to commit many transactions. `commit_delay` can be used to increase the window of transactions to be flushed.


### Journaling filesystems {#journaling-filesystems}

-   In certain cases if the underlying [Filesystem]({{< relref "20221101145053-filesystems.md" >}}) (the [sqlite]({{< relref "20230702184501-sqlite.md" >}}) VFS interface)  there's journaling support, in those cases we can probably not use sqlite journaling and gain some performance gains but this is not needed usually and unnecessarily complex. This can also be vice versa, we can disable filesystem journaling too.


## MVCC Implementation {#mvcc-implementation}

See [MVCC]({{< relref "20231116004530-mvcc.md" >}})


## VACUUM {#vacuum}

See [A Detailed Understanding of MVCC and Autovacuum Internals in PostgreSQL 14](https://www.youtube.com/watch?v=meU2qKRzkCM)


### Terms {#terms}

-   Live Tuples : Tuples that are Inserted or up-to-date or can be read or modified.
-   Dead Tuples : Tuples that are changed (Updated/Deleted) and unavailable to be used for any future transactions. (This is what Vaccum clears)


### Usecase {#usecase}


#### MVCC Version Cleanup {#mvcc-version-cleanup}

-   VACUUM is like a [Garbage collector]({{< relref "20221101213015-garbage_collection.md" >}}) for older versions of rows created by [MVCC]({{< relref "20231116004530-mvcc.md" >}})
-   `transaction id` is a `32-bit integer`
    -   the `VACUUM` process is responsible for making sure that the id does not overflow.
    -   Never disable the VACUUM, else transaction wraparound
-   See [Database Transactions]({{< relref "20231113145513-database_transactions.md" >}})


#### Run ANALYZE on tables {#run-analyze-on-tables}


### Freezing {#freezing}

{{< figure src="/ox-hugo/20221102123302-postgresql-489278158.png" >}}


### autovaccum conditions {#autovaccum-conditions}

{{< figure src="/ox-hugo/20221102123302-postgresql-1284047124.png" >}}


## Internals {#internals}


### Pages {#pages}

-   AKA Data blocks
-   Page number starts from 0


## Tools {#tools}


### Routing {#routing}

-   <https://github.com/pgbouncer/pgbouncer>
-   [Prepared Statements in Transaction Mode for PgBouncer](https://www.crunchydata.com/blog/prepared-statements-in-transaction-mode-for-pgbouncer#how-much-faster-are-prepared-statements)


### Others {#others}

-   [Ways to capture changes in Postgres | Hacker News](https://news.ycombinator.com/item?id=37610899)
-   <https://github.com/zalando/patroni>
-   <https://github.com/pganalyze/libpg_query>
-   <https://github.com/wal-g/wal-g>
-   <https://github.com/pgbackrest/pgbackrest>
-   <https://github.com/cybertec-postgresql/pgwatch2>
-   <https://github.com/citusdata/pg_cron>
-   <https://github.com/cybertec-postgresql/pg_timetable>


### Extension {#extension}

-   [Postgres Extensions Overview | Supabase Docs](https://supabase.com/docs/guides/database/extensions)
-   [Pg_jsonschema – JSON Schema Support for Postgres | Hacker News](https://news.ycombinator.com/item?id=35258323)
-   [pgvector: Embeddings and vector similarity | Supabase Docs](https://supabase.com/docs/guides/database/extensions/pgvector)


### Ecosystem {#ecosystem}

-   psql: [Psql Tips | Hacker News](https://news.ycombinator.com/item?id=34909670) | <https://github.com/okbob/pspg>
-   pgcli: CLI for Postgres with auto-completion and syntax highlighting. (Can be used instead of psql)
-   [gunnarmorling/pgoutput-cli](https://github.com/gunnarmorling/pgoutput-cli): Examining the output of logical replication slots using pgoutput encoding
-   <https://github.com/dalibo/pg_activity> : htop for pg
-   <https://gitlab.com/dmfay/pdot>


## Links {#links}

-   [Types of Indexes in PostgreSQL](https://www.highgo.ca/2020/06/22/types-of-indexes-in-postgresql/)
-   <https://twitter.com/samokhvalov/status/1713094683159941508>
-   <https://twitter.com/samokhvalov/status/1713094683159941508>
-   <https://twitter.com/samokhvalov/status/1702812327261950130>
-   [Explain Guide](https://www.pgmustard.com/docs/explain)
-   <https://github.com/allaboutapps/integresql>
-   [Five Tips For a Healthier Postgres Database in the New Year](https://www.crunchydata.com/blog/five-tips-for-a-healthier-postgres-database-in-the-new-year)
-   [Making PostgreSQL tick: New features in pg_cron | Hacker News](https://news.ycombinator.com/item?id=38029671)
-   [Show HN: Light implementation of Event Sourcing using PostgreSQL as event store | Hacker News](https://news.ycombinator.com/item?id=38084098)
-   [Postgres WAL Files and Sequence Numbers](https://www.crunchydata.com/blog/postgres-wal-files-and-sequuence-numbers)
-   [Operating on a minimal two-core Postgres instance: Query optimization insight...](https://news.ycombinator.com/item?id=38276727)
-   [An automatic indexing system for Postgres | Hacker News](https://news.ycombinator.com/item?id=38300297)
-   [Postgres schema changes are still a PITA | Lobsters](https://lobste.rs/s/ze70h7/postgres_schema_changes_are_still_pita)


### Other comments {#other-comments}


#### tw1 {#tw1}

Some PostgreSQL footguns:

-   default configuration
-   long transactions mixed with OLTP workload
-   repmgr (and other HA tools not based on consensus algorithms)
-   LISTEN/NOTIFY
-   "pg_dump is a utility for backing up a PostgreSQL database" says the official doc
-   moving/copying PGDATA and ignoring glibc version change
-   hot_standby_feedback on/off dilemma
-   partitioning of tables having high number of indexes and receiving QPS &gt;&gt; 1k
-   "setting statement_timeout in postgresql.conf is not recommended because it would affect all sessions" says the official doc
-   using replication slots w/o setting max_slot_wal_keep_size
-   relying only on statement_timeout &amp; idle_in_transaction_session_timeout and thinking it's enough (lack of transaction_timeout)
-   data types "money", "enum", "timestamp" (3 different cases)
-   int4 PK
-   massive DELETE
-   attempts to use transactional version of DDL (e.g., CREATE INDEX) under heavy loads
-   subtransactions
-   DDL under load without lock_timeout
