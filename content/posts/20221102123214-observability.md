+++
title = "Observability"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Kubernetes]({{< relref "20221102125748-kubernetes.md" >}}), [PromQL]({{< relref "20221102122633-promql.md" >}}), [Logging]({{< relref "20221101183142-logging.md" >}}), [Prometheus]({{< relref "20231230172853-prometheus.md" >}})


## FAQ {#faq}


### What about Performance? {#what-about-performance}

-   I'll have another page when I have it, but for now, this be it.
-   Outside scope of [Observability]({{< relref "20221102123214-observability.md" >}}) but consider things like
    -   Universal Scalability Law
    -   Amdahl's law (system limited by the seq processes)
    -   Little's law
    -   Kernel/Compiler level optimizations
-   See [Perf Little Book]({{< relref "20221101135342-perf_little_book.md" >}})


## What? {#what}

-   It's about bring better visibility into the system
-   It's a property of a system. Degree of system's observability is the degree to which it can be debugged.
-   Failure needs to be embraced
-   Once we have the info, we also need to know how to examine the info


### Checklist {#checklist}

-   [ ] Observability is not purely an operational concern.
    -   [ ] Allows test in prod.
    -   [ ] Reproducible failures
    -   [ ] Rollback/Forward flexibility
    -   [ ] Allows us to understand it when it runs
-   [ ] Let's us dig deeper v/s just let us know about the issue
-   [ ] Let's us debug better w evidence v/s conjecture/hypothesis


### The levels {#the-levels}


#### Primary {#primary}

-   Logs, Metrics, Traces
-   These need to be used in the right way to attain needed observability

<!--list-separator-->

-  Logs

    -   See [Logging]({{< relref "20221101183142-logging.md" >}})
    -   We don't tolerate loss here. Because we need this shit so we can query a needle in the haysack

<!--list-separator-->

-  Metrics

    -   System centric

<!--list-separator-->

-  Traces

    -   Request centric
    -   Most other tooling suggests you take proactive approaches which are useful for known issues. When you are debugging production systems you are trying to understand previously unknown issues. This is why tracing is useful.


#### Secondary {#secondary}

-   Exceptions (Thread local storage ([Threads]({{< relref "20221101173032-threads.md" >}})), stack trace)
-   Profiles (Mem, CPU, Mutex etc). Things that work here usually don't directly work with [Distributed Systems]({{< relref "20221102130004-distributed_systems.md" >}})


### Observability in dev/testing time {#observability-in-dev-testing-time}

-   Strive to write debuggable code (being able to ask questions in the future, via metrics, logs, traces, exceptions, combination etc)
-   We should be testing for failure aswell
    -   Best effort failure mode simulation, we can't catch all failure modes.
    -   We can be aware of the presence of failures
    -   We can't be aware of absence of failures ever
-   Assume the system will fail
-   Dev should be aware of things like
    -   How deployed? envars?. How it gets loaded/unloaded
    -   How it interacts w network? how it disconnects, exposed?
    -   How it handles IPC, configs
    -   How it discovers other stuff etc etc
-   Understand leaky abstractions of dependencies
    -   Default configs of dependencies
    -   Caching guarantees of dependencies
    -   Threading model of dependencies


### Testing in prod {#testing-in-prod}

-   Essentially means we can check something on a live system and the system allows us to see what's happening to the system when we want to check it.

{{< figure src="/ox-hugo/20221102123214-observability-1618516162.png" >}}


## Monitoring {#monitoring}


### Boxes {#boxes}

-   Blackbox: Symptom based(effect), less trigger(cause) based
-   Whiebox: We get data from inside of the system. (Detailed stuff)


### What it need to give {#what-it-need-to-give}

-   Show the failure
-   Impact of the failure
-   Ways to dig deeper
-   Effect of fix deployed


### What metrics to use? {#what-metrics-to-use}


#### USE (System Performance) {#use--system-performance}

-   U: Utilization
-   S: Saturation
-   E: Error


#### RED (Request Driven Applications) {#red--request-driven-applications}

-   R: Req. Rate
-   E: Req. Error Rate
-   D: Req. Duration (Histogram)


#### LETS (For Alerting) {#lets--for-alerting}

-   L: Latency
-   E: Error
-   T: Traffic
-   S: Saturation


#### Databases {#databases}

-   No. of queries made v/s no. of rows returned


#### Other practices {#other-practices}

-   We want to drop timeseries data that we don't need to save bandwidth and space


## Tracing {#tracing}


### Concepts {#concepts}

-   Spans
    -   Single operation within a trace
-   Trace Context
    ![](/ox-hugo/20221102123214-observability-47684442.png)


### Grafana Tempo {#grafana-tempo}

{{< figure src="/ox-hugo/20221102123214-observability-172928637.png" >}}


## Latency {#latency}

-   Finish watching ["How NOT to Measure Latency" by Gil Tene - YouTube](https://www.youtube.com/watch?v=lJ8ydIuPFeU)
-   [Want to Debug Latency?](https://rakyll.medium.com/want-to-debug-latency-7aa48ecbe8f7)
-   w databases, we'll have [Database]({{< relref "20221102123145-database.md" >}}) latency + Network latency


### Percentiles {#percentiles}

> Reminded me about a story when Google tried to optimise their response times in Africa. Because of poor infra, many users were getting timeouts when searching on G, so they worked to improve it. They managed to cut number of bytes, change geo-location of some switches, and what they saw that the average response time increased, but their p99 stayed the same. What really happened is that users who never could connect before, became their &gt;p95 and p99, and users who were p99 became p60-p80. So G engis made positive changes, but the numbers didn't reflect it. Is average and p90 BS, and what's the alternative?
>
> The lesson here is that [all models are wrong, but some are useful](<https://en.wikipedia.org/wiki/All_models_are_wrong>) It's not about what is BS and isn't, it's just a simple number, the problem is how you use it. If it's useful for you and you know how to use it, then a blog post shouldn't stop you from doing it.
>
> -   Some orange side guy

See [Statistics]({{< relref "20231017200424-statistics.md" >}})


## OpenTelemetry {#opentelemetry}

-   Basically this is hot shit. In some sense, OTEL is very much the k8 for observability (good and bad)
-   OTel lets the open source projects use an abstraction layer so that you have an option to `buy` instead of `self-host` in any cases.
    -   "oh shit, all this open source software emits Prometheus metrics and Jaeger traces, but we want to sell our proprietary alternatives to these and don't want to upstream patches to every project". - Some guy
    -   Otel is great for avoiding vendor lock-in
    -   A vendor-agnostic replacement for the client side of DataDog, New Relic, or Azure App Insights.


### Pros {#pros}

-   `otel-collector` itâ€™s easy to get logs, tracing and metrics in a standardised manner that allows you to forward any metrics/tracing to SRE teams or partners.
    -   Eg. send logs to Splunk, traces to New Relic &amp; metrics to Prometheus. Also send a filtered traces and logs to a partner that wants the details.
    -   devs can add whatever observability to their code and ops team can enforce certain filtering in a central place as well as only needing one central ingress path that applications talk to.
-   Despite of the cons, there seems to be no better alternative that otel. The alternative of not using otel is you'll have to export data in certain `exposition format` which will not be compatible w other backends handling that data(monitoring/logs/traces), but sometimes that's all you need.


### Cons {#cons}

-   OT might good for a certain language/stack and completely suck for another. Eg. exemplars are not supported in JavaScript/Node.
-   "It doesn't know what the hell it is. Is it a semantic standard? Is a protocol? It is a facade? It is a library? What layer of abstraction does it provide? Answer: All of the above! All the things! All the layers!" - Someone on orange site
-   "Otel markets itself as a universal tracing/metrics/logs format and set of plug and play libraries that has adapters for everything you need. It's actually a bunch of half/poorly implemented libraries with a ton of leaky internals, bad adapters, and actually not a lot of functionality." - Another user on orange site
-   "OTel tries to assert a general-purpose interface. But this is exactly the issue with the project. That interface doesn't exist." - Another guy


### Resources {#resources}

-   [Getting started with OpenTelemetry on Kubernetes | SigNoz](https://signoz.io/blog/opentelemetry-kubernetes/)
-   [OpenTelemetry in 2023 - by Kevin Lin - Bit by Bit](https://bit.kevinslin.com/p/opentelemetry-in-2023)
-   [How we used ClickHouse to store OpenTelemetry Traces and up our Observability Game](https://clickhouse.com/blog/how-we-used-clickhouse-to-store-opentelemetry-traces)
-   <https://github.com/keyval-dev/odigos>


## War Stories {#war-stories}


### When overloaded you should be doing as little work as possible {#when-overloaded-you-should-be-doing-as-little-work-as-possible}

> Basically what's happened for us on some very high transaction per second services is that we only log errors. Or Trace errors. And the service basically never has errors. So imagine a service that is getting 800,000 to 3 million request a second. And this is happily going along basically not logging or tracing anything. Then all the sudden a circuit opens on redis and for every single one of those requests that was meant to use that open circuit to redis you log or trace an error. You went from a system that is doing basically no logging or tracing to one that is logging or tracing at 800,000 to 3 million times a second. What actually happens is you open the circuit on redis because red is a little bit slow or you're a little bit slow calling redis and now you're logging or tracing 100,000 times a second instead of zero and that bit of logging makes the rest of the requests slow down and now you're actually within a few seconds logging or tracing 3 million requests a second. You have now toppled your tracing system your logging system and the service that's doing the work. Death spiral ensues. Now the systems that are calling this system starts slowing down and start tracing or logging more because they're also only tracing or logging mainly on error. Or sadly you have a better code that assumes that the tracing are logging system is up always and that starts failing causing errors and you get into doing extra special death loop that can only be recovered from by only attempting to log or error during an outage like this and you must push to fix. All the scenarios have happened to me in production.

-   So prefer sampling errors, See [Logging]({{< relref "20221101183142-logging.md" >}})
