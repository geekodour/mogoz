+++
title = "Nomad"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Kubernetes]({{< relref "20221102125748-kubernetes.md" >}}), [Infrastructure]({{< relref "20221101183735-infrastructure.md" >}})

> These notes are from when I am trying to run Nomad on hetzner


## Intro {#intro}

-   Nomad `server` is just a workload orchestrator. It only is concerned about things like Bin Packing, scheduling decisions.
-   Nomad doesn't interfere in your DNS setup, Service Discovery, secrets management mechanisms and pretty much anything else


### Concepts {#concepts}


#### Availability of "Platform" v/s Availability of "Application" {#availability-of-platform-v-s-availability-of-application}

-   When you're talking about nomad fault tolerance, we're talking about "availability of platform/infra"
-   Application availability is controlled by the `migrate` stanza in the configuration


#### Implications of Raft consensus for nomad `server` consensus {#implications-of-raft-consensus-for-nomad-server-consensus}

![](/ox-hugo/20240407004157-nomad-2134735834.png)
See [Consensus Protocols]({{< relref "20231118205116-consensus_protocols.md" >}})

-   Raft is used between `servers`
-   Nomad, like many other distributed systems, uses the raft consensus algorithm to determine who is the leader of the server cluster.
-   In nomad, the IP address is a component of the member ID in the raft data.
    -   So if the IP address of a `node` changes, you will either have to do peers.json recovery or wipe your state.

<!--list-separator-->

-  Fault Tolerance

    With FT, the state is replicated across servers

    -   ****WE WANT TO MAINTAIN ATLEAST 1 QUORUM AT ALL COSTS****
    -   If we loose quorum, then
        -   You be able to perform read actions
        -   But you won't be able to change the state until a quorum is re-established and a new leader is elected.
    -   Ideally you'd want to run `server` in a different failure domain than the `client(s)`

    <!--list-separator-->

    -  2 `servers`

        -   2 servers = 2 quorum (i.e we need atleast 2 servers to elect a leader and maintain quorum)
        -   If you have 2 `servers`, it's impossible to elect a leader since none of the servers has a winning vote.
            -   For this reason, there is zero fault tolerance with two servers
        -   Running two servers only gives you no fault tolerance in the failure of `servers`
        -   If one `server` fails, we loose the `quorum`, hence no leader **for the entire cluster**.
            -   Without leader, no state writes, no re-schedules, no cluster state changes.

    <!--list-separator-->

    -  3 servers

        -   3 servers = 2 quorum (i.e we need atleast 2 servers to elect a leader and maintain quorum)
        -   In this case, if we loose 1 server agent, quorum will still be maintained and things will keep running as expected.

<!--list-separator-->

-  Doubt

    -   I don't quite understand the [3/5(odd no) server requirement](https://cs.stackexchange.com/questions/57087/what-is-the-consensus-algorithm-that-requires-an-odd-number-of-nodes), I think it should become more clear as I do [Data Replication]({{< relref "20231021151742-data_replication.md" >}})
        -   Q: What happens if I run with 2 servr nodes, 4 server nodes?


### FAQ {#faq}


#### Sidecar pattern inside a task group {#sidecar-pattern-inside-a-task-group}

-   `main task`: Main tasks are tasks that do not have a `lifecycle` block.
-   Sidecar tasks: Init/post/sidecar tasks
    -   `sidecar` set to true means sidecar, else ephemeral task
-   For log shipper pattern, we also want to set `leader:true` in the `main task`
    -   When the `main task` completes all other tasks within the group will be gracefully shutdown.
    -   The log shipper should set a high enough `kill_timeout` such that it can ship any remaining logs before exiting.


#### Allocation and Port collision for static port {#allocation-and-port-collision-for-static-port}

-   When we use static port we occupy port in host machine/node
-   When we set task group count &gt; 1 and be using static port, and our cluster has &lt;2 `client`, then we don't have any way to make this happen. We either use dynamic port or go add a new client(node) or something.


#### `service` block &amp; `network` block (can there be multiple?) {#service-block-and-network-block--can-there-be-multiple}

| Block Name | Level                                                            | What                                      | Multiple                                                                                                                                                                                                    |
|------------|------------------------------------------------------------------|-------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
| service    | `task` and `task group` (added later for consul connect support) | How `other services` will find the `task` | Yes, multiple `network:port` and multiple `services` is [good combo](https://discuss.hashicorp.com/t/can-a-service-advertise-multiple-ports/28431/2), eg. expose metrics and web traffic in different ports |
| network    | `group`                                                          | How the `tg` connects to the `host/node`  | [Not yet](https://github.com/hashicorp/nomad/issues/11085), but can have multiple `ports`                                                                                                                   |

-   Note
    -   If you have multiple services in a `task group` then you need to explicitly specify the names for all the `services`. You can omit the name of atmost one service definition inside a `Task block`.


#### Hierarchy {#hierarchy}

<!--list-separator-->

-  Control

    ```nil
    - Job
      - Task Group(s) / Allocation(s)
        - Task(s)
    ```

    | Type       | Control                                                                                                                                                                        | How many                                                                                                                                                                                                                                                        |
    |------------|--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|
    | Job        | Controlled externally(job runner/airflow/cicd etc)                                                                                                                             | As many as the business needs are                                                                                                                                                                                                                               |
    | Task Group | There's no nomad specific ordering, they fire parallely but we can wait on things                                                                                              | Can be singleton(eg. [PostgreSQL]({{< relref "20221102123302-postgresql.md" >}})), can be multiple TG(independent, web&amp;db), [dependent on some other](https://discuss.hashicorp.com/t/multiple-tasks-in-a-single-job-with-order/24263/8) service by waiting |
    | Task       | `lifecycle` blocks, these are [inside the task group](https://discuss.hashicorp.com/t/dependency-order-of-groups/14820) and don't get to control how TG themselves are ordered | This is based on the idea of main task and supporting tasks(init/cleanup/sidecar etc)                                                                                                                                                                           |

<!--list-separator-->

- <span class="org-todo todo TODO">TODO</span>  Placement

    ```nil
    - Region
      - Datacenter
        - Nodepool
    - Namespace
    ```

    <!--list-separator-->

    -  Difference between nodepool and namespace and when to use what

        -   [Namespaces | Nomad | HashiCorp Developer](https://developer.hashicorp.com/nomad/tutorials/manage-clusters/namespaces)

<!--list-separator-->

- <span class="org-todo todo TODO">TODO</span>  Communication

    ```nil
    - Federation
    - Gossip
    - RPC
    - See the architecture doc
    ```


#### <span class="org-todo todo TODO">TODO</span> Allocation(task group) single or multiple tasks {#allocation--task-group--single-or-multiple-tasks}

-   This really depends but stick to the "if these really need to be on the same client"
-   If you have a singleton pg suppose then maybe `1TG:1Task(sidecars)` would be good
-   If you have another application in which you run the custom db and web app, these 2 can be different


#### Terms {#terms}

-   `Task Group`
    -   Group of tasks that need to run on the same `client agent`.
    -   Usually a `host/node` will only run one `client`, so put things in the same task group when we need them to run together. (Eg. shared filesystem, low latency etc)
-   Allocation
    -   It's a `Task Group` when submitted. i.e `"count" of a task group` = `no. of allocations for the task group`. It's like the same thing different forms. ice and water kind.
    -   Allocations is as the space/resources dedicated to a task on each machine.
    -   Each allocation can be across different machine/hosts/node/clients but, as the promise of the `task group`, each individual `task group` will run on a single `node/host`
        -   Eg. If we do `count=100`, there will be 100 allocation across devices
-   Deployment
    -   A deployment is the actual object that represents a change or update to a job.
    -   As of the moment the concept of deployment only really applies to `service jobs`
    -   `update block`
        -   Update strategy for new allocation
-   Evaluation
    -   It's the submission to the scheduler. Eg. After successful `evaluation` of the job, a `task group` becomes an `allocation`


## Configuration {#configuration}


### Setup Config {#setup-config}

See <https://gist.github.com/thimslugga/f3bbcaf5a173120de007e2d60a2747f7>


#### Nomad {#nomad}


#### Server agent {#server-agent}


#### Client agent {#client-agent}


### Workload Config {#workload-config}

-   Workload configuration is `JobSpec`
    -   This can be managed using [Terraform]({{< relref "20231229022230-terraform.md" >}}) ? (Q: Do we really need TF here?)
    -   The `JobSpec` itself sometimes need templating
        -   eg. We [cannot have a local file on your laptop](https://github.com/hashicorp/nomad/issues/3580) [be directly](https://discuss.hashicorp.com/t/passing-local-config-file-to-task/24598/3) provided. In these cases we can either use [Terraform]({{< relref "20231229022230-terraform.md" >}}) variables [as seen here](https://github.com/mr-karan/homelab/blob/06653f8f276e53b3ef987d80ce65b7d7e190cdce/terraform/modules/caddy/job.tf) or we can use HCL2 or maybe something else.
        -   I am of the opinion that we use terraform only for infra provisioning(the backbone) and not for deployments and stuff, so we'd try to avoid terraform for deployments of services as much as we can unless we don't have an option.
    -   Note about HCL2 `file` function usage in nomad

        > Functions are evaluated by the CLI during configuration parsing rather than job run time, so this function can only be used with files that are already present on disk on operator host.


#### Application arguments {#application-arguments}

-   This is done via `task>config>args`, this also supports "variable interpolation"


#### application configuration file {#application-configuration-file}

-   The `template` stanza helps in writing files to `NOMAD_TASK_DIR` (via `destination`) which is isolated for each task and accessible to each `task`
-   This can later be supplied to the task in various ways
    -   `bind` mounts for `docker task driver`
    -   Directly accessing `local` (`NOMAD_TASK_DIR`) if using `exec task driver`
-   For docker task driver flow is like
    -   First get the file into `NOMAD_TASK_DIR` using `template` or combination of  `template` and `artifact`
    -   Then bind mount the file from `local` (NOMAD_TASK_DIR) to the container


#### Env Vars and Secrets Management {#env-vars-and-secrets-management}

<!--list-separator-->

-  Environment variables

    -   `env stanza`
        -   Directly specify, supports interpolation
    -   `template stanza`
        -   Can be used to generate env vars from files on disk, Consul keys, or secrets from Vault:

<!--list-separator-->

-  Nomad variables

    -   Securely stores encrypted and replicated secrets in Nomad’s state store.
    -   Nomad Variables is not meant to be a Vault replacement. It stores small amounts of data which also happens to be encrypted and replicated. My advice is to treat Nomad Variables as a stopgap until you are ready to transition to Vault.
    -   These as of the now can be using using the `template` stanza, writing it into a file in nomad task dir and setting `env=true` in `template` stanza
    -   ALC is preconfigured for certain paths
    -   Two ways of using nomad variables
        -   Using explicit mention of path
            -   `{{ with nomadVar "nomad/jobs/redis" }}{{ .maxconns }}{{ end }}`
        -   Using all of nomad variables in the task using `nomadVarList`
        -   I prefer explicitly writing the path (1st choice)

<!--list-separator-->

-  Consul

    -   Using it with {{key}} and {{secret}} if you know the variable path

    <!--list-separator-->

    -  Vault

    <!--list-separator-->

    -  KV


## Auxiliaries {#auxiliaries}


### Plugins {#plugins}


### Volumes and Persistence {#volumes-and-persistence}

-   We have
    -   Nomad volumes (host volumes, filesystem or mounted network filesystem)
    -   Container Storage Interface (CSI) plugins
    -   Docker volumes (Supported but not recommended plus nomad scheduler will not be aware of it)
-   So we either want to use Nomad volumes or CSI
    -   We've to specify it in the `client agent` and then use that volume in our `job`


#### Docker driver specific {#docker-driver-specific}

-   As mentioned before, better to use nomad volumes instead of docker volumes
-   But docker `mount` config in the nomad docker driver can be useful sometimes. Currently supports: `volume`, `bind`, and `tmpfs` type
    -   usecase could be config loading, but the `atrifact` stanza also helps with that
        -   `template` stanza is enough for local files, for remote files, we might want to use `artifact`
    -   Eg. The caddy [official image](https://hub.docker.com/_/caddy) takes in mount points, in which can use `mount` to mount the data and config file.
        -   CONFIRM: This can be combination of volume(data directory) and bind mount(config file with artifact and template stanza)


## Restarts &amp; Checks {#restarts-and-checks}

```nil
update > healthcheck > check_restart > restart > fail? > reschedule
```


### `restart` {#restart}

```hcl
# example

# if things don't come up based on check_restart
# - attempts: restart the machine 2 times
# - interval: till we reach 5m, with 25s(delay) gaps
# - mode: If sevice doesn't come up declare it fail
restart {
  interval = "5m"
  attempts = 2
  delay    = "25s"
  mode     = "fail"
}
```

-   This can be set in the `task group` and `task` level. Values are inherited and merged.
-   no. of restart "attempts" should happen within the set "interval" before nomad does what "mode" says
-   `mode` : `delay` makes sense when re-running the job after `interval` would possibly make it succeed, otherwise, we would go with `fail`


### `check_restart` {#check-restart}

-   When the `restart` happens is controlled by the `check_restart` stanza
-   `grace` : This is important because we want to wait for the container to come up before we evaluate the healthcheck results


### `check` (Health Checks for Services) {#check--health-checks-for-services}

-   This is just the health check, this does not trigger any activity
-   `check_restart` stanza can be set inside `check` block to specify what happens when the health check fails
    -   Then the `restart` block in turn later determines how. Then how often and till when rerstart is triggered is controlled by the `restart` block properties.


### Shutdown Delay {#shutdown-delay}

-   For `service`, we have `shutdown_delay`
    -   Useful if the application itself doesn't handle graceful shutdowns based on the `kill_signal`
    -   The configured delay will provide a period of time in which the service is no longer registered in the provider
    -   Thus not receiving additional requests


## Service Discovery {#service-discovery}


### Nomad Service {#nomad-service}

-   They can be at the `task group` or `task` level. I usually like to put them at `task group` level because it then is next to the `network` block and makes things easier to read.
-   Health checks
    -   Are set by `check`
    -   Can be multiple checks for the same service
    -   Used provider mandates what kind checks are possible


### Providers {#providers}


#### Native Nomad Service Discovery {#native-nomad-service-discovery}

-   This was added later, prior to 1.3 only consul was there
-   It'll work for simpler usecases, for complex usecases, we might want to adopt consul


#### Consul {#consul}


#### Consul Connect {#consul-connect}

-   lets you use service mesh between nomad services.
    -   Q: If this does this, what does only Consul do?


#### Others {#others}

-   Worth noting that Traefik also supports discovery of Nomad services without Consul since Nomad 1.3.


## Networking {#networking}

-   See [Containers]({{< relref "20230218104617-containers.md" >}})
-   For each `task group` we have have a `network` stanza
    -   In the `taskgroup:network` we specify the `port(s)` that we want to use in our tasks. In other words we "allocate" the `ports` for the task.
-   But each task-driver which is specified in `task` stanza also can have its own networking configuration.


### On `network:port` {#on-network-port}

-   `to` : Which port in the container it should allocate to
-   `static` : Which port in the host nomad should map the container allocated(`to`) port to.


### Detour of `modes` {#detour-of-modes}


#### <span class="org-todo todo TODO">TODO</span> Networking modes {#networking-modes}

> See [network Block - Job Specification | Nomad | HashiCorp Developer](https://developer.hashicorp.com/nomad/docs/job-specification/network)
>
> -   You set these in the `network` stanza
> -   You can use

-   `none`
-   `bridge`
    -   This is useful w nomad service discovery
    -   Has a "mapping with a certain interface" via the `port` staza
    -   This uses CNI plugins
    -   Things would **NOT** show up in `netstat -tulnp`, you'll have [to switch namesapce](https://unix.stackexchange.com/questions/203723/how-can-i-list-all-connections-to-my-host-including-those-to-lxc-guests) if you need to check this.
    -   If you have access to the `interface`, even if it????????
-   `host`
    -   No shared namespace
    -   Use host machine's network namesapce
    -   Things would show up in `netstat -tulnp`
-   `cni/<cni network name>`
    -   This needs to be configured per client


#### <span class="org-todo todo TODO">TODO</span> Service address modes {#service-address-modes}


### Docker Task Driver Networking notes {#docker-task-driver-networking-notes}

-   Nomad uses `bridged` networking by default, like Docker.
    -   You can configure `bridge` mode both in `network` stanza and `docker task driver:network` stanza, both are conflicting. See nomad documentation on this. Ideally, you'd only want to use nomad network bridge.
-   The Docker driver configures ports on both the tcp and udp protocols. No way to change it.
-   with `task:config:ports` the port name is used to create a dynamic env var. Eg. something like `NOMAD_PORT_<port_name>`, where `port_name` are the items in `config:ports`. This can be used inside the container.


### Service Discovery and Networking {#service-discovery-and-networking}

-   `networking` stanza has different modes, `host` is the simplest. `bridge` mode needs various CNI plugins.
-   If you're using `host` networking mode and then use SD mechanisms to say populate the configuration file for some job. If you update the upstream job and the upstream job is having an dynamic port, now you configuration file has become stale! (This happened to me with nomad native SD and `host` mode for a job which was being used in Caddy reverse proxy)


#### More on bridge networking {#more-on-bridge-networking}

-   Once you
-   How do you access jobs running on bridge from machine localhost??
    -   Interestingly, even if the ports don't show up in `netstat -tulnp`, you will be able to access them from your local machine.
-   Inspecting bind traffic


## Production setup {#production-setup}


### How nomad is supposed to be run {#how-nomad-is-supposed-to-be-run}

> -   Hashicorp doc sometimes uses the term `node` and `client` interchangeably. Which is confusing.
>     -   In docs, A more generic term used to refer to machines running Nomad agents in client mode.
> -   Here's what I'll follow for my understanding
>     -   `node` is a bare metal server or a cloud server(eg. hetzner cloud vm(s)), can also be called the `host`
>     -   `nomad agent` : can be either nomad `server` or `client`
>     -   `client`: `agent` that is responsible for "running" `tasks`
>     -   `server`: `agent` that is responsible for scheduling `jobs` based on `allocation` &amp; `group` to certain `nodes` that are running a `client agent`

Following the above terms, here are the guidelines

-   Nomad is supposed to be HA and be quorum-based, i.e you run multiple `agents` on different `nodes`. (Preferably in different regions)
-   You're supposed to run one `agent(server/client)` in each `node`
-   You're not supposed to run multiple `agent` on same `node`
    -   Not supposed to run multiple `client agent` on same `node`
    -   Not supposed to run multiple `server agent` on same `node`
    -   Not supposed to run `server agent` and `client agent` on same `node` (There's [demand](https://discuss.hashicorp.com/t/single-node-nomad-combined-server-client-agent-feedback/42405) around this)
-   For true HA, there are certain recommendation around how many `server agent(s)` should you run.


#### More on `client` {#more-on-client}

-   A single `client agent` process can handle running many `allocations` on a single `node`, and `nomad server` will schedule against it until it runs out of schedulable resources


#### More on `server` {#more-on-server}

-   The `server` itself is not managed. i.e If the server runs out of memory it's not going to get re-scheduled etc.
    -   So need to make sure we run the `server` in an environment where it can easily do what's needed.
-   A `server` is light-weight. For a starting cluster, you could likely colocate a `server` process with another application—clients less so because of the workload aspect. (But it depends on if you need HA)
    -   `server` can run with other workloads(other selfhosted tools), even `consul` / `vault` etc.


### Different topologies {#different-topologies}


#### single node nomad cluster {#single-node-nomad-cluster}

Run just one `node`, it in, run both `client` and `server`.

> On `dev-agent` vs single node cluster
>
> -   Nomad has something called `dev-agent`, which is for quickly testing out nomad etc. It runs the server and client both in the same node.
>     -   This essentially means running the agent with `-dev` flag.
>     -   It enables a pre-configured dual-role agent (client + server) which is useful for developing or testing Nomad.
> -   Single Node nomad cluster is essentially the same as dev mode (-dev), but here we specifically specify that in our config and do not use the `-dev` flag.

<!--list-separator-->

-  Ways to do it

    -   With the -dev flag and a single agent 🚫
        -   It won’t save your data and it turns off ACLs
    -   With two agents using different ports. You could do this but additional hassle.
    -   With a single agent and both the client and server configs in the same config file
        -   This is the suggested way

<!--list-separator-->

-  Warnings &amp; Issues

    -   It is [strongly](https://github.com/hashicorp/nomad/issues/5053#issuecomment-451296159) recommended not to run both `client` and `server` in the same node
    -   HA only at application level, not at `node` level
        -   If we have 2 `allocation` of same app, in a single-node setup. If one allocation fails, we get the second up because `server` is up and running and realizes that happened. But if the `node` itself goes down, then everything goes down.
    -   Because we'll be running just 1 `client`, drains and rolling upgrades will not work as documented because there is no place to move the active workload to.
        -   TODO/DOUBT: I [am not super sure about](https://discuss.hashicorp.com/t/1-machine-per-cluster/18203/2) this one, why not just create another task on the same `node`?
    -   You can't run more than one instance of a workload that uses static ports.
    -   Ideally in a nomad setup, you'd run `server` as non-root, and `client` as root as it needs [OS isolation mechanisms](https://discuss.hashicorp.com/t/is-it-mandatory-to-run-nomad-client-as-root-user/11926) that require root privilege. But in a single node setup where you're running client and server from the same agent with combined configuration we'll run both as root and not provide any `User/Group` directive in the systemd file hence.

<!--list-separator-->

-  Issues

    -   I am getting this warning as-well: [Nomad 1.4 Autopilot with only single server - Nomad - HashiCorp Discuss](https://discuss.hashicorp.com/t/nomad-1-4-autopilot-with-only-single-server/45614)


#### multi node but 1:1 server:client {#multi-node-but-1-1-server-client}

-   Run 3 `nodes`, run pair of `sever:client` on each.


#### Recommended topology {#recommended-topology}

-   See deployment docs


### <span class="org-todo todo TODO">TODO</span> Don't understand / Doubts {#don-t-understand-doubts}

-   When we say 3 `node` cluster, do we mean 3 `servers`?
    -   I am confused about if these ppl [are talking about](https://news.ycombinator.com/item?id=28879130) running `server` or `client`. Given the assumption that we either run `server` or `client` only in one `node/host`
-   An even number of server nodes (outside of the case of a temporary failure) can make the raft cluster unhappy.
-   If each client is isolated from the others though, then any number is fine :).
-   Pro Tip Idea: be sure to specify a different node_class to the client on the nomad server.
-   documentation recommends running client as root and server as nomad user?
-   Does it mean that minimum nomad cluster requires minimum of 5 instances? 3 server + 2 clients?


### Failure Modes {#failure-modes}

-   clients losing their jobs
-   servers losing quorum


### Backup of nomad data? {#backup-of-nomad-data}

<https://mrkaran.dev/posts/home-server-nomad/>


### Other notes {#other-notes}

-   If you do not run Nomad as root, make sure you add the Nomad user to the Docker group so Nomad can communicate with the Docker daemon.


## Learning resources {#learning-resources}


### Ongoing ref {#ongoing-ref}

-   <https://github.com/mr-karan/homelab/blob/master/docs/SETUP.md>
-   Caddy
    -   We'll need to use static port in network
-   To read tutorials
    -   [Job Specifications | Nomad | HashiCorp Developer](https://developer.hashicorp.com/nomad/tutorials/job-specifications)
    -   [Advanced Scheduling | Nomad | HashiCorp Developer](https://developer.hashicorp.com/nomad/tutorials/advanced-scheduling)
    -   Deployment
        -   [Job Failure Handling | Nomad | HashiCorp Developer](https://developer.hashicorp.com/nomad/tutorials/job-failure-handling)
        -   [Job Updates | Nomad | HashiCorp Developer](https://developer.hashicorp.com/nomad/tutorials/job-updates)
        -   github oidc: [Google Cloud Platform workload identity federation | Nomad | HashiCorp Developer](https://developer.hashicorp.com/nomad/tutorials/fed-workload-identity/integration-gcp)


### Concepts {#concepts}

-   [Scheduling | Nomad | HashiCorp Developer](https://developer.hashicorp.com/nomad/docs/concepts/scheduling)


### Nomad Cluster on Production {#nomad-cluster-on-production}

-   Main prod ref: [Nomad deployment guide | Nomad | HashiCorp Developer](https://developer.hashicorp.com/nomad/tutorials/enterprise/production-deployment-guide-vm-with-consul)
    -   Involves consul, we need to figure if we need consul
    -   [Nomad reference architecture | Nomad | HashiCorp Developer](https://developer.hashicorp.com/nomad/tutorials/enterprise/production-reference-architecture-vm-with-consul#one-region)
-   [Nomad clusters on the cloud | Nomad | HashiCorp Developer](https://developer.hashicorp.com/nomad/tutorials/cluster-setup/cluster-setup-overview#extensibility-notes)
    -   <https://github.com/hashicorp/learn-nomad-cluster-setup>
        -   Think because we're using nixos, lot of tf boilerplate that this repo has is things we don't need
        -   But we do have to think about ACL and consul tough
-   [How We Develop and Operate Pirsch - Pirsch Analytics](https://pirsch.io/blog/techstack/)
-   [Deploying Nomad on NixOs | Heitor's log](https://heitorpb.github.io/bla/nomad-on-nixos/)
-   [My NixOS + Nomad Dev Environment](https://headless-render-api.com/blog/2024/03/22/nixos-nomad-dev-env)


### ACL {#acl}


#### What are the different tokens? {#what-are-the-different-tokens}

-   [Access Control | Nomad | HashiCorp Developer](https://developer.hashicorp.com/nomad/tutorials/access-control)


### Service Discovery {#service-discovery}

-   [Task Dependencies | Nomad | HashiCorp Developer](https://developer.hashicorp.com/nomad/tutorials/task-deps)
-   <https://developer.hashicorp.com/nomad/docs/integrations/consul>
-   Do we need consul? or native service discovery enough?
    -   When is the native service discover not enough?
    -   Examples
        -   <https://github.com/wenzel-felix/terraform-hetzner-nomad-consul-module>
        -   <https://github.com/datakurre/nomad-with-nix>
-   [Nomad 1.3 Adds Native Service Discovery and Edge Workload Support](https://www.hashicorp.com/blog/nomad-1-3-adds-native-service-discovery-and-edge-workload-support?product_intent=nomad)


## Sandboxing Questions {#sandboxing-questions}

-   Client agent is being run as root, what are the jobs run as. (in docker driver and in exec driver)???


## Fault Tolerance in Single Cluster Nomad {#fault-tolerance-in-single-cluster-nomad}


### Availability {#availability}

-   An available system will also be recoverable


### Recoverability {#recoverability}

-   After a fix, the system has recover making sure correctness is ensured
-   Eg. WAL helps with this


## Networking Q {#networking-q}

-   Does SD really not work in nomad if I have set mode to host?
-   same port, same ip address, same interface, different namespaces
    -   netstat is not showing port
    -   but i am able to access it from host
    -   I can listen for something on the same port on my server, curl still follows the old route(???)
