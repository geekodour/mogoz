+++
title = "Algorithms"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Computation and Computer Theory]({{< relref "20221101221439-computation_and_computer_theory.md" >}})


## Analysis of Algorithms Lectures Notes {#analysis-of-algorithms-lectures-notes}

Source: [Skiena's Algorithms](https://www3.cs.stonybrook.edu/~skiena/373/videos/)


### Lecture 1 {#lecture-1}

-   We seek algorithms which are correct and efficient.
-   No efficient, correct algorithm exists for the traveling salesman problem.
-   Proving that an algorithm in [NP complete](https://www.hillelwayne.com/post/np-hard/) [will help](https://matklad.github.io/2023/02/21/why-SAT-is-hard.html) you sleep well at night because you'll know that there exists **no fast** algorithm for that problem vs thinking you're being stupid while coming up with the algorithm to solve the problem.


#### Algorithm designer skills {#algorithm-designer-skills}

-   Proving Incorrectness
    -   We have a heuristic and then we try to disprove the correctness
    -   Proving incorrectness(eg. come up with counter examples, instance that breaks the algorithm)
    -   Heuristic: A algorithm that isn't correct or may not be correct
    -   The easier and smaller the counter example is the better
-   Proving correctness
    -   Failing to find a counter example != the algorithm is correct
    -   Induction is a useful way to prove correctness of **recursive algorithms**
    -   Proof by contradiction


### Lecture 2 {#lecture-2}

> Complexity v/s Big Oh
>
> In the RAM model of complexity
>
> -   we sometimes may have bumps in functions. i.e in different input sizes results may vary
> -   sometimes too much detail to specify precisely eg. `T (n) = 12754n2 + 4353n + 834 lg2 n + 13546`
>
> So big oh helps here.
>
> **Important distinction:**
>
> -   **Complexity**
>     -   **Best, Worst and Average case**
>     -   Ways of counting the complexity of an **algorithm**
> -   **The Big Oh Notation**
>     -   A language to talk about **functions**
>     -   We're just talking about properties of functions, we're not talking about meanings.


#### RAM Model of computation {#ram-model-of-computation}

-   Enables us to design algorithms in machine independent manner
-   It's not an accurate model but it's simple to work with and has a lot of truth to it
-   We measure the run time of an algorithm by counting the number of steps.
-   Assumptions
    -   Each “simple” operation (+, -, =, if, call) takes 1 step.
    -   Loops and subroutine calls are not simple operations.
    -   Each memory access takes exactly 1 step.


#### Complexity {#complexity}

-   WCRT, ACRT, BCRT

<!--listend-->

-   For any function(algorithm)
    -   We'll get a graph with worst, average and best case.
    -   Each dot represent different variation of the same input size(eg. sorting [1234] vs [4321])
        ![](/ox-hugo/20230205172402-algorithms-735131562.png)
-   About Complexities
    -   Average case: picking cases at random
        -   Randomized algorithms are getting popular, those algorithms usually require average-case type analysis to show merits
    -   Each of the complexities(worst, avg, best) define a numerical function: `Time v/s Size`


#### Big Oh Notation {#big-oh-notation}

<!--list-separator-->

-  About the notation

    -   \\(f(n)=O(g(n))\\)
        -   actually means \\(f(n) \in O(g(n))\\)
        -   read as "\\(f(n)\\) is order \\(g(n)\\)" or "\\(f(n)\\) is \\(O(g(n))\\)"

<!--list-separator-->

-  The Details

    -   With Big Oh Notation, we are able to reason about complex functions because we can see the upper and lower bound of the function.
    -   Functions are sometimes complex (Wiggly curve in diagram), so it's easier to talk about the function in terms of upper bound and lower bound.

        ![](/ox-hugo/20230205172402-algorithms-870521473.png)
        ![](/ox-hugo/20230205172402-algorithms-144599832.png)

        -   \\(f(n)=O(g(n))\\) means \\(c.g(n)\\) is **upper bound** on \\(f(n)\\)
        -   \\(f(n)=\Omega(g(n))\\) means \\(c.g(n)\\) is **lower bound** on \\(f(n)\\)
        -   \\(f(n)=\Theta(g(n))\\) means \\(c\_{1}.g(n)\\) is **upper bound** on \\(f(n)\\) and \\(c\_{2}.g(n)\\) is **lower bound** on \\(f(n)\\)
    -   \\(C\\) and \\(n\_{0}\\) are key to figuring out why some equation will satisfy the \\(\in \Omega,O,\Theta\\) relationships.


#### Big Oh x Complexity {#big-oh-x-complexity}

<!--list-separator-->

-  Gotchas

    -   [big-o growth](https://www.desmos.com/calculator/kgwiv5zizm)
    -   **asymptotic behavior of the algorithm(Big Oh)** and **complexity analysis** are different concepts.
    -   As programmers, we use this notation to describe complexity, but there's absolutely nothing stopping us from using asymptotic notation with other types of equations, things like the population of rabbits over time, the temperature given a change in pressure, and so forth.
    -   Lot of analysis we can run by using Big Oh
        -   worst-case running time (most common)
        -   average-case analysis
        -   best-case analysis
        -   expected time analysis for randomized algorithms
        -   amortized time analysis, and many others.
    -   Any statement "The X case is Y(f(n))", for any values of X, Y or f
        -   i.e Nothing is stopping you from also saying "The best case is \\(O(1)\\)" or "The worst case is \\(\Omega(n^{2})\\)"
    -   **"The worst case run time is \\(O(n^{2})\\) vs the worst case run time is \\(\Theta(n^{2})\\)?"**
        -   Sometimes we can make exact statements about the rate of growth of that function, sometimes we want to express upper or lower bounds on it.
        -   Not all algorithms can have \\(\Theta\\) complexity. There are algorithms that vary in complexity. Those can only be described with Big-O complexity
        -   When we say \\(O(n^{2})\\), the function may also be \\(O(n)\\)   , we do not know.
        -   When we say \\(\Theta(n^{2})\\), we know for a fact the function is \\(O(n^{2})\\) and \\(\Omega(n^{2})\\), because it is bounded by \\(c\_{1}n^{2} \le f(n) \le c\_{2}n^{2}\\)
        -   When we say \\(f(n)\\) ) will be lower-bounded by \\(log(n)\\)and upper-bounded by \\(n\\), we know \\(f(n) \in \Omega(log(n)) \\) and \\(f(n) \in O(n)\\)
        -   Therefore we can say that \\(\Theta(n^{2})\\) gives us a more accurate (or at least equal) sense of worst-case run time than \\(O(n)\\)
    -   In algorithm complexity analysis we're more interested in \\(O, \Omega, \Theta\\)
        -   **Upper bounded** = \\(O\\) (best known upper bound, or a simplification of the best-known upper bound, actually care about in practice)
        -   **Lower bounded** = \\(\Omega\\) (notoriously difficult to prove)
        -   **Theta bounded** = \\(\Theta\\)


### Lecture 3 {#lecture-3}


#### Analysis {#analysis}

When we say nothing infront of "running time", we usually mean worst case.

<!--list-separator-->

-  Worst case analysis

    -   For `selection sort`
        -   \\(n \times n \to O(n^{2})\\) : It's **No worse than \\(n^{2}\\)**
        -   \\(n/2 \times n/2 \to n^{2}/4 \to \Omega(n^{2})\\) : It's **No better than** \\(n^{2}\\)
        -   \\(n/2 \times n/2 \to n^{2}/4 \to \Theta(n^{2})\\) : It **is** \\(n^{2}\\)

    {{< figure src="/ox-hugo/20230205172402-algorithms-219328318.png" >}}

    -   There's no discussion about best case etc here. i.e It's analyzing the worst case by trying to figure out the upper and lower bounds.
    -   `DOUBT`: I am still confused about how we come up with \\(\Omega\\), esp for insertion sort.

<!--list-separator-->

-  [Logarithm]({{< relref "20221101154748-logarithm.md" >}})

    In the context of algorithms, Logarithms reflect:

    -   How many times we can double something util we get to `n`
    -   How many times we can half something util we get to `1`

    Eg. Binary search is \\(log(n)\\), Binary tree height is \\(log(n)\\)
