+++
title = "Algorithms"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Computation and Computer Theory]({{< relref "20221101221439-computation_and_computer_theory.md" >}}), [Data Structures]({{< relref "20230403192236-data_structures.md" >}}), [Algorithm Problems]({{< relref "20230407001048-algorithm_problems.md" >}}), [Recursion]({{< relref "20230429205506-recursion.md" >}}), [Big Oh Notation]({{< relref "20230423114200-big_oh_notation.md" >}})


## Meta {#meta}

-   nlogn : we do n, logn amount of times.


## Search {#search}


### Binary search {#binary-search}

-   List must be sorted


#### Worst case {#worst-case}

-   In binary search, worst case is till we half it till we can't half no more. i.e item not in list.
-   So we need to find out "How many times we can `half` something util we get to `1`"

> &rArr; \\(\frac{n}{2^{k}}=1\\)
> &rArr; \\(n = 2^{k}\\)
> &rArr; \\(\log\_{2}(n) = k\\)

-   i.e we can `half` something `k` times till we get `1`.


#### About one-offs w binary search {#about-one-offs-w-binary-search}

-   The algorithm is rather simple. But there's a solid possibility of off by one errors with implementing Binary search.
-   Things mostly differ based on how `low` and `high` is defined
    -   It changes the condition
    -   It changes what we return
-   There can be different ways to calculate `low` and `high` and all these do the same thing
    -   eg. The pseudo-code in wikipedia differs from what's there in CLRS, but they do the same exact thing.
    -   It's just about knowing what you set for `low` and `high` and then seeing that through the implementation of your algorithm. So you'll be able to follow any implementation of binary search that way.
    -   You can define this in terms of inclusive and exclusiveness.
        -   \\([low, high)\\) : `low` idx from the array, `high` idx out of array. `high` is `len(arr)-1`
        -   \\([low, high]\\) : `low` idx from the array, `high` idx from the array. `high` is `len(arr)`
        -   In both these cases, you can come up w an algorithm for binary search but how you calculate other variables will differ based on this thing.


#### Integer overflow {#integer-overflow}

-   We need to be careful about our invariants.
-   First, `high` and `low` both need to be in the integer range that's allowed.
-   Second, the usual way of calculating `mid` is `(high+low)/2` which is susceptible to integer overflow.
-   So usually `mid` is calculated as `low + (high-low)/2` which results in a integer that is in range and is [mathematically the same](https://math.stackexchange.com/questions/3709462/why-can-the-average-midpoint-of-two-numbers-be-described-as-the-sum-of-the-numbe) as `(high+low)/2`. This is something we need to do due to the limitations of our computers. As mentioned here, [Nearly All Binary Searches and Mergesorts are broken](https://ai.googleblog.com/2006/06/extra-extra-read-all-about-it-nearly.html)


#### Resources {#resources}

-   [Bitwise Binary Search: Elegant and Fast | orlp.net](https://orlp.net/blog/bitwise-binary-search/)
-   [Beautiful Branchless Binary Search | Probably Dance](https://probablydance.com/2023/04/27/beautiful-branchless-binary-search/)


## Sorting {#sorting}

-   If \\(X\_i\\) &le; \\(X\_{i+1}\\) for the entire array, array is sorted.
-   Largely two types, comparison based and non-comparison. (Lookup wikipedia). Me(a pleb) will mostly deal w comparison algorithms, non-comparison based include things like bucket sort, radix sort etc.


### Exchange based {#exchange-based}


#### Bubble sort {#bubble-sort}

-   Repeatedly swapping adjacent elements that are out of order
-   At the end of 1st pass, you'll have the max(arr) at the end of the array


### Selection based {#selection-based}


#### Selection sort {#selection-sort}


#### Heapsort {#heapsort}

-   Good to understand before Quicksort


### Insertion based {#insertion-based}

-   For scenarios where they need to merge 2 lists together


### Divide and Concur based {#divide-and-concur-based}


#### Quick sort {#quick-sort}

-   In place sort
-   Uses partitioning using pivot item
-   Worst case: Reverse sorted and we pick pivot to be the smallest item.
-   Time complexity: \\(O(n\log n)\\) , can be \\(O(n^{2})\\) in worst case
-   Implementation
    -   Partition func : Produces the pivot index and moves the items from one side to other
    -   Quicksort func: Calls partition func, gets the pivot, recalls itself


#### Merge sort {#merge-sort}

-   For situations where you're are getting in a piecemeal fashion. (eg. sorting a big file by loading chunks)
-   Time complexity: \\(O(n\log n)\\) (Always), but involves copying of data unlike quicksort.


### Others {#others}


#### Topological sort {#topological-sort}

-   <https://github.com/python/cpython/blob/3.11/Lib/graphlib.py>
-   [7.17. Topological Sorting â€” Problem Solving with Algorithms and Data Structures 3rd edition](https://runestone.academy/ns/books/published/pythonds3/Graphs/TopologicalSorting.html#topological-sorting)


## Tree Traversal {#tree-traversal}


### Meta ideas about tree traversal {#meta-ideas-about-tree-traversal}

-   We usually just go L -&gt; R
-   All trees have `n - 1` edges, `n` being the number of nodes.
    -   Time complexity: `O(V + E)` &rArr; \\(O(n + (n-1)) = O(n)\\)


#### Recursion and traversal {#recursion-and-traversal}

|           | BFS (Queue)         | DFS (Stack)                                     |
|-----------|---------------------|-------------------------------------------------|
| Recursive | Not natural         | Yes, that's the way. Preserves shape/structure. |
| Iterative | Yes, that's the way | Unsual to do, might help w limited call stack   |

-   See [Recursive process vs Recursive procedure]({{< relref "20230429205506-recursion.md" >}})


### DFS (Stack) {#dfs--stack}

{{< figure src="/ox-hugo/20230205172402-algorithms-1202411924.png" >}}

-   **Workhorse:** push and pop things on a `stack` (call stack)


#### Preorder {#preorder}

-   `VisitNode() + RecurseL() + RecurseR()`
-   Root's in the beginning


#### Inorder {#inorder}

-   `RecurseL() + VisitNode() + RecurseR()`
-   Root's in the middle
-   Inorder traversal of a Binary search tree will print things in order


#### Postorder {#postorder}

-   `RecurseL() + RecurseR() + VisitNode()`
-   Root's in the end


#### Implementation {#implementation}

-   Recursive
    -   DFS uses a stack to maintain a frontier, and recursion has an implicit stack (call stack)
    -   Implementing DFS using recursion simply means replacing the stack with a call stack.
-   Iterative
    -   To convert it into an iterative DFS you can simply use an actual stack data structure, but you now need to manage the stack yourself.


### BFS (Queue) {#bfs--queue}

{{< figure src="/ox-hugo/20230205172402-algorithms-1385823146.png" >}}

-   **Workhorse:** enqueue and dequeue things on a `queue` (auxiliary storage)

<!--listend-->

```text
while !q.empty?
  curr = q.rm_from_start() // dequeue
  print(curr) // do whatever w curr
  q.enqueue(curr.l)
  q.enqueue(curr.r)
```


#### Implementation {#implementation}

-   BFS and [recursion cannot be combined](https://codeforces.com/blog/entry/18642) as naturally since BFS does not use a stack


## Graph {#graph}


### Dijkstra's shortest path {#dijkstra-s-shortest-path}
