+++
title = "Modern AI Stack"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Programming Languages]({{< relref "20221101220306-programming_languages.md" >}}), [Machine Learning]({{< relref "20230408190056-machine_learning.md" >}})

This is to understand what happened in AI post 2020. Also to keep track of things.


## NLP {#nlp}


### History of NLP {#history-of-nlp}

{{< figure src="/ox-hugo/20230326092427-modern_ai_stack-150746386.png" >}}

-   We did have attention when we were using DNN
-   But with the 2017 paper, we suggested that "attention is all you need", aka Transformers.
-   [AllenNLP - Demo](https://demo.allennlp.org/reading-comprehension/bidaf-elmo)


### General ideas in NLP {#general-ideas-in-nlp}


#### Tokenization {#tokenization}

<!--list-separator-->

-  Sub word

    -   See [GitHub - google/sentencepiece](https://github.com/google/sentencepiece)
    -   Uses `byte-pair encoding` (BPE)
        ![](/ox-hugo/20230326092427-modern_ai_stack-1623535554.png)

<!--list-separator-->

-  Token count

    -   Large language models such as GPT-3/4, LLaMA and PaLM work in terms of tokens.
    -   32k tokens ~ 25k words ~ 100 single spaced pages
    -   See [Understanding GPT tokenizers](https://simonwillison.net/2023/Jun/8/gpt-tokenizers/)


#### Embeddings vs Tokens {#embeddings-vs-tokens}

-   [The Illustrated Word2vec - A Gentle Intro to Word Embeddings in Machine Learning - YouTube](https://www.youtube.com/watch?v=ISPId9Lhc1g)
-   Tokens
    -   These are inputs
    -   The basic units of text that the language model operates on
    -   "I love cats" would be tokens: ["I", "love", "cats"] if using word level tokenization.
-   Embeddings
    -   Embeddings are learned as part of the model training process.
    -   Refer to the vector representations of tokens in a continuous vector space.
    -   The model maps token to an embedding vector, representing semantic properties.
    -   As a result, two tokens with similar embeddings have similar meaning.
    -   Any deep learning model that uses tokens as input at some point is an embedding model.


#### LIMA style training? {#lima-style-training}


### LLM {#llm}

-   Language Models w &gt; 100m parameters
-   They don't have to use Transformers, but many do
-   They take text, convert it into tokens (integers), then predict which tokens should come next.
-   Pre-trained

{{< figure src="/ox-hugo/20230326092427-modern_ai_stack-1054377055.png" >}}


#### LLM Types {#llm-types}

{{< figure src="/ox-hugo/20230326092427-modern_ai_stack-332904700.png" >}}

-   Autoencoder LLMs are efficient for encoding (“understanding”) NL
-   Autoregressive LLMs can encode and generate NL, but may be slower


#### LLM Implementations/Architectures {#llm-implementations-architectures}

![](/ox-hugo/20230326092427-modern_ai_stack-788841550.png)
![](/ox-hugo/20230326092427-modern_ai_stack-763526078.png)
![](/ox-hugo/20230326092427-modern_ai_stack-2012364792.png)
![](/ox-hugo/20230326092427-modern_ai_stack-950276543.png)
![](/ox-hugo/20230326092427-modern_ai_stack-1807072495.png)


#### LLM Training {#llm-training}

Generally LLMs are trained on 1 eval (epoch)

<!--list-separator-->

-  Gradient Accumulation

    {{< figure src="/ox-hugo/20230326092427-modern_ai_stack-1089943995.png" >}}

<!--list-separator-->

-  Gradient Checkpointing

    {{< figure src="/ox-hugo/20230326092427-modern_ai_stack-170687801.png" >}}

<!--list-separator-->

-  Mixed-Precision

    {{< figure src="/ox-hugo/20230326092427-modern_ai_stack-1850968713.png" >}}

<!--list-separator-->

-  Dynamic Padding &amp; Uniform-Length Batching

    {{< figure src="/ox-hugo/20230326092427-modern_ai_stack-1234901233.png" >}}

<!--list-separator-->

-  PEFT with Low-Rank Adaptation


### RLHF {#rlhf}

-   This is the secret sauce in all new LLMs
-   Reinforcement learning from human feedback


#### Readings {#readings}

-   [Illustrating Reinforcement Learning from Human Feedback (RLHF)](https://huggingface.co/blog/rlhf)
-   [How RLHF Works (And How Things May Go Wrong)](https://www.assemblyai.com/blog/how-rlhf-preference-model-tuning-works-and-how-things-may-go-wrong/)


### GPT in production {#gpt-in-production}


#### Embedding search + vectorDB {#embedding-search-plus-vectordb}

<!--list-separator-->

-  Basic idea

    -   Embed internal data using the LLM tokenizer(create chunks), load it into a vectorDB
    -   Then query the vector DB for the most relevant information
    -   Add into the context window.

<!--list-separator-->

-  When documents/corpus are too big to fit into prompt. Eg. Because of token limits.

    -   Obtain relevant chunks by similarity search on query from vector DB
    -   Find top k most similar chunk embeddings.
    -   Stuff as many top k chunks as you can into the prompt and run the query

<!--list-separator-->

-  Example

    -   Imagine you have an LLM with a token limit of 8k tokens.
    -   Split the original document or corpus into 4k token chunks.
    -   Leaf nodes of a "chunk tree" are set to these 4k chunks.
        -   Run your query by summarizing these nodes, pair-wise (two at a time)
        -   Generate parent nodes of the leaf nodes.
        -   You now have a layer above the leaf nodes.
        -   Repeat until you reach a single root node.
        -   That node is the result of tree-summarizing your document using LLMs.

<!--list-separator-->

-  Tools

    -   llmaindex and langchain allow us to do this stuff.
    -   OpenAI cookbook suggests this approach, see [gpt4langchain](https://github.com/pinecone-io/examples/blob/master/generation/gpt4-retrieval-augmentation/gpt-4-langchain-docs.ipynb)
    -   [pinecode embedded search](https://github.com/openai/openai-cookbook/blob/main/examples/vector_databases/pinecone/Gen_QA.ipynb)
    -   [GPT 4: Superpower results with search - YouTube](https://www.youtube.com/watch?v=tBJ-CTKG2dM&t=787s)


#### Prompt tuning {#prompt-tuning}

-   Idea is similar to embedding search thing but here, you are allowed to insert the embeddings of the prompt into the LLM.
-   This is not currently possible with OpenAI's API
-   This claims to better perform prompt search


#### Finetune {#finetune}

![](/ox-hugo/20230326092427-modern_ai_stack-1346438305.png)
Train a model on how to respond, so you don’t have to specify that in your prompt.


#### LLMOps {#llmops}

-   The cost of LLMOps is in inference.
-   Input tokens can be processed in parallel, output is sequential


### OSS LLMS {#oss-llms}

See [Open Source LLMs]({{< relref "20230719050449-open_source_llms.md" >}})


## Transformers {#transformers}

![](/ox-hugo/20230326092427-modern_ai_stack-1255652474.png)
![](/ox-hugo/20230326092427-modern_ai_stack-2069610963.png)


### Transformers FAQ {#transformers-faq}


#### zero-shot/one-zero/few-shot learning? {#zero-shot-one-zero-few-shot-learning}

-   Full training: This is not even in the shot spectrum, we train the whole data to get predictions. We can do "shot" kind of things on pre-trained models.
-   few-shot: few more example/runs, we give it few examples etc.
-   one-shot: give one example, it'll be able to do it.
-   zero-shot
    -   We do not want to give any concrete examples at all
    -   But instruct the model in a different way.  (Eg. Prompting)


#### What attention? {#what-attention}

-   In each layer of the network
    -   Encoders taking words from an input sentence, converting them into a representation
    -   Each decoder takes all the encoders’ representation of words and transforms them into words in another language.
-   Decoders getting all the encoders’ output provides a wider context and enables Attention.


### Usage (Modality) {#usage--modality}

-   Text
    -   Text classification
    -   Test generation
    -   Summarization
-   Audio
    -   Audio classification
    -   Automatic speech recognition
-   Vision
    -   Object detection
    -   Image classification
    -   Image segmentation
-   Multi Modal
    -   Visual QA
    -   Document QA
    -   Image captioning


## Diffusion models {#diffusion-models}

-   These are different from transformers in arch, training process, how they infer, usecase etc.
-   See
    -   [What are Diffusion Models? | Lil'Log](https://lilianweng.github.io/posts/2021-07-11-diffusion-models/)
    -   [Diffusion models are autoencoders – Sander Dieleman](https://sander.ai/2022/01/31/diffusion.html)


### Fineturning diffusion models (Stable Difussion) {#fineturning-diffusion-models--stable-difussion}


#### Dreambooth {#dreambooth}

{{< figure src="/ox-hugo/20230326092427-modern_ai_stack-494918082.png" >}}


#### Textual Inversion {#textual-inversion}

{{< figure src="/ox-hugo/20230326092427-modern_ai_stack-1116443791.png" >}}


#### LoRA {#lora}

{{< figure src="/ox-hugo/20230326092427-modern_ai_stack-743950201.png" >}}


#### Depth-2-Img {#depth-2-img}

{{< figure src="/ox-hugo/20230326092427-modern_ai_stack-318853441.png" >}}


#### ControlNet {#controlnet}

-   It's a training strategy, a way of doing fine tuning
-   It's different from Dreambooth and LoRA in ways that they don't freeze the original model.

![](/ox-hugo/20230326092427-modern_ai_stack-1701974168.png)
![](/ox-hugo/20230326092427-modern_ai_stack-1033145681.png)
![](/ox-hugo/20230326092427-modern_ai_stack-1213872828.png)
![](/ox-hugo/20230326092427-modern_ai_stack-1557053891.png)
![](/ox-hugo/20230326092427-modern_ai_stack-361886987.png)

-   The complimentary external model can be distributed independently or can be baked into one model.
-   The complimentary model is specific to freezed main model so it'll only work with that version so we need to care about compatibility


## TTS {#tts}


### Bark {#bark}

-   <https://github.com/suno-ai/bark>
-   <https://github.com/coqui-ai/TTS>
-   <https://github.com/serp-ai/bark-with-voice-clone>


### tortoise {#tortoise}

-   <https://github.com/neonbjb/tortoise-tts>
-   <https://git.ecker.tech/mrq/ai-voice-cloning>
-   <https://github.com/facebookresearch/fairseq/tree/main/examples/mms>


## STT {#stt}


### Whisper {#whisper}

-   <https://github.com/guillaumekln/faster-whisper>


### Piper {#piper}

-   <https://github.com/rhasspy/piper>
