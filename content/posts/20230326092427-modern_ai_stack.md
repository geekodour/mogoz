+++
title = "Modern AI Stack"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Programming Languages]({{< relref "20221101220306-programming_languages.md" >}}), [Machine Learning]({{< relref "20230408190056-machine_learning.md" >}})

This is to understand what happened in AI post 2020. Also to keep track of things.


## NLP {#nlp}


### History of NLP {#history-of-nlp}

{{< figure src="/ox-hugo/20230326092427-modern_ai_stack-150746386.png" >}}

-   We did have attention when we were using DNN
-   But with the 2017 paper, we suggested that "attention is all you need", aka Transformers.


### General ideas in NLP {#general-ideas-in-nlp}


#### Tokenization {#tokenization}

<!--list-separator-->

-  Sub word

    -   Uses `byte-pair encoding`
        ![](/ox-hugo/20230326092427-modern_ai_stack-1623535554.png)

<!--list-separator-->

-  Token count

    -   32k tokens ~ 25k words ~ 100 single spaced pages


### LLM {#llm}

-   Language Models w &gt; 100m parameters
-   They don't have to use Transformers, but many do
-   Pre-trained

{{< figure src="/ox-hugo/20230326092427-modern_ai_stack-1054377055.png" >}}


#### LLM Types {#llm-types}

{{< figure src="/ox-hugo/20230326092427-modern_ai_stack-332904700.png" >}}

-   Autoencoder LLMs are efficient for encoding (“understanding”) NL
-   Autoregressive LLMs can encode and generate NL, but may be slower


#### LLM Implementations/Architectures {#llm-implementations-architectures}

![](/ox-hugo/20230326092427-modern_ai_stack-788841550.png)
![](/ox-hugo/20230326092427-modern_ai_stack-763526078.png)
![](/ox-hugo/20230326092427-modern_ai_stack-2012364792.png)
![](/ox-hugo/20230326092427-modern_ai_stack-950276543.png)
![](/ox-hugo/20230326092427-modern_ai_stack-1807072495.png)
![](/ox-hugo/20230326092427-modern_ai_stack-1144765404.png)


#### LLM Training {#llm-training}

Generally LLMs are trained on 1 eval (epoch)

<!--list-separator-->

-  Gradient Accumulation

    {{< figure src="/ox-hugo/20230326092427-modern_ai_stack-1089943995.png" >}}

<!--list-separator-->

-  Gradient Checkpointing

    {{< figure src="/ox-hugo/20230326092427-modern_ai_stack-170687801.png" >}}

<!--list-separator-->

-  Mixed-Precision

    {{< figure src="/ox-hugo/20230326092427-modern_ai_stack-1850968713.png" >}}

<!--list-separator-->

-  Dynamic Padding &amp; Uniform-Length Batching

    {{< figure src="/ox-hugo/20230326092427-modern_ai_stack-1234901233.png" >}}

<!--list-separator-->

-  PEFT with Low-Rank Adaptation


## Transformers {#transformers}

![](/ox-hugo/20230326092427-modern_ai_stack-1255652474.png)
![](/ox-hugo/20230326092427-modern_ai_stack-2069610963.png)


### Transformers FAQ {#transformers-faq}


#### zero-shot/one-zero/few-shot learning? {#zero-shot-one-zero-few-shot-learning}

-   zero-shot: chatgpt
-   one-shot: give one example, it'll be able to do it
-   few-shot: few more example/runs
-   Can be controlled with \`beams\`?


### Components {#components}

-   Model (checkpoints) : The main deal
-   Weights: This sometimes comes w the model file, sometimes separately
-   Tokenizer : The algorithm to use for tokenization that the model will understand


### Usage (Modality) {#usage--modality}

-   Text
    -   Text classification
    -   Test generation
    -   Summarization
-   Audio
    -   Audio classification
    -   Automatic speech recognition
-   Vision
    -   Object detection
    -   Image classification
    -   Image segmentation
-   Multi Modal
    -   Visual QA
    -   Document QA
    -   Image captioning


## TTS {#tts}


## STT {#stt}


### Whisper {#whisper}


## Diffusion Models {#diffusion-models}
