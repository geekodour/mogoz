+++
title = "Database"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [SQL]({{< relref "20230217190123-sql.md" >}}), [Distributed Systems]({{< relref "20221102130004-distributed_systems.md" >}}), [PostgreSQL]({{< relref "20221102123302-postgresql.md" >}}), [Database Transactions]({{< relref "20231113145513-database_transactions.md" >}})


## Database types {#database-types}

{{< figure src="./images/sql-vs-nosql-cheatsheet.webp" >}}


### Based on Relational(SQL) v/s Non-relational(NoSQL) {#based-on-relational--sql--v-s-non-relational--nosql}

{{< figure src="/ox-hugo/20221102123145-database-357404598.png" >}}


#### Relational {#relational}

-   Schema
-   [PostgreSQL]({{< relref "20221102123302-postgresql.md" >}}), MySQL etc


#### Non-Relational / NoSQL {#non-relational-nosql}

> -   Access paths
>     -   Relational model allows you to do "point" queries. i.e you can get "any" or "all" of something. Eg. any or all comments, any or all users.
>     -   w Non-relational(usually tree), you need to worry about access path.
>         -   Find any comment? you need to know what post it is attached to.
>     -   Find all comments? you have to traverse all posts to all comments.
>         -   If path changes, you have to go update application logic
> -   Start node
>     -   Finding start nodes is what SQL explicitly excels at.
>     -   Once we do have a start node (a comment, post, user, etc.), a Graph query language is a better fit for application queries than SQL.
>
> Few words by tantaman^

-   Schemaless
-   This should have been named NoRDBMS instead because it's not no sql that is. Infact, no-sql dbs can support sql like queries etc.


### Based on [CAP]({{< relref "20221102130004-distributed_systems.md#cap" >}}) {#based-on-cap--20221102130004-distributed-systems-dot-md}

{{< figure src="/ox-hugo/20221102123145-database-267730290.png" >}}

-   While most Relational DBs guarantee ACID, NoSQL DBs give [you BASE](https://dl.acm.org/doi/10.1145/1394127.1394128)


### Based on Data model {#based-on-data-model}

These data models can be implemented in both Relational and Non-Relational types. Eg. [PostgreSQL]({{< relref "20221102123302-postgresql.md" >}}) can also be turned into column store/json data store with indexes and extensions etc. But database built for specific data models would most probably be the ideal solution rather than make up for it w extensions etc.
![](/ox-hugo/20221102123145-database-813312395.png)
Eg. KV, Graph, Document/Object, Column-family, Array/Matrix/Vectors etc.


#### KV {#kv}

-   Redis, DynamoDB etc.


#### Document/Object based {#document-object-based}

-   MongoDB, DocumentDB etc.


#### Column {#column}

-   Wide column: `(Partition Key, Cluster Key, Value)`
-   AWS Redshift, [DuckDB](https://news.ycombinator.com/item?id=34741195), Cassandra, Clickhouse, HBase(NoSQL), Scylla
-   Clickhouse is a Column SQL DB while, Scylla is a Column NoSQL DB.
-   Data Warehouse are built on columnar storage, See [Data Engineering]({{< relref "20230405003455-data_engineering.md" >}})


### Based on Query {#based-on-query}

![](/ox-hugo/20221102123145-database-828653387.png)
![](/ox-hugo/20221102123145-database-1967162762.png)


#### OLAP {#olap}

-   Analytics workload


#### OLTP {#oltp}

-   Transactional workload


#### Operational {#operational}

> -   "automated interventions in the business"
> -   Think of OLTP workload on a OLAP database

![](/ox-hugo/20221102123145-database-1795534000.png)
![](/ox-hugo/20221102123145-database-996881664.png)
![](/ox-hugo/20221102123145-database-687611546.png)

-   Operational is slightly different from transactional&amp;analytical, somewhere in the middle
-   Usually involves maintenance workloads etc. Sometimes queries need to run on big datasets.
-   Operational workloads have fundamental requirements that are diametrically opposite from the requirements for analytical systems
-   If an operational system goes down for the day, there are people who will either be unable to do their job
-   Similarities with Analytics workload is that both sort of will share SQL queries that are complex, join-heavy, multi-level.
-   Solutions
    -   Reverse ETL (See [Data Engineering]({{< relref "20230405003455-data_engineering.md" >}})) for this purpose
    -   Cache results (but now need to worry about cache invalidation)


## Relational Model basics {#relational-model-basics}


### Meta {#meta}

-   Access path independence: Decouple “what data do I want to retrieve” from “how do I retrieve it.”


### Components {#components}

-   tables = relations
    -   Unordered set that contain the relationship of attributes that represent entities.
    -   Table w `n-ary relation` = Table w n columns
-   attributes = columns (has a type/domain)
-   tuple = row = set of attributes/columns
-   Values
    -   Normally atomic/scalar
    -   `NULL` is a member of every domain if allowed


## Modeling of Databases {#modeling-of-databases}

See [RDBMS / SQL / DB Data Modeling]({{< relref "20231114082501-rdbms_sql_db_data_modeling.md" >}})


### Partitioning {#partitioning}

-   See [Scaling Databases]({{< relref "20230608143206-scaling_databases.md" >}})
-   A rather general concept and can be applied in many contexts.
-   Partitioning could be
    -   Different database inside MySQL on the same server
    -   Different tables
    -   Different column value in a singular table


#### Horizontal/Row based {#horizontal-row-based}

![](/ox-hugo/20230608143206-scaling_databases-673884427.png)
![](/ox-hugo/20230608143206-scaling_databases-1571241611.png)

-   Splitting one table into different tables. Same table structure.


#### Vertical/Column based {#vertical-column-based}

-   Splitting one table into different tables. Each table will have a different table structure.
-   This is similar to the idea of **Normalization**
    -   Usually vertical partitioning refers to a physical optimization
    -   Whereas normalization is an optimization on the conceptual level


## Important Features {#important-features}


### Caching {#caching}


#### External {#external}

-   External Materialized views
    -   See [Materialize: An Operational Data Warehouse](https://materialize.com/blog/operational-data-warehouse/)
-   Query caching
    -   See [ReadySet | Same database, (much) faster queries](https://readyset.io/)
    -   [Redis]({{< relref "20230406214230-redis.md" >}}) should also be able to do this?


### Migrations {#migrations}

-   There are different migration patterns
    -   [Online migrations at scale](https://stripe.com/blog/online-migrations)
    -   [safe sb migrations](https://www.aviransplace.com/post/safe-database-migration-pattern-without-downtime-1#ixzz3vsEunxmA)
    -   [Database Migrations](https://vadimkravcenko.com/shorts/database-migrations/)
-   Tools
    -   <https://sqitch.org/>
        -   [What migration/versioning tool do you use? : PostgreSQL](https://www.reddit.com/r/PostgreSQL/comments/u5qkbr/what_migrationversioning_tool_do_you_use/)
    -   <https://atlasgo.io/getting-started/postgresql-declarative-sql>
    -   <https://github.com/bikeshedder/tusker>


#### Migration FAQ {#migration-faq}

<!--list-separator-->

-  Migration version issue

    -   Sequential versions vs Timestamps
        -   0005_first_developer_account_changes.sql vs 2017...08_first_developer_account_changes.sql
    -   See this thread for [proper explanation](https://github.com/pressly/goose/issues/63)
    -   Solution is to use a hybrid approach.
        -   If using goose, it has the fix command which is suggested to be run in CI. Goose will also create a table `goose_db_version` in the same postgres schema.
        -   `Timestamps`
            -   in Local/Dev
            -   Avoid annoying file renaming &amp; conflicts on rebase
        -   `Sequential`
            -   in staging/prod
            -   Rename those timestamps-based filenames to serial numbers

<!--list-separator-->

-  Online/Live/Realtime Migrations

    -   Migrating from one database to another without downtime and compromising data correctness


#### Tips {#tips}

<!--list-separator-->

-  No direct table access

    Never expose real tables to an application. Create an "API" schema which contains only views, functions, procedures, and only allow applications to use this schema. This gives you a layer of indirection on the DB side such that you can nearly eliminate the dance of coordinating application changes and database migrations

<!--list-separator-->

-  Get comfortable with `forward-only migrations`

    Rollbacks work fine for easy changes, but you can't rollback the hard ones (like deleting a field), and you're better off getting comfortable with `forward-only migrations`

<!--list-separator-->

-  Application code and Migrations

    -   Usually recommended to decouple schema rollout from the application
    -   Whether you can run migrations before application startup is something to be decided based on provisioned infra. Eg. in k8s, some ppl run migrations in init containers, don't do rollbacks.
    -   It **isn't possible** to atomically deploy both the migration and the application code that uses it.
    -   This means if you want to avoid a few seconds/minutes of errors
        -   You need to deploy the migration first in a way that doesn't break existing code
        -   Then the application change
        -   And then often a cleanup step to complete the migration in a way that won't break.


### Cursors {#cursors}


### Functions {#functions}


#### Triggers {#triggers}


#### Stored Procedures {#stored-procedures}


#### Deferred Constraints {#deferred-constraints}


#### Application code constraints {#application-code-constraints}


### Query Planner {#query-planner}

-   Also see [Query Engines]({{< relref "20231113151855-query_engines.md" >}}). The term query planner and query optimizer is synonymous.
-   There can be muliple ways a result of a DB query can be found. Query planner’s role is to determine which strategy is the best option.
    -   Eg. choosing which indexes to scan for a given query,
    -   Eg. choosing the order in which to perform joins, and
    -   Eg. choosing the join algorithm to use for any given join
-   It analyzes the queries + optimize them before running.
-   Some query planner offer escape hatches for end user to fiddle with
-   The life of a query begins and ends in the database itself.


#### Component of Query Planner {#component-of-query-planner}

These are not actual component names but abstract ideas for understanding.

<!--list-separator-->

-  Compiler

    -   A query will be lexed, parsed, typechecked, translated into an intermediate representation, “optimized”
    -   Finally directly translated into a form more suitable for execution.

<!--list-separator-->

-  Domain knowledge extractor

    -   Approximate domain knowledge that it can exploit to make queries run as fast as possible.
    -   This is done through making approximations on `statistics` collected. Eg. In the following query this needs human intervention, but query planner will try its best to detect which way to go with from the `statistics` (how big is each table, how many distinct values does each column contain, etc).
        ![](/ox-hugo/20221102123145-database-125955626.png)


#### <span class="org-todo todo TODO">TODO</span> Plans {#plans}

<!--list-separator-->

-  Postgres Specific

    <!--list-separator-->

    -  General plan

    <!--list-separator-->

    -  Custom plan

<!--list-separator-->

-  Physical Plan

    -   The plan that gets executed


### Others {#others}

-   Locks: See [Database Locks]({{< relref "20231114211916-database_locks.md" >}})
-   Vacuum: See [PostgreSQL]({{< relref "20221102123302-postgresql.md" >}})
-   Indexes: See [Database Indexing]({{< relref "20231113185310-database_indexing.md" >}})


## ACID {#acid}

-   ACID are [correctness criteria]({{< relref "20221126204257-concurrency.md#correctness-criteria" >}}) properties of database transactions
-   See [Database Transactions]({{< relref "20231113145513-database_transactions.md" >}})


### What are these? {#what-are-these}

-   Loose description instead of a strict implementation standard.
-   Properties database transactions need to guarantee to their users for validity even in the event of crash, error, hardware failures and similar.
-   These guide application developers what’s their responsibility v/s what the databases provides


### ACID can have multiple meanings {#acid-can-have-multiple-meanings}

-   There are tradeoffs in implementing ACID capabilities/properties. So, if a DB claims to be ACID complaint, there might still be edge cases
-   Each property has its own **spectrum**, the spectrum is of about trafeoffs. So implementation of the property will fall somewhere in the spectrum based on the tradeoffs the DB developers took
-   Implementing some capabilities is more expensive than others. `consistency` and `isolation` are pretty expensive.


### The properties {#the-properties}


#### Atomicity {#atomicity}

-   Database ordering of transaction if different from what application dev sees in their code. If you want it two transactions to maintain order, better wrap them in a database transaction.
-   `ROLLBACK` helps with atomicity


#### Consistency {#consistency}

-   `Atomicity+Isolation` leads to consistency
-   Consistency has different meanings based on context. (See [Concurrency Consistency Models]({{< relref "20231113121413-concurrency_consistency_models.md" >}}))
-   When it comes to ACID in databases, it simply means satisfying application specific constraints. Think of maintaining referential integrity etc.


#### Isolation {#isolation}

-   Question to ask: "Is it serializable?"
-   Question to ask: "Whether or not concurrent transactions can see/affect each other's changes"
-   Helps avoid race conditions
-   Isolation is achieved using [Database Transactions]({{< relref "20231113145513-database_transactions.md" >}})

<!--list-separator-->

-  Weak and Strong

    -   Weak isolation = faster + more data races
        -   Weak doesn't mean unsafe
    -   Strong isolation = slower + lesser data races
        -   Eg. Cockroach and SQLite have effectively serializable transactions. But they don't catch all bugs either.

<!--list-separator-->

-  Read Phenomena/Issues/Anomaly

    NOTE: See [PostgreSQL: Documentation: 16: 13.2. Transaction Isolation](https://www.postgresql.org/docs/current/transaction-iso.html) for better summary

    <!--list-separator-->

    -  Non-repetable reads

        In the same thread, you read the same data twice and you now get different value because another thread committed. That's non-repetable! i.e you're not getting consistent results
        ![](/ox-hugo/20221102123145-database-872730703.png)

    <!--list-separator-->

    -  Dirty reads

        ![](/ox-hugo/20221102123145-database-1633264743.png)
        In the same thread, you read the data, but another thread also changed the same data but did not even commit. You read the data again, you're seeing uncommitted data now in your thread.

    <!--list-separator-->

    -  Phantom reads

        {{< figure src="/ox-hugo/20221102123145-database-629050487.png" >}}

        -   This is similar to Non-repeatable reads but more related to search conditions/aggregates finding that previous results of the search/aggregations and current results are different

<!--list-separator-->

-  SQL standard isolation levels

    ![](/ox-hugo/20221102123145-database-1397967951.png)
    There are more levels to this than what the SQL standard offers. Eg. Google’s Spanner guarantee external serializability with clock synchronization (stricter isolation but not in the [SQL]({{< relref "20230217190123-sql.md" >}}) standard, see [Concurrency Consistency Models]({{< relref "20231113121413-concurrency_consistency_models.md" >}}))

    | Name              | Description                                                                                                                            | Strength(1-4) | Availability      |
    |-------------------|----------------------------------------------------------------------------------------------------------------------------------------|---------------|-------------------|
    | Serializable      | As if execution was sequential                                                                                                         | 4             | Unavailable       |
    | Repeatable reads  | Changes by other transactions aren't visible, but can see own uncommitted reads.                                                       | 3             | Unavailable       |
    | Read committed    | Only committed changes are visible, no uncommitted reads. New committed changes by others can be seen (potential for `phantom reads`). | 2             | Totally Available |
    | Read Un-committed | Allows dirty reads, transactions can see changes by others that aren't yet committed.                                                  | 1             | Totally Available |

    -   The isolation levels only define what must not happen. Not what must happen. (i.e `BTNH`, See [Correctness criteria]({{< relref "20221126204257-concurrency.md#correctness-criteria" >}}))
    -   `Read commited` is the default in [PostgreSQL]({{< relref "20221102123302-postgresql.md" >}}) but allows no phantom reads.
    -   Most DBs don't even implement `Read Un-committed` as it's not much useful

<!--list-separator-->

-  Databases and their implementation of isolation

    {{< figure src="/ox-hugo/20221102123145-database-1450356367.png" >}}

    -   Martin Kleppmann's Hermitage is an excellent resource on this.
    -   DBs provide different isolation layers based on tradeoffs, SQL standard has 4. Eg. [PostgreSQL]({{< relref "20221102123302-postgresql.md" >}}) only provides 3.
    -   DBs say something and mean something else from [Concurrency Consistency Models]({{< relref "20231113121413-concurrency_consistency_models.md" >}})
    -   See [ept/hermitage](https://github.com/ept/hermitage)


#### Durability {#durability}

-   This is about what happens after `COMMIT`
-   When i hit save, things should go to non-volatile shit, so that if system crashes, i can still read the data
-   Some sacrifice durability for performance.
-   This is again different for different DBs, eg. SQLite only acknowledges a transaction when it is safely persisted to the underlying storage.


## Database Administration {#database-administration}

See [Infrastructure]({{< relref "20221101183735-infrastructure.md" >}}), [System Design]({{< relref "20230113141133-system_design.md" >}})


### Sessions, Connections and Transactions {#sessions-connections-and-transactions}


#### Connection {#connection}

| DB         | Default connection limit      |
|------------|-------------------------------|
| sqLite     | N/A, limited by single-writer |
| poStgresql | 100 (configurable)            |
| mySql      | 150 (configurable)            |

-   A Connection is established when a client (like an application or a user) connects to the PostgreSQL server.
-   Each connection runs in its own process in the server.
-   Idle connections [consume resources](https://aws.amazon.com/blogs/database/resources-consumed-by-idle-postgresql-connections/). pgbouncer has `server_idle_timeout` on which it'll close a connection to the db if it has been idle for more than the specified duration.
-   In terms of [PostgreSQL]({{< relref "20221102123302-postgresql.md" >}}), each connection is then handled by a `worker backend`
-   "To get more storage, you buy SSD, and to get more connections, you buy more RAM &amp; networking hardware"


#### Session {#session}

-   A Session begins when a connection is successfully established.
-   In PostgreSQL, a session is essentially synonymous with a connection.
-   [PostgreSQL](6d720224-a9a9-4858-bebe-c4aa5596ae89) can handle multiple connection/sessions concurrently


#### Transaction {#transaction}

-   Transaction is a sequence of one or more SQL operations treated as a unit.
-   A session can have muliple transactions
-   All `query` run within a transaction
-   Life cycle: starts, runs the queries it contains, and ends


#### Pools {#pools}

-   See [How to Manage Connections Efficiently in Postgres, or Any Database](https://brandur.org/postgres-connections)
-   See [Scaling PostgreSQL with PgBouncer: You May Need a Connection Pooler](https://www.percona.com/blog/scaling-postgresql-with-pgbouncer-you-may-need-a-connection-pooler-sooner-than-you-expect/)
-   The fundamental idea behind PgBouncer is to reduce the overhead associated with opening and closing connections frequently by reusing them.
-   Usually `session` and `connection` have 1-1 relationship, pools change that.
-   Check
    -   Free connection is available: Assigns connection to the client.
    -   No free connection, pool not at max: Opens new connection, assign client.
    -   Pool at max: Queue client request

<!--list-separator-->

-  pgbouncer

    Since pgbouncer is single threaded, you can run [multiple of it](https://www.enterprisedb.com/postgres-tutorials/pgbouncer-setup-extended-architecture-use-cases-and-leveraging-soreuseport) like this
    ![](/ox-hugo/20221102123145-database-571545794.png)

    <!--list-separator-->

    -  Pooling types

        <!--list-separator-->

        -  Session pooling

            -   Pgbouncer merely acts as a connection broker
            -   We still get the 1-1 connection-session relationship with added queue

        <!--list-separator-->

        -  Transaction pooling

            -   Can assign the `connection` to different client
                -   Once the current transaction is completed
                -   Session might not be complete
            -   i.e same connection be used for multiple sessions
            -   i.e connections can be shared among different sessions
            -   Limitations when working with Prepared Statements.
            -   **We must make sure that, we do not depend on session-level states** (like temporary tables, session variables)

    <!--list-separator-->

    -  Configuring pgbouncer

        <!--list-separator-->

        -  Auth (2 ways)

            I prefer the `auth_query` method. But even when using `auth_query` method, we'll still need to use `auth_file` (userlist.txt) for `auth_user` pass. (yeah it's [damn confusing](https://dba.stackexchange.com/questions/333507/postgresql-pgbouncer-authentication))

            -   Adding the SCRAM verifiers to the `userlist.txt` file that the pgbouncer.ini references
            -   Setting up an `auth_query` to be used by the pgbouncer.ini method


### Edge and Databases {#edge-and-databases}

-   Usually edge environments don’t support making arbitrary TCP outbound connections
-   Only [HTTP]({{< relref "20230222161545-http.md" >}}) and [WebSockets]({{< relref "20230222181643-websockets.md" >}}) are allowed.
-   DBs usually use a custom TCP-based network protocol. (now that's a conflict)
-   You will need to use a HTTP -&gt; TCP proxy of some sort
-   Some edge ready DBs are now supporting
    -   HTTP fetch (best for single-shot queries)
    -   WebSocket (for sessions and transactions)


### Management Processes {#management-processes}

See [PostgreSQL]({{< relref "20221102123302-postgresql.md" >}})


## ORMs {#orms}

They are simply an abstraction with a quintessential pro and con—they abstract away some visibility and occasionally incur some performance hits.

-   Produce
    -   ORM: Produce objects. (directed graph)
    -   DB tables: Bidirectionally linked data via shared keys (undirected graph)
-   See [Sketch of a Post-ORM](https://borretti.me/article/sketch-post-orm)
