+++
title = "Open Source LLMs"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Modern AI Stack]({{< relref "20230326092427-modern_ai_stack.md" >}}) , [Machine Learning]({{< relref "20230408190056-machine_learning.md" >}}), [StableDiffusion]({{< relref "20230719120753-stablediffusion.md" >}})


## Resources {#resources}

-   Airtable link: [MLModels Airtable](https://airtable.com/appWjub9re6jaNTpA/tbly7fUiGaBkTVKvw/viwuTtXhizXBK9ybF?blocks=hide)
-   All links: <https://github.com/imaurer/awesome-decentralized-llm>
-   <https://github.com/evanmiller/LLM-Reading-List>


### Fine tuning base model {#fine-tuning-base-model}

-   [Fine-tuning Alpaca](https://www.youtube.com/watch?v=4-Q50fmq7Uw)
-   [ML Blog - Fine-Tune Your Own Llama 2 Model in a Colab Notebook](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html)
-   [Google Colab](https://colab.research.google.com/drive/1Zmaceu65d7w4Tcd-cfnZRb6k_Tcv2b8g)
-   [Axolotl (from OpenAccess-AI-Collective ) github repo now supports flash attention with QLora fine tunes](https://www.reddit.com/r/LocalLLaMA/comments/15htqa9/axolotl_from_openaccessaicollective_github_repo/)
-   <https://github.com/mshumer/gpt-llm-trainer>


### Tools/libs {#tools-libs}

-   <https://github.com/marella/ctransformers> (ggml python bindings)
-   <https://huggingface.co/hkunlp/instructor-xl> (embeddings)


## Meta {#meta}

![](/ox-hugo/20230326092427-modern_ai_stack-1144765404.png)
![](/ox-hugo/20230719050449-open_source_llms-314571890.png)

-   Base models are not supposed to used directly, they are meant for fine-tuning in a way


## LLaMA {#llama}

{{< figure src="/ox-hugo/20230719050449-open_source_llms-1051572189.png" >}}

-   LLaMA is not tuned for instruction following like ChatGPT
-   llma.cpp story : [What is the meaning of hacked? · Issue #33 · ggerganov/llama.cpp · GitHub](https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022)

![](/ox-hugo/20230326092427-modern_ai_stack-179978562.png)
![](/ox-hugo/20230326092427-modern_ai_stack-1028578828.png)


## Alpaca {#alpaca}

{{< figure src="/ox-hugo/20230326092427-modern_ai_stack-404328522.png" >}}

-   What's Alpaca-LoRA ? Technique used to finetune llama using lora


## Case studies/Tools {#case-studies-tools}

-   [ReLLM: Exact Structure for Large Language Model Completions](https://matt-rickard.com/rellm)
-   [Context-Free Grammar Parsing with LLMs](https://matt-rickard.com/context-free-grammar-parsing-with-llms)
-   [GitHub - microsoft/guidance: A guidance language for controlling large language models.](https://github.com/microsoft/guidance)


## Comparison {#comparison}


### Guanaco 7B (llma.cpp) {#guanaco-7b--llma-dot-cpp}

-   CPU
    -   1 Thread, CPU: 0.17-0.26 tokens/s
    -   11 Threads, 12vCPU: ~1token/s
    -   21 Threads, 12vCPU: ~0.3token/s
    -   10 Threads, 12vCPU: ~0.3token/s
    -   1 Thread, CPU, cuBALS: 0.17-0.26 tokens/s
    -   9 Thread, CPU, cuBALS: 5 tokens/s
-   GPTQ (GPU)
    -   ~25token/s


### Others {#others}

-   Someone on internet
    -   My llama2 inference performance on desktop RTX 3060
        -   5bit quantization (llama.cpp with cuda)
            -   47 tokens/sec (21ms/token) for llama7b
            -   27 tokens/sec (37ms/token) for llama13b using


### Resources {#resources}

-   <https://github.com/ggerganov/llama.cpp/blob/master/docs/token_generation_performance_tips.md>
-   <https://github.com/oobabooga/text-generation-webui/blob/main/docs/Generation-parameters.md>
