+++
title = "Open Source LLMs (Transformers)"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Modern AI Stack]({{< relref "20230326092427-modern_ai_stack.md" >}}) , [Machine Learning]({{< relref "20230408190056-machine_learning.md" >}}), [StableDiffusion]({{< relref "20230719120753-stablediffusion.md" >}}), [Deploying ML applications (applied ML)]({{< relref "20241130100028-deploying_ml_applications_applied_ml.md" >}})


## Resources {#resources}

-   Airtable link: [MLModels Airtable](https://airtable.com/appWjub9re6jaNTpA/tbly7fUiGaBkTVKvw/viwuTtXhizXBK9ybF?blocks=hide)
-   All links: <https://github.com/imaurer/awesome-decentralized-llm>
-   <https://github.com/evanmiller/LLM-Reading-List>


### Fine tuning base model {#fine-tuning-base-model}

-   [Fine-tuning Alpaca](https://www.youtube.com/watch?v=4-Q50fmq7Uw)
-   [ML Blog - Fine-Tune Your Own Llama 2 Model in a Colab Notebook](https://mlabonne.github.io/blog/posts/Fine_Tune_Your_Own_Llama_2_Model_in_a_Colab_Notebook.html)
-   [Google Colab](https://colab.research.google.com/drive/1Zmaceu65d7w4Tcd-cfnZRb6k_Tcv2b8g)
-   [Axolotl (from OpenAccess-AI-Collective ) github repo now supports flash attention with QLora fine tunes](https://www.reddit.com/r/LocalLLaMA/comments/15htqa9/axolotl_from_openaccessaicollective_github_repo/)
-   <https://github.com/mshumer/gpt-llm-trainer>


### Tools/libs {#tools-libs}

-   <https://github.com/marella/ctransformers> (ggml python bindings)
-   <https://huggingface.co/hkunlp/instructor-xl> (embeddings)


## Meta {#meta}

![](/ox-hugo/20230326092427-modern_ai_stack-1144765404.png)
![](/ox-hugo/20230719050449-open_source_llms-314571890.png)

-   Base models are not supposed to used directly, they are meant for fine-tuning in a way


## LLaMA {#llama}

{{< figure src="/ox-hugo/20230719050449-open_source_llms-1051572189.png" >}}

-   LLaMA is not tuned for instruction following like ChatGPT
-   llma.cpp story : [What is the meaning of hacked? · Issue #33 · ggerganov/llama.cpp · GitHub](https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022)

![](/ox-hugo/20230326092427-modern_ai_stack-179978562.png)
![](/ox-hugo/20230326092427-modern_ai_stack-1028578828.png)


## Alpaca {#alpaca}

{{< figure src="/ox-hugo/20230326092427-modern_ai_stack-404328522.png" >}}

-   What's Alpaca-LoRA ? Technique used to finetune llama using lora


## Case studies/Tools {#case-studies-tools}

-   [ReLLM: Exact Structure for Large Language Model Completions](https://matt-rickard.com/rellm)
-   [Context-Free Grammar Parsing with LLMs](https://matt-rickard.com/context-free-grammar-parsing-with-llms)
-   [GitHub - microsoft/guidance: A guidance language for controlling large language models.](https://github.com/microsoft/guidance)


## Comparison {#comparison}


### Guanaco 7B (llma.cpp) {#guanaco-7b--llma-dot-cpp}

-   CPU
    -   1 Thread, CPU: 0.17-0.26 tokens/s
    -   11 Threads, 12vCPU: ~1token/s
    -   21 Threads, 12vCPU: ~0.3token/s
    -   10 Threads, 12vCPU: ~0.3token/s
    -   1 Thread, CPU, cuBALS: 0.17-0.26 tokens/s
    -   9 Thread, CPU, cuBALS: 5 tokens/s
-   GPTQ (GPU)
    -   ~25token/s


### Others {#others}

-   Someone on internet
    -   My llama2 inference performance on desktop RTX 3060
        -   5bit quantization (llama.cpp with cuda)
            -   47 tokens/sec (21ms/token) for llama7b
            -   27 tokens/sec (37ms/token) for llama13b using


### Resources {#resources}

-   <https://github.com/ggerganov/llama.cpp/blob/master/docs/token_generation_performance_tips.md>
-   <https://github.com/oobabooga/text-generation-webui/blob/main/docs/Generation-parameters.md>


## Techniques {#techniques}

See [Deploying ML applications (applied ML)]({{< relref "20241130100028-deploying_ml_applications_applied_ml.md" >}})

> &gt; "For most other out-of-the-box LLM frameworks, the value is in looking at their prompts and then doing it yourself."
>
> &gt; i'd just roll my own extraction pipeline from the start, rather than learning someone elses complex setup


### Chat bot training {#chat-bot-training}


#### Useful links {#useful-links}

-   <https://lmarena.ai>

<!--list-separator-->

-  Existing SAAS

    -   [Chatbot Arena: Find the Best Chatbot Builder](https://chatbotarena.com/)
    -   [EmbedAI | Custom ChatGPT for your website](https://www.thesamur.ai/)


#### Background {#background}

<!--list-separator-->

-  Defining problem

    -   Before starting, make sure that the problem that needs to be solved and the expectations are fully defined. "Teaching the model about xyz" is not a problem, it is a wish. It is hard to solve "wishes", but we can solve problems. For example: "I want to ask the model about xyz and get accurate answers based on abc data". This is needed to offer non stop answering chat for customers. We expect customer to ask "example1, 2, 3, .. 10" and we expect the answers to be in this style "example answers with example addressation, formal, informal, etc). We do not want the chat to engage in topics not related to xyz. If customer engage in such topics, politely explain that have no knowledge on that. (with example). This is a better description of the problem.
    -   It is important to define what is the expected way to interact with the model. Do you want to chat with it? Should it follow instructions? Do you want to provide a context and get output in the provided context? Do you want to complete your writing (like Github Copilot or Starcoder)? Do you want to perform specific tasks (eg grammar checking, translation, classification of something etc)?

<!--list-separator-->

-  Dataset preparation

    | Description                                                                                                                  | Example                                                                                                                                          |
    |------------------------------------------------------------------------------------------------------------------------------|--------------------------------------------------------------------------------------------------------------------------------------------------|
    | enriching data with context or system prompts                                                                                | [databricks/databricks-dolly-15k](https://huggingface.co/datasets/databricks/databricks-dolly-15k)                                               |
    | can significantly improve the final model's quality.                                                                         | [Open-Orca/OpenOrca](https://huggingface.co/datasets/Open-Orca/OpenOrca?row=9)                                                                   |
    | chains of multi-step Q&amp;A with solid results. (CoT + few shot)                                                            | [cognitivecomputations/wizard_vicuna_70k_unfiltered](https://huggingface.co/datasets/cognitivecomputations/wizard_vicuna_70k_unfiltered?row=21)  |
    |                                                                                                                              | [Fredithefish/ShareGPT-Unfiltered-RedPajama-Chat-format](https://huggingface.co/datasets/Fredithefish/ShareGPT-Unfiltered-RedPajama-Chat-format) |
    | a dataset for quotes is much simpler, because there will be no actual interaction,                                           |                                                                                                                                                  |
    | the quote is a quote. In this case, we want the bot to answer things like: `"Who wrote this quote: [famous quote content]?"` | [Abirate/english_quotes](https://huggingface.co/datasets/Abirate/english_quotes)                                                                 |
    | the `instruction, input(optional), output` format                                                                            | [yahma/alpaca-cleaned · Datasets at Hugging Face](https://huggingface.co/datasets/yahma/alpaca-cleaned)                                          |
    | Hugginface format                                                                                                            | [HuggingFaceH4/no_robots](https://huggingface.co/datasets/HuggingFaceH4/no_robots?row=43)                                                        |

    -   Automated tools can only do so much; manual work is indispensable and in many cases, difficult to outsource. Those who genuinely understand the product/process/business should scrutinize and cleanse the data. Even if the data is top-notch and GPT4 does a flawless job, the training could still fail. For instance, outdated information or contradictory responses can lead to poor results.
    -   Involve a significant portion of the organization in the process. I develop a simple internal tool allowing individuals to review rows of training data and swiftly edit the output or flag the entire row as invalid.

    <!--list-separator-->

    -  Useful links

        -   <https://platform.openai.com/docs/guides/fine-tuning/preparing-your-dataset#crafting-prompts>
        -   <https://distilabel.argilla.io/latest/> (Synthetic Datasets)
        -   <https://github.com/mlabonne/llm-datasets> (Catalog of LLM Datasets)

    <!--list-separator-->

    -  fine tuning dataset formats

        -   Shaping your data in the correct format is usually the most difficult and time-consuming step
        -   Most models are trained on a particular format, but you can often reformat a dataset to fit another format. This impacts the end user results. See [this post by HF](https://huggingface.co/blog/chat-templates) for more info. However, community is steering [more standardization of the format](https://huggingface.co/posts/dctanner/975913831192894). (so matching the format used during training is extremely important)
        -   If training on base model, the format depends on how do you want to use the chatbot (chatbot/instruct bot/completion bot etc). But if you're doing LoRA also depends what the pre-trained model is trained on. In other words, It's important to note that a base model could be fine-tuned on different chat templates, but when we're using an instruct model we need to make sure we're using the correct chat template that was used to create the instruct model from the base model.

<!--list-separator-->

-  Training

    <!--list-separator-->

    -  pre-training

        See <https://x.com/nrehiew_/status/1872318161883959485> (Deepseek technical report)

        > Goal: to recognize and follow arbitrary text, rich with meaning, intent, style, and structural features.

        -   pre-training requires LLMs to generate contextually coherent text completions across diverse documents. This teaches the model basic language proficiency to do it.
        -   The process involves sophisticated, nuanced pattern recognition at an advanced level.
        -   The primary pre-training techniques include Masked Language Modeling (MLM), where random words are masked and the model must predict them, and Next Sentence Prediction (NSP), which involves predicting the subsequent sentence in a given context.
        -   If you have a vast amount of data, i.e., tens of thousands of instructions, it's best to fine-tune the actual model.

        This pre-training can be followed by RLHF, instruction tuning etc.

    <!--list-separator-->

    -  post-training

        > Goal: Task specific training
        >
        > Useful for adapting the model for a certain style, get the model more into something etc.
        >
        > Some fine-tunes are
        >
        > -   full fine tunes which basically aim to replace all the original weights.
        > -   LoRAs

        <!--list-separator-->

        -  Useful links

            -   See [Trainer performance comparison: torchtune vs. axolotl vs. Unsloth](https://wandb.ai/augmxnt/train-bench/reports/Trainer-performance-comparison-torchtune-vs-axolotl-vs-Unsloth---Vmlldzo4MzU3NTAx) and [this](https://docs.google.com/spreadsheets/d/1kT4or6b0Fedd-W_jMwYpb63e1ZR3aePczz3zlbJW-Y4/edit?gid=1652827441#gid=1652827441)
            -   [How to fine-tune open LLMs in 2025 with Hugging Face](https://www.philschmid.de/fine-tune-llms-in-2025) 🌟
            -   [My experience on starting with fine tuning LLMs with custom data : LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/14vnfh2/my_experience_on_starting_with_fine_tuning_llms/)
            -   [Wow! llama-3-8b's in-context learning is unbelievable : LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/comments/1c7r2jw/wow_llama38bs_incontext_learning_is_unbelievable/)

        <!--list-separator-->

        -  fine tuning tools

            | Tool                                                                           | Description                                                                                                     | Other notes                         |
            |--------------------------------------------------------------------------------|-----------------------------------------------------------------------------------------------------------------|-------------------------------------|
            | [HF Autotrain Advanced](https://github.com/huggingface/autotrain-advanced%20)  | for finetune                                                                                                    |                                     |
            | [HF TLR](https://github.com/huggingface/trl)                                   | for rlhf, improve by RL (the same way gpt did it)                                                               | Uses unsloth, contains `SFTTrainer` |
            | [h2oai finetune script](https://github.com/h2oai/h2ogpt/blob/main/finetune.py) |                                                                                                                 |                                     |
            | [ligpt](https://github.com/Lightning-AI/litgpt)                                |                                                                                                                 |                                     |
            | [unsloth](https://github.com/unslothai/unsloth)                                | fasterst                                                                                                        |                                     |
            | [axolotl](https://github.com/axolotl-ai-cloud/axolotl)                         |                                                                                                                 |                                     |
            | [torchtune](https://github.com/pytorch/torchtune)                              | PyTorch native post-training library, see [usage](https://github.com/kinggongzilla/ai-clone-whatsapp/tree/main) | Post-training                       |
            | [llama-factory](https://github.com/hiyouga/LLaMA-Factory%20)                   | GUI, uses hf/unsloth under the hood                                                                             |                                     |
            | [instructor-embedding](https://github.com/xlang-ai/instructor-embedding)       | finetuning [with embeddings](https://www.youtube.com/watch?v=fYyZiRi6yNE)                                       |                                     |

            <!--list-separator-->

            -  More on SFTTrainer

                <https://gist.github.com/geekodour/989ded9c69e89f6738dc38d21f8057ae> (this script is configurable via yaml)

                The SFTTrainer is a subclass of the Trainer from the transformers library and supports all the same features, including logging, evaluation, and checkpointing, but adds additiional quality of life features, including:

                -   Dataset formatting, including conversational and instruction format
                -   Training on completions only, ignoring prompts
                -   Packing datasets for more efficient training
                -   PEFT (parameter-efficient fine-tuning) support including Q-LoRA, or Spectrum
                -   Preparing the model and tokenizer for conversational fine-tuning (e.g. adding special tokens)
                -   distributed training with accelerate and FSDP/DeepSpeed

                This SFTTrainer scripts works at a lower level than something like axolotl giving more visibility&amp;flexibility, but we could use that aswell.

        <!--list-separator-->

        -  It's experimental

            fine-tuned model's performance isn't necessarily indicative of the base model's capabilities" (but fewshot is indicative!)

            Getting a good LoRA is a trial and error process

            Sometimes [less is more](https://www.reddit.com/r/LocalLLaMA/comments/1c7r2jw/wow_llama38bs_incontext_learning_is_unbelievable/)

            -   In the case of 70b fine-tunes who can take a small suggestion of instruction-following and "wing it" impressively due to their superior ability to recognize fine details of the context.
            -   Also you can be very clever about reinforcing specific behaviors, such as "step by step thinking," and get a very proficient chatbot who eventually falters on moderately complex instructions, like Mistral 7b.

        <!--list-separator-->

        -  Tuning hyperparameters

            -   Tuning hyperparameters is pretty overrated once you have a solid baseline. once you find hyperparams that perform well you don't need to adjust much and many can be left as default, just do a quick sweep of learning rate + num epochs and you can basically use that forever unless metrics really start to drop off.
            -   When we do quantaization, we'd also want to do evals. So that quality is maintained.

        <!--list-separator-->

        -  Hardware

            -   fine tune a 12b model using LoRA for 10 epochs within 20 mins on 8 x A100 with h2o scipt, the but with HF's SFT it takes almost a day.
                -   Lora and 8bit quantisation for all the training (3 epoch), batchsize, seq_length
                -   Based on the the way you tune, the final model may take different vram etc. eg. [unsloth is more optimized than hf](https://www.reddit.com/r/LocalLLaMA/comments/18ny05c/finetuned_llama_27b_on_my_whatsapp_chats/kef27i0/)
                -   LoRA or qLoRA : When working w a smaller dataset
                -   Can happen on a cloud A6000 with 48GB VRAM, which costs about 80 cents per hour.
                -   Anything larger than a 13B model, whether it's LoRA or full fine-tuning, I'd recommend using A100.
                -   A100 is OK, H100 is better but the hassle of adapting the tools can be overwhelming

<!--list-separator-->

-  Retrieval

    <!--list-separator-->

    -  RAG

        See [RAG]({{< relref "20241227084628-rag.md" >}})

<!--list-separator-->

-  Prompting

    <!--list-separator-->

    -  one-shot vs few-shot

        It's somewhat vague what is called one-shot and what is called few-shot. OpenAI pushed the terminology to say "few-shot" when you provide "examples in instructions", as it's not taking advantage of in-context task learning, it's relying on facets of the fine-tuned task. If we want our llm to follow examples, we want to go with few shot learning.

        <!--list-separator-->

        -  one shot

            ```nil
            Help me perform sentiment analysis on some reviews. Here are a few examples:

            "This movie rocks!" - Positive
            "This movie sucks!" - Negative
            "The movie was meh" - Neutral
            ```

            This relies on the instruct training, and assumes the fine-tuning taught it well how to fish examples out of instructions. It may leverage some of the in-context learning ability to some extent, while also working against it.

            This example should NOT be called few-shots. This is just following instructions, which might not be strong. The llm might not know how to follow your instruction with examples.

        <!--list-separator-->

        -  few shot

            ```nil
            [
            {'role':'system', 'content':'Help me perform sentiment analysis on some reviews'},
            {'role':'user', 'content':'This movie rocks!'},
            {'role':'assistant', 'content':'Positive'},
            {'role':'user', 'content':'This movie sucks!'},
            {'role':'assistant', 'content':'Negative'},
            {'role':'user', 'content':'This movie is meh'}, {'role':'assistant', 'content':'Neutral'}
            ]
            ```

            This is few-shot, it follows a pattern, and targets `in-context learning` which is something LLMs learn in `pre-training`.

            -   In this example, we have a system instruction prompt, we don't even need that. We will find that we can remove that instruction. Because rest of the prompt are actual instructions and it follows a pattern. It is strong. So strong that there's really no point in putting instructions (like your system prompt) as the strong pattern-following will just overpower them.
            -   One can frame nearly arbitrary problems as documents that follow a simple pattern of input/output pairs. That's the basis of "few-shot" prompting.
            -   Good thing about few-shots is that it's indicative of the base model's capabilities.


#### Our process {#our-process}

-   Fine Tuning: For style
-   Fine Tuning + prompt : For QA
-   Fine Tuning + Few Shots + prompt: For
-   Fine Tuning + Few Shots + RAG : For retrieval

<!--list-separator-->

-  Practical

    -   <https://colab.research.google.com/drive/1bMOKOBzxQWUIGZBs_B0zm8pimuEnZdfM?usp=sharing#scrollTo=LjY75GoYUCB8>


### Font Loading / Index Building {#font-loading-index-building}

> method for data augmentation / index building

-   Instead of running slow and expensive LLMs at query time, you run them at index time.
-   See <https://github.com/xlang-ai/instructor-embedding> for it does something very similar
-   Basically, you use LLMs to add context to chunks that doesn’t exist on either
    -   the word level (searchable by BM25)
    -   the semantic level (searchable by embeddings)
-   Basically, you know your usecase and access patterns in advance and then you add the "handle" using natural language. This "handle" can be time frames, objects, styles, emotions whatever.
-   Sometimes this is all you need and not some complicated GraphRAG framework.
    -   Usually these GraphRAG framework would add garbage which later affects the search instead of helping.
    -   These graphRAG systems are built to cover a broad range of applications and are not adapted at all to your problem domain.


#### How to go about it? {#how-to-go-about-it}

Prefer working backwards

-   What kinds of queries do I need to handle?
-   What does the prompt to my query time LLM need to look like?
-   What context will the LLM need?
-   How can I have this context for each of my chunks, and be able to search by match air similarity?
-   How can I make an LLM return exactly that kind of context, with as few hallucinations and as little filler as possible, for each of my chunks?

This gives you a very lean, very efficient index that can do everything you want.


### GraphRAG {#graphrag}

See [RAG]({{< relref "20241227084628-rag.md" >}})


### More on LoRA {#more-on-lora}


#### Basics {#basics}

![](/ox-hugo/20230719050449-open_source_llms-466906546.png)
![](/ox-hugo/20230719050449-open_source_llms-231053531.png)
![](/ox-hugo/20230719050449-open_source_llms-74855914.png)


#### Cost tradeoff {#cost-tradeoff}

![](/ox-hugo/20230719050449-open_source_llms-1539628643.png)
![](/ox-hugo/20230719050449-open_source_llms-1886113774.png)

-   Sometimes loading the model into GPU itself takes a while, but you'll be charged for that so in that case is scale to 0 worth it? these kind of tradeoffs.
