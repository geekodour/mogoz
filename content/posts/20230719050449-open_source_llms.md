+++
title = "Open Source LLMs"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Modern AI Stack]({{< relref "20230326092427-modern_ai_stack.md" >}}) , [Machine Learning]({{< relref "20230408190056-machine_learning.md" >}}), [StableDiffusion]({{< relref "20230719120753-stablediffusion.md" >}})


## Resources {#resources}

-   Airtable link: [MLModels Airtable](https://airtable.com/appWjub9re6jaNTpA/tbly7fUiGaBkTVKvw/viwuTtXhizXBK9ybF?blocks=hide)
-   Fine tuning base model: [Fine-tuning Alpaca](https://www.youtube.com/watch?v=4-Q50fmq7Uw)
-   All links: <https://github.com/imaurer/awesome-decentralized-llm>
-   <https://github.com/evanmiller/LLM-Reading-List>


### Tools/libs {#tools-libs}

-   <https://github.com/marella/ctransformers> (ggml python bindings)
-   <https://huggingface.co/hkunlp/instructor-xl> (embeddings)


## Meta {#meta}

![](/ox-hugo/20230326092427-modern_ai_stack-1144765404.png)
![](/ox-hugo/20230719050449-open_source_llms-314571890.png)

-   Base models are not supposed to used directly, they are meant for fine-tuning in a way


## LLaMA {#llama}

-   LLaMA is not tuned for instruction following like ChatGPT
-   llma.cpp story : [What is the meaning of hacked? · Issue #33 · ggerganov/llama.cpp · GitHub](https://github.com/ggerganov/llama.cpp/issues/33#issuecomment-1465108022)

![](/ox-hugo/20230326092427-modern_ai_stack-179978562.png)
![](/ox-hugo/20230326092427-modern_ai_stack-1028578828.png)


## Alpaca {#alpaca}

{{< figure src="/ox-hugo/20230326092427-modern_ai_stack-404328522.png" >}}

-   What's Alpaca-LoRA ? Technique used to finetune llama using lora


## Case studies/Tools {#case-studies-tools}

-   [ReLLM: Exact Structure for Large Language Model Completions](https://matt-rickard.com/rellm)
-   [Context-Free Grammar Parsing with LLMs](https://matt-rickard.com/context-free-grammar-parsing-with-llms)
-   [GitHub - microsoft/guidance: A guidance language for controlling large language models.](https://github.com/microsoft/guidance)


## Comparison {#comparison}

-   Guanaco 7B (llma.cpp)
    -   1 Thread, CPU: 0.17-0.26 tokens/s
    -   11 Threads, 12vCPU: ~1token/s
    -   21 Threads, 12vCPU: ~0.3token/s
    -   10 Threads, 12vCPU: ~0.3token/s
    -   1 Thread, CPU, cuBALS: 0.17-0.26 tokens/s
    -   9 Thread, CPU, cuBALS: 5 tokens/s
    -   <https://github.com/ggerganov/llama.cpp/blob/master/docs/token_generation_performance_tips.md>
    -   <https://github.com/oobabooga/text-generation-webui/blob/main/docs/Generation-parameters.md>
-   GPTQ
    -   ~25token/s
