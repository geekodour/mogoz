+++
title = "Cricket'24"
author = ["Hrishikesh Barman"]
draft = false
+++

## Feedbacks on our Logic Model {#feedbacks-on-our-logic-model}

-   Q: Are they thinking about RC?
    -   Non explicit assessment (Design something and see and not just ask)
-   Very generic ideas about solving the problem
-   What's RC, What's Irresponsible RC (Need them for evaluation)
-   Do we care about improving programing skills? No. But taking care of RC would make them better computer scientists because they'll be able to connect with other eng domains and will be able to connect better
-   Missing
    -   Responsibity: Ask for tools?
    -   How do you prevent other them from building irresponsible sw? We can't
-   What about diff countries have different meaning of "responsible", how do we decide on that
-   No skills being captured??
-   What something falling through the cracks?
-   How does it compare to their ethics in other domains of engineering.
    -   Major diff: The field work(other domain), in CS(it's more abstract)


## Day1 {#day1}

-   Observe and Notice: Put up a map/image/others(which contain data) thinks which are not related to the subject and have a say about it


## DAY2 {#day2}


### RQ {#rq}

-   Does not need to be to specific
-   Not a boolean answer


#### Relation of RC to Logic Model {#relation-of-rc-to-logic-model}


#### Survey {#survey}

-   Likert scale: Odd vs Even points. With these we can have a neutral/forced position for the user

<!--list-separator-->

-  What to do with textual data?

    -   Wordcloud?
    -   LLMs?
    -   We basically classify it into rebrics
    -   Pyrate execution
        -   How do you label [USE],[FIX],[UPDATE] etc? from raw code to these?
            -   Rubrics: Took 2 months?
            -   Different raters would assign rubrics to samples
        -   Agreement measure
            -   Inter-Rater Reliability: % agreement correction (if incorrect by luck/chance)
                -   2 rater/n-rater: The raters are humans! (researchers/teachers/students)
            -   Multi agreement measures. When it's multiple rebrics added. Eg. [USE],[FIX] to one code snippet

<!--list-separator-->

-  Notes

    -   Grounded theory: Infer from the data


### 2nd Half {#2nd-half}


#### Survey {#survey}

-   Survey uppper bound
-   Look for correlation
-   Rely on assessment ofsuevey


#### aa {#aa}

-   Record their growing understanding of the systemm


#### tutor {#tutor}

-   misconception driven tutors


## DAY3 {#day3}


### RQ {#rq}


#### Things to do before research {#things-to-do-before-research}

-   RQ
-   Sampling &amp; Instruments
-   Participants
-   Data Analysis
-   Hypothesis
    -   You state your biases already(don't hide things)
    -   Sometimes if it's too open, sometimes we might not have any hypothesis


### Notes {#notes}

-   More real serious research rarely succeeds
-   School system is actually giving schedule
-   Epistimoloical Prespective (Qualitative Methods)
    -   Positivist is not necessasirily positive result
-   Botstrapworld
    -   Positivist: control group
    -   Interpretive: How students tell how they play and did etc
    -   Critical: We can add comp sci to school via other deciplines to improve critical thinking using proxy, change the structure
        -   Unplugged is more in the critical column.
-   Success
    -   Positivist: As a result something got better
    -   Interpretive: Was I able to infer any meaning?
    -   Critical: Do I achieve social change? Am I changing the power structure
        -   Having a social change does not necessasirily mean you've improved student lives
            -   In those cases, we probably need to do some positivist research into the result of a critical analysis. Eg. Change policy etc.
            -   Eg. Unplugged is more in the critical column.
            -   Once you do a critical analysis, you probably need to go back into prositivist analysis.
-   Skinners work, Behaviour psychology
