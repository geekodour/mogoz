+++
title = "Concurrency"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Operating Systems]({{< relref "20221101172456-operating_systems.md" >}}), [Systems]({{< relref "20221101150250-systems.md" >}})


## Concurrent V/S Parallel {#concurrent-v-s-parallel}

-   Parallelism is a subset of concurrency
-   Concurrent is 1 queues and 1 coffee machine. Parallel is 2 queues and 2 coffee machines.
-   Concurrency is juggling, parallelism is using two hands to do it
-   Concurrency is what you use to make it correct, parallelism is to make it faster.
-   In practice
    -   If you had `single core` and `multiple threads`, those threads will be run concurrently. Eg. only one thread at a time, but **no thread can assume any other thread's current state**.
    -   If you have `multiple cores`, it is possible to achieve parallelism by running `two threads at exactly the same time`.


### Concurrency {#concurrency}

-   Multiple things are in the middle of running, and they have to communicate/coordinate with each other in order to **get the overall job done**.
-   Concurrency is a **way to organize code**: pieces of your program get more flexible ways to communicate with each other than just call/return.
-   Concurrency is important when you're doing anything where the CPU has to wait. For example, network and disk heavy workloads.
-   Part of the value of concurrency is that it can highlight opportunities for parallelism.
-   If the processes are independent of each other you're not guaranteed to benefit from concurrency, especially if you only have 1 core.
-   **Doing it wrong causes:** Semantically wrong behavior (producing the wrong result, failing to produce a result at all).


### Parallelism {#parallelism}

-   Multiple things are in the middle of running, and several of them make progress simultaneously.
-   Parallelism is a **way to speed up code**: pieces of your program don't have to all wait their turn while one task at a time executes.
-   If the processes are independent of each other:
    -   You can distribute those calculations to 8, 16 or 32+ cores and significantly cut down your runtime.
    -   Otherwise, parallel code often requires some concurrency when tasks share intermediate results with each other.
-   **Doing it wrong causes:** inferior performance.


## Glossary {#glossary}

Not absolute, there can me mix/modifications.


### Distributed {#distributed}

-   Multiple independent processes
-   No shared memory
-   Communicating via message passing


### Multithreaded {#multithreaded}

-   Multiple threads share memory
-   Locks, mutexes, countdowns, condition variables, semaphores, and similar things come into play.


### Event Queue {#event-queue}

-   One a single thread exists!
-   A single loop reads from the event queue and invokes the handlers.
-   [4.3 The Case of Threads vs. Events](http://berb.github.io/diploma-thesis/original/043_threadsevents.html)


### Threads {#threads}

-   A sequential flow of control within a process.
-   One or many in the process
-   Can share same VAS as process


### Agents {#agents}

-   Actors: Communicate with each other via mailboxes. See [Actor Model]({{< relref "20221101221524-actor_model.md" >}})
-   Goroutines: Communicate with each other via `channels`
-   Coroutines: Communicate by `yielding` to each other.


### Extra Theory {#extra-theory}

-   [Process calculus - Wikipedia](https://en.wikipedia.org/wiki/Process_calculus)


## Modeling {#modeling}


### Non-realtime interleaved execution model {#non-realtime-interleaved-execution-model}

-   Study of interleaved execution sequences of atomic instructions
-   Each of the instructions
    -   Executes arbitrarily
    -   In finite amount of time.
    -   Can take many interleavings
-   No assumption of
    -   Relative speed of instructions
    -   How the scheduler is working

{{< figure src="/ox-hugo/20221126204257-concurrency-1787925353.png" >}}


### Levels to it {#levels-to-it}

-   Instruction Level: CPU
-   Statement Level:  (Certain languages support )
-   Procedure Level: Threads
-   Program Level: Multiple processes in OS
-   There are discussion around whether concurrency should be part of language, or the environment/library. Golang vs pthreads for example.


### Communication {#communication}

-   See [Inter Process Communication]({{< relref "20221101173527-ipc.md" >}})


### Tools {#tools}

See [Formal Methods]({{< relref "20230403235716-formal_methods.md" >}})


## Concurrency primitives {#concurrency-primitives}


### Mutex (Mutual Exclusion Lock) {#mutex--mutual-exclusion-lock}

-   Mutex guarantees that checking or modifying the value of a semaphore can be done safely
-   **OS guarantees** not to create a race condition between threads attempting to use the lock.


### Semaphores + Mutex {#semaphores-plus-mutex}

```text
lock := acquire_a_lock()
register := read_balance()
register += 100
write_balance(register)
release_lock()
```

-   +ve counter that can be used to synchronize multiple threads.


### Condition Variables + Mutex {#condition-variables-plus-mutex}

```text
critical_section do
  register := read_balance()
  register += 100
  write_balance(register)
end
```

-   Implement more complex conditions under which threads execute.
-   Generally used to
    -   Avoid busy waiting while waiting for a resource to become available.
    -   Instead, implement a condition under which a thread executes
    -   Inversely, implement a condition under which the thread is blocked


### Semaphores vs Condition Variables {#semaphores-vs-condition-variables}

-   Semaphores and condition variable can be used together
-   Condition variables has no counter/memory unlike semaphores.


### Lockless {#lockless}

-   [It's "locking" if it's blocking](https://www.reddit.com/r/programming/comments/15kbp4/its_locking_if_its_blocking_a_definition_lockfree/)
-   [An Introduction to Lock-Free Programming](https://preshing.com/20120612/an-introduction-to-lock-free-programming/)


## Correctness {#correctness}


### What Hoare said {#what-hoare-said}


#### Partial Correctness {#partial-correctness}

```text
if precondition & program terminates
  postconditions remain same (holds)
end
```

-   If an answer is returned
    -   It will be correct
-   This [can be proven](https://en.wikipedia.org/wiki/Hoare_logic#Partial_and_total_correctness) via [Correctness criteria](#correctness-criteria)


#### Total Correctness {#total-correctness}

```text
if precondition
  terminate program & postconditions remain same (holds)
end
```

-   If an answer is returned
    -   It will be correct
    -   It will return(terminate)
-   [Proving if a program](https://en.wikipedia.org/wiki/Correctness_(computer_science)) will terminate is undecidable


### Correctness criteria {#correctness-criteria}

Properties of a program are mathematical statements we can make about itâ€™s traces.


#### Safety properties (Always hold, BTNH) {#safety-properties--always-hold-btnh}

-   Bad things never happen
-   Requires only one counter example to refute
-   What to prove?
    -   Never does the wrong thing
    -   Never spits out the wrong answer
    -   Never violates invariants during execution
-   Most of us have a natural intuition for safety properties as programmers.
-   Example: Crash like things, Vulnerabilities, Memory errors etc.


#### Liveness properties (Eventually hold, GTAH) {#liveness-properties--eventually-hold-gtah}

-   Good things always happen
-   What to prove?
    -   A **finite proof of progress** resulting in the result we need
-   Example: Guaranteed availability, and termination


## Scheduling {#scheduling}

-   M processors and N threads. If M &lt; N you need a scheduler
-   In operation of a scheduler, `thread` can have many `states`


## Concurrency issues {#concurrency-issues}


### Race conditions {#race-conditions}

{{< figure src="/ox-hugo/20221126204257-concurrency-993213316.png" >}}

-   Result depends on: Relative timing of events in the threads.
-   Can exist on both single core and multicore systems.


#### Solution: Mutex Lock + Critical Section (atomic) {#solution-mutex-lock-plus-critical-section--atomic}

-   Only 1 process deals w `critical section` at a given time.
-   Critical section is basically chunking subsequences into atomic instruction
-   Now this solves Race conditions, but causes other issues like `deadlock`, `livelock`, `starvation`, `reliability`


### Deadlocks {#deadlocks}

-   One or more threads are stuck waiting for something that never will occur. Something only the other thread which is also blocked can unblock.
    ```text
        TX 1 asks, gets lock A
        TX 2 asks, gets lock B
        TX 1 asks, blocks waiting for lock B (held by TX 2)
        TX 2 asks for lock A. Boom!
    ```
-   Can be solved via understanding the order and re-thinking.


### Livelock {#livelock}

-   No progress is made because
    -   Each thread tries to enter its critical section
    -   then times out, then tries again, and so on, forever.
-   Can be solved via some jitter or something


### Starvation {#starvation}

-   Put upper bound on the time a process enters critical section while others wait.


### Reliability ðŸŒŸ {#reliability}

-   Thread crashes in the middle of the critical section. What now?
-   Thread also could be holding lock other processes wait for. Too bad now.
-   It's developer responsibility to make sure that critical sections are short and they terminate.


## By Language {#by-language}


### Javascipt {#javascipt}

-   See [Javascript Runtime and Browser]({{< relref "20221127082259-javascript_runtime.md" >}})
-   The single-threaded model has made Node.js a popular choice for server-side programming due to its non-blocking IO, making handling a large number of database or file-system requests very performant.
-   However, CPU-bound (computationally intensive) tasks that's pure JavaScript will still block the main thread.
-   To achieve real paralleling, you may need to use [workers](https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers).
-   A web worker or a cross-origin iframe has its own stack, heap, and message queue.


### Golang {#golang}

See [Golang]({{< relref "20221101220915-golang.md" >}})


#### Channels {#channels}

-   What good for?
    -   Process an endless stream of things
    -   Easily serve multiple goroutines
    -   Receive a notification of something at some point in the future
-   Channel Tips
    -   Channels orchestrate; mutexes serialize.
    -   Always close channels from sending side.
    -   Good to pass a channel as a parameter. (You have control over the channel)
    -   Don't buffer channels unless you're sure you need to


#### More info on Go Concurrency {#more-info-on-go-concurrency}

-   Has a [Race Detector](http://blog.golang.org/race-detector)
-   [Concurrency in Go {Book}](https://www.oreilly.com/library/view/concurrency-in-go/9781491941294/)
-   [Share memory by communicating Â· The Ethically-Trained Programmer](https://blog.carlmjohnson.net/post/share-memory-by-communicating/)


### Rust {#rust}

-   Rust offers safety from data races(not race conditions)


## Resources {#resources}

-   [introconcurrency](https://cs.lmu.edu/~ray/notes/introconcurrency/)
