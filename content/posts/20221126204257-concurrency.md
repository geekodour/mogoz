+++
title = "Concurrency"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Operating Systems]({{< relref "20221101172456-operating_systems.md" >}}), [Systems]({{< relref "20221101150250-systems.md" >}})


## FAQ {#faq}


### What Structured concurrency? {#what-structured-concurrency}

-   Structured concurrency is a higher-level pattern that helps in managing and controlling concurrent tasks in a structured manner. It's a wrapper over what concurrency style we apply to give us better control of error handling, leaking resources, leaving tasks unfinished etc.
-   Structured concurrency can be applied to any style, more of a meta thing.


### How do concurrent processes communicate? {#how-do-concurrent-processes-communicate}

-   See [Inter Process Communication]({{< relref "20221101173527-ipc.md" >}})


## Concurrent V/S Parallel {#concurrent-v-s-parallel}

<div class="warning small-text">

> The use of word "thread" is not very strict here.
</div>

-   Parallelism is a subset of concurrency
-   Concurrent is 1 queues and 1 coffee machine. Parallel is 2 queues and 2 coffee machines.
-   Concurrency is juggling, parallelism is using two hands to do it
-   Concurrency is what you use to make it correct, parallelism is to make it faster.
-   In practice
    -   If you had `single core` and `multiple threads`, those threads will be run concurrently. Eg. only one thread at a time, but **no thread can assume any other thread's current state**.
    -   If you have `multiple cores`, it is possible to achieve parallelism by running `two threads at exactly the same time`.
-   Example
    -   Concurrent: Internals of a web server
    -   Parallel: Google's MapReduce

{{< figure src="/ox-hugo/20221126204257-concurrency-1512579699.png" >}}


### Concurrency {#concurrency}

{{< figure src="/ox-hugo/20221126204257-concurrency-1421013282.png" >}}

-   Multiple things are in the middle of running, and they have to communicate/coordinate with each other in order to **get the overall job done**. This means, **more work for the scheduler** as things are dependent on order of execution.
-   Concurrency is a **way to organize code**: pieces of your program get more flexible ways to communicate with each other than just call/return.
-   Concurrency is important when you're doing anything where the CPU has to wait. For example, network and disk heavy workloads.
-   Part of the value of concurrency is that it can highlight opportunities for parallelism.
-   If the processes are independent of each other you're not guaranteed to benefit from concurrency, especially if you only have 1 core.
-   **Doing it wrong causes:** Semantically wrong behavior (producing the wrong result, failing to produce a result at all).


### Parallelism {#parallelism}

{{< figure src="/ox-hugo/20221126204257-concurrency-1149897538.png" >}}

-   Multiple things are in the middle of running, and several of them make progress simultaneously.
-   Parallelism is a **way to speed up code**: pieces of your program don't have to all wait their turn while one task at a time executes.
-   If the processes are independent of each other:
    -   You can distribute those calculations to 8, 16 or 32+ cores and significantly cut down your runtime.
        ![](/ox-hugo/20221126204257-concurrency-614463799.png)
    -   It's a promise by the user to the scheduler that the result of the code will be correct however it is scheduled to run.
-   **Doing it wrong causes:** inferior performance.
-   We can use concurrent mechanism(eg. thread pools) to run parallel code but it'll be slow. Because, now the user is not giving the scheduler the guarantee that there are no cross-task dependencies.


### Serial {#serial}

{{< figure src="/ox-hugo/20221126204257-concurrency-1647421493.png" >}}


## Concurrency Styles {#concurrency-styles}

<div class="warning small-text">

> This is not a strict list, just trying to categorize shit i see in the wild. The terminologies are so confusing. someone needs to make a feature matrix or something, maybe by language and make it the defacto thing when ppl argue about these things. These ideas can be using together, some might be othogonal etc.
</div>


### Thread based {#thread-based}

-   See [Threads]({{< relref "20221101173032-threads.md" >}}) and [Coroutines]({{< relref "20230516192836-coroutines.md" >}})
-   Can share memory/ or not
-   Uses things like locks, mutexes, countdowns, condition variables, semaphores etc.
-   A sequential flow of control within a process.


### Event Queue/Event driven {#event-queue-event-driven}

{{< figure src="/ox-hugo/20221126204257-concurrency-68786927.png" >}}

-   One a single thread exists!
-   A single loop reads from the event queue and invokes the handlers.


### Actor Model {#actor-model}

-   See [Actor Model]({{< relref "20221101221524-actor_model.md" >}})
-   No concept of "events", "threads" or a "processes", only Actors and messages.


### CSP {#csp}

-   [Communicating sequential processes - Wikipedia](https://en.wikipedia.org/wiki/Communicating_sequential_processes#Comparison_with_the_actor_model)


### Instruction level {#instruction-level}


#### SIMD &amp; MIMD {#simd-and-mimd}

See [Flynn's Taxonomy]({{< relref "20230408161230-flynn_s_taxonomy.md" >}})


#### GPGPU {#gpgpu}

See [GPGPU]({{< relref "20230408051445-gpgpu.md" >}})


### Statement level based {#statement-level-based}

-   Language syntax based.
-   `channels` and `groutines` in [Golang]({{< relref "20221101220915-golang.md" >}}).
-   Atomic operations in languages


### Distributed {#distributed}

-   Multiprocessor and Multi-computer stuff


## Primitives and Issues {#primitives-and-issues}


### Concurrency primitives {#concurrency-primitives}


#### Mutex (Mutual Exclusion Lock) {#mutex--mutual-exclusion-lock}

-   Mutex guarantees that checking or modifying the value of a semaphore can be done safely
-   **OS guarantees** not to create a race condition between threads attempting to use the lock.


#### Semaphores + Mutex {#semaphores-plus-mutex}

```text
lock := acquire_a_lock()
register := read_balance()
register += 100
write_balance(register)
release_lock()
```

-   +ve counter that can be used to synchronize multiple threads.


#### Spinlocks {#spinlocks}

-   Spinlocks were non-sleeping locks
-   Code waiting for a spinlock would spin in a loop until the lock became available.
-   Despite their advantages, spinlocks couldn't replace semaphores entirely due to their inability to sleep.


#### Condition Variables + Mutex {#condition-variables-plus-mutex}

```text
critical_section do
  register := read_balance()
  register += 100
  write_balance(register)
end
```

-   Implement more complex conditions under which threads execute.
-   Generally used to
    -   Avoid busy waiting while waiting for a resource to become available.
    -   Instead, implement a condition under which a thread executes
    -   Inversely, implement a condition under which the thread is blocked


#### Semaphores vs Condition Variables {#semaphores-vs-condition-variables}

-   Semaphores and condition variable can be used together
-   Condition variables has no counter/memory unlike semaphores.
-   [The shrinking role of semaphores {LWN.net}](https://lwn.net/Articles/928026/)


#### Lockless {#lockless}

-   [It's "locking" if it's blocking](https://www.reddit.com/r/programming/comments/15kbp4/its_locking_if_its_blocking_a_definition_lockfree/)
-   [An Introduction to Lock-Free Programming](https://preshing.com/20120612/an-introduction-to-lock-free-programming/)


### Concurrency issues {#concurrency-issues}

{{< figure src="/ox-hugo/20221126204257-concurrency-872428154.png" >}}


#### Race conditions {#race-conditions}

{{< figure src="/ox-hugo/20221126204257-concurrency-993213316.png" >}}

-   Result depends on: Relative timing of events in the threads.
-   Can exist on both single core and multicore systems.

<!--list-separator-->

-  Solution: Mutex Lock + Critical Section (atomic)

    -   Only 1 process deals w `critical section` at a given time.
    -   Critical section is basically chunking subsequences into atomic instruction
    -   Now this solves Race conditions, but causes other issues like `deadlock`, `livelock`, `starvation`, `reliability`


#### Deadlocks {#deadlocks}

-   One or more threads are stuck waiting for something that never will occur. Something only the other thread which is also blocked can unblock.
    ```text
        TX 1 asks, gets lock A
        TX 2 asks, gets lock B
        TX 1 asks, blocks waiting for lock B (held by TX 2)
        TX 2 asks for lock A. Boom!
    ```
-   Can be solved via understanding the order and re-thinking.


#### Livelock {#livelock}

-   No progress is made because
    -   Each thread tries to enter its critical section
    -   then times out, then tries again, and so on, forever.
-   Can be solved via some jitter or something
-   If deadlock is like playing on the beats, livelock is like playing on the offs. Either-way you end up in a situation where things are stuck.


#### Starvation {#starvation}

-   Put upper bound on the time a process enters critical section while others wait.


#### Reliability ðŸŒŸ {#reliability}

-   Thread crashes in the middle of the critical section. What now?
-   Thread also could be holding lock other processes wait for. Too bad now.
-   It's developer responsibility to make sure that critical sections are short and they terminate.


## Theoretical view of concurrency {#theoretical-view-of-concurrency}


### Modeling concurrency {#modeling-concurrency}

See [Formal Methods]({{< relref "20230403235716-formal_methods.md" >}})


#### Non-realtime interleaved execution model {#non-realtime-interleaved-execution-model}

-   Study of interleaved execution sequences of atomic instructions
-   Each of the instructions
    -   Executes arbitrarily
    -   In finite amount of time.
    -   Can take many interleavings
-   No assumption of
    -   Relative speed of instructions
    -   How the scheduler is working

{{< figure src="/ox-hugo/20221126204257-concurrency-1787925353.png" >}}


### Correctness {#correctness}


#### What Hoare said {#what-hoare-said}

<!--list-separator-->

-  Partial Correctness

    ```text
    if precondition & program terminates
      postconditions remain same (holds)
    end
    ```

    -   If an answer is returned
        -   It will be correct
    -   This [can be proven](https://en.wikipedia.org/wiki/Hoare_logic#Partial_and_total_correctness) via [Correctness criteria](#correctness-criteria)

<!--list-separator-->

-  Total Correctness

    ```text
    if precondition
      terminate program & postconditions remain same (holds)
    end
    ```

    -   If an answer is returned
        -   It will be correct
        -   It will return(terminate)
    -   [Proving if a program](https://en.wikipedia.org/wiki/Correctness_(computer_science)) will terminate is undecidable


#### Correctness criteria {#correctness-criteria}

Properties of a program are mathematical statements we can make about itâ€™s traces.

<!--list-separator-->

-  Safety properties (Always hold, BTNH)

    -   Bad things never happen
    -   Requires only one counter example to refute
    -   What to prove?
        -   Never does the wrong thing
        -   Never spits out the wrong answer
        -   Never violates invariants during execution
    -   Most of us have a natural intuition for safety properties as programmers.
    -   Example: Crash like things, Vulnerabilities, Memory errors etc.

<!--list-separator-->

-  Liveness properties (Eventually hold, GTAH)

    -   Good things always happen
    -   What to prove?
        -   A **finite proof of progress** resulting in the result we need
    -   Example: Guaranteed availability, and termination


## Scheduling {#scheduling}

-   M processors and N threads. If M &lt; N you need a scheduler
-   In operation of a scheduler, `thread` can have many `states`


## By Language {#by-language}


### Javascipt {#javascipt}

-   See [Javascript Runtime and Browser]({{< relref "20221127082259-javascript_runtime.md" >}})
-   The single-threaded model has made Node.js a popular choice for server-side programming due to its non-blocking IO, making handling a large number of database or file-system requests very performant.
-   However, CPU-bound (computationally intensive) tasks that's pure JavaScript will still block the main thread.
-   To achieve real paralleling, you may need to use [workers](https://developer.mozilla.org/en-US/docs/Web/API/Web_Workers_API/Using_web_workers).
-   A web worker or a cross-origin iframe has its own stack, heap, and message queue.


### Golang {#golang}

See [Golang]({{< relref "20221101220915-golang.md" >}}), [Concurrency in Golang]({{< relref "20230412015037-concurrency_in_golang.md" >}})


### Rust {#rust}

-   Rust offers safety from data races(not race conditions)


## Resources {#resources}

-   [introconcurrency](https://cs.lmu.edu/~ray/notes/introconcurrency/)
-   [Process calculus - Wikipedia](https://en.wikipedia.org/wiki/Process_calculus)
