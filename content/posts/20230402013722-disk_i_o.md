+++
title = "Disk I/O"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Linux]({{< relref "20221101150211-linux.md" >}}), [Systems]({{< relref "20221101150250-systems.md" >}}), [Filesystems]({{< relref "20221101145053-filesystems.md" >}}), [Storage]({{< relref "20221101164723-storage.md" >}})

Thanks to [Alex](https://medium.com/databasss/on-disk-io-part-1-flavours-of-io-8e1ace1de017) for his amazing blogposts explaining these concepts!


## FAQ {#faq}


### What is I/O really? {#what-is-i-o-really}

I/O is the process of copying data between RAM memory and external devices such as a disk driver. If you want to copy one section of the driver to another section in the driver the data **MUST go through RAM** at some point. (from some reddit comment)


### How does data from device get to memory for I/O? {#how-does-data-from-device-get-to-memory-for-i-o}


#### DMA {#dma}

-   See [DMA]({{< relref "20230517205323-dma.md" >}})


#### via CPU {#via-cpu}

1.  Device Interrupt: The device notifies the CPU that it has data ready.
2.  Interrupt Handler: The CPU executes a routine to manage the data transfer.
3.  Polling/Programmed I/O: The CPU interacts with the device to initiate the transfer.
4.  Data Transfer: The CPU reads the data from the device and stores it temporarily.
5.  Copying to Memory: The CPU transfers the data from its registers to the memory.
6.  Completion and Resume: Additional operations are performed, and the CPU resumes its interrupted task.


### Is Page Cache inevitable? {#is-page-cache-inevitable}

-   Yes. Both memory mapped I/O and standard I/O internally access files `on disk` through `page cache`. We can use the `O_DIRECT` to skip the page cache.
-   If you're doing any I/O things will go via RAM, and when it does so it'll be using page cache.


### Is Page cache involved when doing [DMA]({{< relref "20230517205323-dma.md" >}})? {#is-page-cache-involved-when-doing-dma--20230517205323-dma-dot-md}

-   Yes
-   Linux does not allow memory-to-memory DMA. It only does
    -   DMA_BIDIRECTIONAL, DMA_TO_DEVICE, DMA_FROM_DEVICE, DMA_NONE
    -   See [Dynamic DMA mapping Guide â€” The Linux Kernel documentation](https://docs.kernel.org/core-api/dma-api-howto.html)


## Raw IO/Unbuffered IO/Direct IO/Low level IO {#raw-io-unbuffered-io-direct-io-low-level-io}

-   These are low level unbuffered operations that usually deal with [File Descriptors]({{< relref "20230315164241-file_descriptors.md" >}}).
-   The name can be confusing, what unbuffered means is
    -   No application level buffering happens(eg. Standard IO)
    -   You **still have to** provide the destination buffer and length
    -   i.e. where you would like the function to store the read data for you to use in your own program.
-   Example for `read()`
    -   `read()` being unbuffered means if you ask for `1 byte` you get `1 byte` read from disk
    -   your next call to `read()` triggers another disk read.
    -   Under the hood it is anyway using the page cache, but just no userspace buffering. (See [Virtual Memory]({{< relref "20221101214011-virtual_memory.md" >}}))


### When to use what? {#when-to-use-what}

Some internet comments.

-   Direct IO with read and write is a good idea if you know exactly what size data you are going to read and that size is large enough for extra buffering not to make much of a difference (or you implement your own buffering).
-   Picking between fread and read is somewhat of a premature optimization I think: even if you think read is unbuffered, it's likely the file is being buffered (again) at the OS level, and probably cached in the hard drive controller too, and in any significant case (big file) your bottleneck is I/O anyway so the price of an extra memcpy is negligible... but now you have to manage your own buffer instead to avoid accidentally causing syscalls.
-   I suggest you stick with fread for "files", read for network sockets and hardware interfaces (RS-232 registers, etc), and if you have a use case for random access in a large file then you will want to look into mmap memory-mapped i/o.
-   If you are using the low-level (syscalls)
    -   open(2), write(2), read(2) etc.
    -   then use the low-level fsync(2).


## Standard IO {#standard-io}

-   It's basically [syscalls]({{< relref "20230330194208-syscalls.md" >}}) combined with userland optimizations. (Eg. `fopen, fwrite, fread, fflush, fclose`).
-   You generally **don't mix** Raw IO stuff like `fsync` with Standard IO stuff like `fflush`. But you probably can?
-   It also involves copying your data from the page cache to the userland buffer cache. So essentially there is 2 cost: `syscalls` and `copying` of data.
    ![](/ox-hugo/20230402013722-disk_i_o-485356102.png)


### Userspace buffering {#userspace-buffering}

-   See `man setbuf`
-   This is not the kernel buffer (`page cache`) (See [Virtual Memory]({{< relref "20221101214011-virtual_memory.md" >}}))


#### Unbuffered {#unbuffered}

-   Information appears on the destination file or terminal as soon as written
-   `stderr` is usually unbuffered


#### Block buffered ðŸŒŸ {#block-buffered}

-   Many characters are saved up and written as a block
-   Usually files are block buffered
    -   If you call `fread()` and ask for `1 byte` then the stdlib might actually read `32k`, give you `1 byte`
    -   Next time you call `fread()` the bytes come from the buffer and not the disk.
    -   `fflush(3)` may be used to force the block out early. (`fflush` flushes the userspace buffer)


#### Line buffered {#line-buffered}

-   Characters are saved up until a newline is output or input is read from any stream attached to a terminal device (typically stdin).
-   `stdout` is usually line buffered


### How disk read happens {#how-disk-read-happens}

-   CPU initiates **disk read**
    -   Sends (`logical block no`, `memory address`, `command`)
    -   Through `bus interface` -&gt; `I/O bridge` -&gt; `I/O bus` -&gt; `disk controller`
-   `Disk controller`
    -   Executes the command
    -   Uses `DMA` to send that data over to memory.
    -   Sends an [Interrupts]({{< relref "20221101173720-interrupts.md" >}}) informing transfer done
-   [Virtual Memory]({{< relref "20221101214011-virtual_memory.md" >}})
    -   Now that stuff is in memory
    -   Kernel will try to caches FS `blocks` in memory into the Page cache.
    -   VM works with `Pages`, which is usually 4096, similar to the size of FS `block` in my case, but it does not have to be like that.


### Blocks v/s Sectors {#blocks-v-s-sectors}

<div class="warning small-text">

> Virtual Memory `pages` &lt;=&gt; Filesystem `blocks` &lt;=&gt; Block Device `sectors`
</div>


#### Filesystem {#filesystem}

-   `Block` is an abstraction of [Filesystems]({{< relref "20221101145053-filesystems.md" >}}) (See [Storage layers]({{< relref "20221101164723-storage.md#storage-layers" >}}))
-   All FS operations can be accessed only in multiple of `Blocks`.
-   `tune2fs` will give you `Block size` for your FS. (Usually `4096`)


#### Block Device {#block-device}

-   [Block device]({{< relref "20230314123832-unix_files.md#block-device" >}}) provides unbuffered access to hardware. Linux uses block device to access disk devices such as [SSDs]({{< relref "20221101213401-memory_hierarchy.md#ssds" >}}) and [HDDs]({{< relref "20221101213401-memory_hierarchy.md#hdds" >}})
-   Smallest addressable unit on a block device is a `sector`. It is not possible to transfer less than one `sector` worth of data.
-   `fdisk -l` will give you the `sector size` of your block device. (Usually `512`)


### Under the hood syscalls {#under-the-hood-syscalls}

-   open, write, read, fsync, sync, close


#### Read {#read}

-   Page Cache is addressed first.
-   If the data is absent, the Page Fault is triggered and contents are paged in.
-   Non-sequential access requires another system call `lseek`.
-   If a `page` gets dirty, `read()` will read from the Page Cache instead of reading from (now outdated) disk.


#### Write {#write}

-   Buffer(userland) contents are first written to Page Cache.
-   Data does not reach the disk right away.
-   Actual hardware write is done when kernel writebacks the `dirty page`.
-   You can also use `fsync`
-   The `writeback` of `dirty page` from Page Cache to disk can be configured via thresholds and ratios.


## Memory mapped IO {#memory-mapped-io}

-   open, mmap, msync, munmap
-   See [mmap]({{< relref "20230405022848-mmap.md" >}}), can tell it to use [Copy on Write]({{< relref "20230405020429-copy_on_write.md" >}}) semantics
-   A memory region in "process address space" is mapped directly to "page cache". No additional userspace buffer cache involved.
    ![](/ox-hugo/20230402013722-disk_i_o-289336833.png)
-   Any dirty page handling is now done by the OS


### How the mapping happens {#how-the-mapping-happens}

-   File is memory mapped
-   Space for the memory mapping is reserved, but not allocated
-   Attempts to access file region, if first read/write, page fault
-   Allocation happens
-   One can also pre-fault using `MAP_POPULATE`


### Improving memory mapped IO {#improving-memory-mapped-io}

-   There's no single way, but using Advice [syscalls]({{< relref "20230330194208-syscalls.md" >}}) can help.


## Async IO (AIO) {#async-io--aio}

Interface allows initiating multiple IO operations &amp; register callbacks


### Pitfalls {#pitfalls}

-   `glibc` has no interface. There's `libio` but people say it sucks.
-   Usually you don't want to be writing code with direct calls to `epoll/poll/select/kqueue` etc. Because not good for portability.
-   It is something of a black art, and has [many pitfalls](https://stackoverflow.com/questions/34572559/asynchronous-io-io-submit-latency-in-ubuntu-linux/46377629#46377629).
-   There are userland libraries such as libenev, [libuv](http://kprotty.me/2023/04/13/libuv-but-multithreaded-but-not-really.html)(used in node.js) etc. which use their own logic to do async io.


### io_uring {#io-uring}

See [io_uring]({{< relref "20230516155822-io_uring.md" >}})


## Vectored IO (Scatter/Gather) {#vectored-io--scatter-gather}

![](/ox-hugo/20230402013722-disk_i_o-497171156.png)
Operates on a vector of buffers and allows reading and writing data to/from disk using multiple buffers per system call.

-   Allows for
    -   Reading smaller chunks
    -   Reducing the amount of system calls
-   Uses
    -   Analytics workloads and/or columnar databases(data stored contiguously)
    -   Processing can be done in parallel in sparse blocks
    -   Eg. Apache Arrow


## Non Blocking IO {#non-blocking-io}

-   Take away: Non-Blocking IO [does not work for regular files](https://www.remlab.net/op/nonblock.shtml).


## IO Schedulers {#io-schedulers}

Typical SSD throughput is greater than 10_000 IOPS, while a typical HDD can handle only about 200 IOPS. There is a queue of requests that have to wait for access to the storage. This is where IO schedulers come in.

-   Arch wiki has great [summary on the algorithms](https://wiki.archlinux.org/title/improving_performance#The_scheduling_algorithms)


### Legacy {#legacy}

-   [Elevator](https://en.wikipedia.org/wiki/Elevator_algorithm) : Order by logical address, not optimal for sequential access
-   [Anticipatory](https://en.wikipedia.org/wiki/Anticipatory_scheduling) : Tries to improve Elevator by waiting for other seq requests
-   [Deadline](https://en.wikipedia.org/wiki/Deadline_scheduler) : Came in to prevent starvation. Bad for overall throughput.
-   [Completely fair queueing](https://en.wikipedia.org/wiki/CFQ) (`cfq`): Allocates timeslice and queue size by process. [cgroups]({{< relref "20230304201630-cgroups.md" >}}) is supported, Good for cloud hosting etc.


### Newer {#newer}

-   [Budget Fair Queueing](https://algo.ing.unimo.it/people/paolo/disk_sched/) (`bfq`): Improves on `cfq`. Has high per operation overhead, so using this on a slow CPU, can slow down the whole thing. Focus on lowest latency rather than highest throughput.
-   mq-deadline: Adaptation of the deadline scheduler to multi-threading. Good for non-rotating media.
-   [Kyber](https://lwn.net/Articles/720675/) : New kid in the block. Token based.


## Other notes {#other-notes}


### Sparse File {#sparse-file}

Sparse files are files stored in a file system where consecutive data blocks consisting of all zero-bytes (null-bytes) are compressed to nothing.

-   Use: Bittorrent, VM images etc.
-   [What are sparse files and how to tell if a file is stored sparsely | Ctrl blog](https://www.ctrl.blog/entry/sparse-files.html) (Best explanation)
-   [Sparse file - Wikipedia](https://en.wikipedia.org/wiki/Sparse_file)
-   [linux - How to view contents of a sparse file? - Unix &amp; Linux Stack Exchange](https://unix.stackexchange.com/questions/521917/how-to-view-contents-of-a-sparse-file)
-   [filesystems - What are functions to manipulate sparse files under Linux? - Unix &amp; Linux Stack Exchange](https://unix.stackexchange.com/questions/366373/what-are-functions-to-manipulate-sparse-files-under-linux)
-   [filesystems - How transparent are sparse files for applications? - Unix &amp; Linux Stack Exchange](https://unix.stackexchange.com/questions/733150/how-transparent-are-sparse-files-for-applications)
-   [filesystems - What is a sparse file and why do we need it? - Stack Overflow](https://stackoverflow.com/questions/43126760/what-is-a-sparse-file-and-why-do-we-need-it)


### Reading large files {#reading-large-files}

-   [What they don't tell you about demand paging in school - offlinemark](https://offlinemark.com/2020/10/14/demand-paging/)


## Resources {#resources}

-   [Learn about different I/O Access Methods and what we chose for ScyllaDB](https://www.scylladb.com/2017/10/05/io-access-methods-scylla/) ðŸŒŸ
-   [Rubber Ducking: The Various Kinds of IO - Blocking, Non-blocking, Multiplexed...](https://www.rubberducking.com/2018/05/the-various-kinds-of-io-blocking-non.html)
-   [The Secret Life of fsync](https://puzpuzpuz.dev/the-secret-life-of-fsync)
-   [Investigating Linux Phantom Disk Reads | QuestDB](https://questdb.io/blog/investigating-linux-phantom-disk-reads/)
-   [Is sequential IO dead in the era of the NVMe drive? | Lobsters](https://lobste.rs/s/hgz1hj/is_sequential_io_dead_era_nvme_drive)
-   [Modern storage is plenty fast. It is the APIs that are bad.](https://itnext.io/modern-storage-is-plenty-fast-it-is-the-apis-that-are-bad-6a68319fbc1a)
-   [50 years in filesystems: 1974 | Die wunderbare Welt von Isotopp](https://blog.koehntopp.info/2023/05/05/50-years-in-filesystems-1974.html#naming-files)
-   [Livestream - P&amp;S Modern SSDs (Spring 2023) - YouTube](https://www.youtube.com/playlist?list=PL5Q2soXY2Zi_8qOM5Icpp8hB2SHtm4z57)
-   [Eat My Data: How Everybody gets File IO Wrong - YouTube](https://www.youtube.com/watch?v=LMe7hf2G1po)
-   [Database Architects: What Every Programmer Should Know About SSDs](https://databasearchitects.blogspot.com/2021/06/what-every-programmer-should-know-about.html)
-   [How does Linux really handle writes?](https://www.cyberdemon.org/2023/06/27/file-writes.html)
-   [Coding for SSDs â€“ Part 1: Introduction and Table of Contents | Code Capsule](https://codecapsule.com/2014/02/12/coding-for-ssds-part-1-introduction-and-table-of-contents/)
