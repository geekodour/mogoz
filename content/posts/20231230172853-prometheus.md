+++
title = "Prometheus"
author = ["Hrishikesh Barman"]
draft = false
+++

tags
: [Observability]({{< relref "20221102123214-observability.md" >}}), [PromQL]({{< relref "20221102122633-promql.md" >}})


## FAQ {#faq}


### Tactical stuff {#tactical-stuff}


#### Measuring latency {#measuring-latency}

-   A system like Prometheus aggregates across time to work efficiently. So you can calculate things like mean latency, but you can't see individual request latency. Eg. with python we can use prometheus histograms.


## Service Discovery {#service-discovery}


### AWS ECS {#aws-ecs}

-   [Service discovery - Amazon Elastic Container Service](https://docs.aws.amazon.com/AmazonECS/latest/developerguide/service-discovery.html#service-discovery-pricing)
-   Seems like there are couple of ways to do it because ECS-SD is somewhat not supported officially.
    -   [Prometheus on ECS - Proof of Concept | 3h4x.github.io](https://3h4x.github.io/tech/2020/02/07/prometheus-on-ecs-poc)
        -   This uses [teralytics/prometheus-ecs-discovery](https://github.com/teralytics/prometheus-ecs-discovery)
    -   Then there is aws suggested methods
        -   This uses [AWS]({{< relref "20231228230358-aws.md" >}}) CloudMap, see [this](https://github.com/aws-samples/prometheus-for-ecs/blob/main/deploy-prometheus/README.md)


## Exposing endpoints {#exposing-endpoints}


### What do we want to expose, prometheus or grafana? {#what-do-we-want-to-expose-prometheus-or-grafana}

-   Assuming a user would need to access the prometheus UI
-   This really is not a question about prometheus/grafana but more of a question about [Infrastructure]({{< relref "20221101183735-infrastructure.md" >}}) [Security]({{< relref "20221101184454-security.md" >}}). (when [Selfhosting]({{< relref "20230212140130-selfhosting.md" >}}))
-   In some cases companies keep their grafana instance publicly accessible from the internet but behind some auth. (Usually behind reverse [Proxies]({{< relref "20230212101521-proxies.md" >}})) and the prometheus instance sits in a private subnet etc.
    -   In other cases, both prometheus and grafana might be in a private subnet and you need to be in the VPC through a [VPN]({{< relref "20230210192007-vpn.md" >}}) or something similar to be in the same network. Otherwise, things can be public and firewalls etc can be used.


### Can we expose on trusted networks? {#can-we-expose-on-trusted-networks}

-   Depends on security posture
-   Exposing that port on a "trusted" network is a possible attack vector for bad actors.
-   Exposing that port on the open internet (as is often the case) is an open invitation for attack.


### Exposing prometheus/grafana {#exposing-prometheus-grafana}

This is about protecting the endpoints provided by the prometheus binary

-   By making it public
    -   Auth + RP + Firewall rules for limited access
    -   [Protecting Prometheus: Insecure configuration exposes secrets](https://jfrog.com/blog/dont-let-prometheus-steal-your-fire/)
-   By not making it public (private subnet)
    -   Use a VPN
    -   Instead of using a VPN we could use other solutions, such as
        -   [Enable Prometheus to Scrape Anything from Anywhere | OpenZiti](https://openziti.io/blog/zitification/prometheus/part1/)
        -   <https://medium.com/@dnorth98/hello-aws-session-manager-farewell-ssh-7fdfa4134696>
        -   [Connect to an AWS VPC using subnet routes · Tailscale Docs](https://tailscale.com/kb/1021/install-aws)
        -   [AWS reference architecture · Tailscale Docs](https://tailscale.com/kb/1296/aws-reference-architecture)
        -   [Run a Tailscale VPN relay on ECS/Fargate - Platformers](https://platformers.dev/log/2022/tailscale-ecs/)
        -   [Access AWS RDS privately using Tailscale · Tailscale Docs](https://tailscale.com/kb/1141/aws-rds)


### Exposing applications exposing prometheus metrics {#exposing-applications-exposing-prometheus-metrics}

-   [Exposing /metrics endpoint to public internet](https://github.com/prometheus-operator/prometheus-operator/issues/800)
    -   Blocking access to path at load balancer/reverse proxy seems like a good way to do it
    -   The service would be accessible from the VPC anyway, so the prometheus collector instance will be able to scrape it using the private IP/whatever it gets from service discovery.


## Instrumentation {#instrumentation}


### Go (client-go) {#go--client-go}


#### Meta notes {#meta-notes}

-   When you simply use the metrics primitives, registering them and then exposing using `promhttp.HandlerFor`, you just get those metrics and some error metric. You don't get the go and process collector metrics etc.
-   Cause of inconsistencies
    -   If a metric inconsistent
    -   If a metric is equal to the new metric already registered.

<!--list-separator-->

-  Registry

    -   `Collector & Metric` have to describe themselves to the `registry`.
        -   Because inconsistencies are supposed to be detected at registration time, not at collect time.
    -   Usually you'd use the `DefaultRegistry`
        -   Comes registered with `NewGoCollector` and `NewProcessCollector`
    -   Custom Registry
        -   Implement your own `Register/Gatherer`
        -   Special properties, see NewPedanticRegistry.
        -   Avoid **global state**, as imposed by DefaultRegisterer.
        -   Use multiple registries at the same time to expose different metrics in different ways.
        -   Separate registries for testing purposes.

<!--list-separator-->

-  Gatherer

    -   `Registry` implements the `Gatherer` interface.
    -   The caller of the Gather method can then expose the gathered metrics in some way.

<!--list-separator-->

-  Metric Types &amp; Vector metric types

    -   You need `<MetricName>Opts` for creation
    -   Fundamental metric types
        -   Gauge, Counter, Summary, and Histogram
        -   Implement the `Metric` interface
        -   `Gauge, Counter, Summary, Histogram` are interfaces themselves (These embed the `Metric` and `Collector` interfaces)
    -   Vector metric types
        -   GaugeVec, CounterVec, SummaryVec, and HistogramVec
        -   This is when we partition samples along dimensions called **"labels"**
        -   Implement the `Metric` interface and the `Collector` interface
        -   `GaugeVec, CounterVec, SummaryVec, HistogramVec` are **NOT** interfaces themselves

<!--list-separator-->

-  Collector

    -   A `Collector` manages the collection of a number of Metrics
    -   For convenience, a `Metric` can also “collect itself”.
    -   Creation of the `Metric` instance happens in the `Collect` method.
    -   Usually you would not implement custom `Metric` but might implement custom `Collector`

    <!--list-separator-->

    -  Why custom collector?

        -   Usecase1: A custom Collector seems handy to bundle `Metrics` for "common registration"
        -   Usecase2: Already have metrics outside of prometheus context, "mirror" existing numbers into Metrics during collection.
            -   Metric instances “on the fly” using `NewConstMetric, NewConstHistogram`, and `NewConstSummary` (also `Must` versions)
                -   `NewConstMetric` for a float64 value: `Counter, Gauge`, and a special “type” called `Untyped` (When unsure of `Counter` or `Gauge`)
                -   Convenience functions: `GaugeFunc, CounterFunc, or UntypedFunc`
            -   The `processCollector`, `goCollector`, `expvarCollector` are collectors used for "mirroring" purposes.

    <!--list-separator-->

    -  Unchecked vs check collector

        -   scrape inconsistencies
        -   When exact metrics to be returned by a Collector cannot be predicted at registration time, but the implementer has sufficient knowledge of the whole system to guarantee metric consistency.

<!--list-separator-->

-  Instrumented Handler vs Uninstrumented Handler


#### Packages {#packages}

<!--list-separator-->

-  prometheus

    -   What it offers
        -   Primitives to instrument
            -   Eg. `NewGauge`, `NewCounterVec` etc.
        -   Registry for metrics
            -   Eg. You use `prometheus.Registerer.MusRegister` for each metric

<!--list-separator-->

-  promhttp

    -   Expose registered metrics via http
    -   Has
        -   `Handler()`
            -   Can think of it as a convenience function
            -   sort of same was using `InstrumentMetricHandler` with `HandlerFor`
        -   `HandlerFor`
            -   Takes in `prometheus.Gatherer` and `promhttp.HandlerOpts`
            -   HandlerOpts defines the behavior of the returned Handler
            -   Custom instrumentation/Gathers etc.

<!--list-separator-->

-  promauto

    -   Metrics constructors with automatic registration.
    -   When you are not concerned with fine-grained control of when and how to register metrics with the registry
    -   Allows you to ignore registration altogether in simple cases.
    -   Provides constructors
        -   Top-level functions
            -   Return Collectors registered with the global registry (`prometheus.DefaultRegisterer`)
        -   Set are methods of the Factory type
            -   Return Collectors registered with the `custom registry` the Factory was constructed with.
            -   Using `prometheus.NewRegistry()` and `promauto.With(reg)`

    <!--list-separator-->

    -  Registration Panicking

        -   Input provided by the caller that is invalid on its own.
        -   Incompatible metric is added to an unrelated part of the code
        -   With global registry
            -   Where simply importing another package can trigger a panic
            -   If the newly imported package registers metrics in its init function
        -   Steps with vanilla `prometheus`
            -   You first create the metric/collectors
            -   Decide explicitly if you want to register it with a local or the global registry
            -   Decide if you want to handle the error or risk a panic.
        -   Steps with vanilla `promauto`
            -   Registration is automatic
            -   If it fails, it will always panic.
            -   Constructors will often be called in the var section of a file
                -   Which means that panicking will happen as a side effect of merely importing a package.

<!--list-separator-->

-  push

    -   Push registered metrics


### Instrumenting Batch Jobs {#instrumenting-batch-jobs}

-   Instrumenting long running services is straight-forward to think about. Batch jobs are somewhat harder to wrap the head around what to include, how to include etc.
-   See the "what to monitor/batch job" section in [Observability]({{< relref "20221102123214-observability.md" >}}) for some more concrete reference


#### Gotchas/Tips {#gotchas-tips}

-   Cardinality
    -   **Don't have** job-run-id as a label [due to cardinality](https://www.reddit.com/r/PrometheusMonitoring/comments/11ktdzx/collection_metrics_on_an_perjob_execution_basis/), you can have by job-name.
        -   But now how do you differentiate between different runs? For many things we'll be using the `gauge` instrument in which only what's the `last value` matters. In this case if you have multiple jobs and no differentiating label, the resulting metric for that series will be confusing.
            -   In most of these cases you'll have `gauge with no labels`, eg. outcome of a job can be indicated with a gauge with `0/1` value with no labels.
        -   However, if it's a type of job and there's a guarantee that there will be no concurrent runs of it then not having the `job-run-id` is not a problem because the data will be time series and we'll see gaps between runs when we visualize the data and things would make sense
    -   If you want per row metric or something when running the job, you most of the time want to be logging that [and process it](https://grafana.com/docs/loki/latest/send-data/promtail/pipelines/) later using recorded queries or something similar.
    -   So basically two ways
        -   Either you push the metrics for each run whenever it is done and are OK with losing some of the metrics
        -   Or go with a separate tool that parses the output files(eg. logs) of your script runs and aggregates everything somehow
-   Model
    -   It has to follow a push model instead of a pull one. Eg. As the script completes, it can push the metrics (e.g. req. duration histogram, CPU utilization, peak memory etc.)
    -   This push model can be implemented via pushgateway and if you're using OTEL(See [Observability]({{< relref "20221102123214-observability.md" >}})) then you can just use the OTEL Collector(eg. Grafana Alloy)


## OTEL instrumentation {#otel-instrumentation}


### Metrics API {#metrics-api}

{{< figure src="/ox-hugo/20231230172853-prometheus-397022078.png" >}}

-   I think usually you'd just need one `MeterProvider` and is expected to be `global`
    -   All configuration should happen at `MeterProvider` not in the `Meter`
-   `Meter` provides creation methods for various `Instrument` (s)
    -   an `Instrument` is mapped to a `Meter` during "creation"


#### Instruments {#instruments}

-   NOTE: Whether an instrument is int/[Float]({{< relref "20221101164343-floating_point.md" >}}) is `identifying`
-   Instrument names can be max 255 chars and case-insensitive.
-   `unit` is optional
-   Variants
    -   `Sync`: Invoked inline with application/business processing logic.
        -   Eg. How many bytes were transmitted
        -   The `Measurement` can be associated with `Context`
    -   `Async`: Register callbacks that invoked on demand (SDK `collection`)
        -   Eg. Async gauge can collect info from source every 15s (i.e callback invoked every 15s)
        -   The `Measurement` CANNOT be associated with `Context`
        -   Callbacks
            -   Callback functions will be called only when the Meter is being observed.
            -   Callbacks can be registered/unregistered after after asynchronous instrumentation creation also
                ![](/ox-hugo/20231230172853-prometheus-648960982.png)
            -   Callbacks should be reentrant safe
            -   Multi-instrument callback
                -   Callback that's associated w multiple instruments
            -   The API MUST treat observations from a single callback as logically taking place at a single instant, such that when recorded, observations from a single callback MUST be reported with identical timestamps.
            -   The `Async Instrument` has `register_callback` and the callback itself will have `unregister`
        -   Attributes
            -   These can be registered at Instrument creation time
            -   API should also be flexible to provide ad-hoc attributes at invocation time
    -   `UpDown`

| Instrument Kind          | Variant | Operations                                                  | Eg.                                                         |
|--------------------------|---------|-------------------------------------------------------------|-------------------------------------------------------------|
| Counter (+ve increment)  | Sync    | `Add` (does not return anything)                            | No. of req, No. of bytes                                    |
| Counter (+ve increment)  | Async   | Callback (returns absolute value: list/observable result)   | CPU time by program, Total process heap size                |
| Gauge (non-additive)     | Sync    | `Record`, on-change -&gt; `Record` (Based on change events) | Background Noise                                            |
| Gauge (non-additive)     | Async   | Callback, accessors -&gt; Observable record                 | Room temperature                                            |
| UpDownCounter (additive) | Sync    | `Add` (takes in increment/delta value)                      | Total heap size, No. of "active" req, No. of items in queue |
| UpDownCounter (additive) | Async   | Callback                                                    | Total heap size                                             |
| Histogram                | Sync    | `Record`                                                    | Req. Duration, Size of Payload                              |


#### Measurement {#measurement}

-   Consist of `value:(attributes)`


### <span class="org-todo todo TODO">TODO</span> Metrics SDK {#metrics-sdk}

<https://opentelemetry.io/docs/specs/otel/metrics/sdk/> (TODO: To read, read only like 30% of it)

-   `MeterProvider` MUST provide a way to allow a `Resource` to be specified
    -   `shutdown` method
    -   `ForceFlush` method
        -   Works only on `push metrics`
        -   Does not work on `pull metrics` (by design)
-   Components
    -   `MetricReaders`
    -   `MetricExporters`
    -   `Views`
        ![](/ox-hugo/20231230172853-prometheus-419616333.png)
        -   Determines the `aggregation` type
            -   (`Drop`, `~Default`, `Sum`, `Last Value`, `Explicit Bucket Histogram`)
        -   An instrumented library can provide both temperature and humidity, but the application developer might only want temperature.
        -   If `MeterProvider` has no View registered, take the Instrument and apply the default Aggregation


## Concepts {#concepts}


### Relabeling {#relabeling}

{{< figure src="/ox-hugo/20231230172853-prometheus-922966043.png" >}}

-   Can be applied to
    -   Targets: `relabel_configs`
    -   Metric samples: `metric_relabel_configs`
        -   Remote write: `write_relabel_configs`
    -   Alerts: `alert_relabel_configs`
    -   See [relabel_configs vs metric_relabel_configs – Robust Perception](https://www.robustperception.io/relabel_configs-vs-metric_relabel_configs/)
-   Action
    -   Change
    -   Filter


#### List of possible actions {#list-of-possible-actions}

{{< figure src="/ox-hugo/20231230172853-prometheus-429993138.png" >}}


#### Rules {#rules}

-   `source_labels` specific
    -   `regex`
        -   When using `regex` you must have a `replacement` config set. It can be the capture group or a static string.
        -   `regex` is matched against the value contained in the `source_label`
    -   `separator`
        -   Use this as a utility to join different label values into the target Eg.
            ![](/ox-hugo/20231230172853-prometheus-27745987.png)
    -   `target_label` will be the final label and `replacement` will be the value of the `target_label`
-   action: `labeldrop`
    -   `labeldrop` needs the `regex` field, and needs nothing else.
    -   `labeldrop` action is applied unconditionally to every label of every metric.
    -   We **cannot** conditionally select certain series and labeldrop on them.
    -   We can however do target_label replacement as shown in this SO answer: [prometheus - Conditionally drop labels - Stack Overflow](https://stackoverflow.com/questions/73716815/conditionally-drop-labels)
        -   Otherwise I think we could also have multiple scrape targets for the same endpoint and keep different set of metrics but this would make the config look ugly i think.
        -   [Drop label for single metric only, not for entire scrape job](https://groups.google.com/g/prometheus-users/c/w2Jh45-fpjs)
    -   [Rationale behind Prometheus Labeldrop Action - Stack Overflow](https://stackoverflow.com/questions/57100303/rationale-behind-prometheus-labeldrop-action)


#### Gotchas/Tips {#gotchas-tips}

-   Labels **starting** with a double underscore (`__`) are automatically removed after the last relabeling rule.
-   After relabeling, Prometheus sets the `instance` label to the value of `__address__` if you do not set the `instance` label explicitly to another value before that.
-   service discovery(SD) mechanisms can provide a set of labels starting with `__meta_` that contain discovery-specific metadata about a target.
-   `labelmap` is nice if you want to preserve the SD `__meta` labels with a different prefix


### Histograms {#histograms}

-   [Histograms and summaries | Prometheus](https://prometheus.io/docs/practices/histograms/)
-   [Prometheus Histograms. Run that past me again? | akuszyk.com](https://andykuszyk.github.io/2020-07-24-prometheus-histograms.html)
-   [How does a Prometheus Histogram work? – Robust Perception | Prometheus Monitoring Experts](https://www.robustperception.io/how-does-a-prometheus-histogram-work/)
-   [How does a Prometheus Summary work? – Robust Perception | Prometheus Monitoring Experts](https://www.robustperception.io/how-does-a-prometheus-summary-work/)
-   <https://medium.com/mercari-engineering/have-you-been-using-histogram-metrics-correctly-730c9547a7a9>
-   <https://bryce.fisher-fleig.org/prometheus-histograms/>
-   [Histograms for Probability Density Estimation: A Primer | Hacker News](https://news.ycombinator.com/item?id=39974229)
